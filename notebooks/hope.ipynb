{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.0.downsample.0 [407, 375, 495, 441, 476, 6, 497, 187, 266, 100, 290, 35, 163, 154, 116, 210, 92, 328, 327, 486, 225, 458, 61, 475, 385, 422, 414, 357, 352, 179, 445, 456, 354, 9, 68, 419, 48, 117, 212, 40, 111, 177, 264, 12, 21, 446, 41, 34, 285, 252, 390, 213]\n",
      "layer3.0.downsample.0 [40, 70, 130, 137, 211, 133, 86, 2, 58, 190, 3, 61, 81, 34, 50, 221, 44, 8, 140, 102, 114, 16, 149, 141, 240, 184]\n",
      "layer2.0.downsample.0 [53, 24, 18, 55, 15, 28, 5, 76, 125, 21, 22, 47, 109]\n",
      "conv1 [18, 4, 7, 54, 35, 9, 25]\n",
      "layer1.0.conv1 [4, 35, 33, 61, 57, 55, 44]\n",
      "layer1.1.conv1 [3, 38, 18, 22, 34, 12, 57]\n",
      "layer2.0.conv1 [71, 79, 22, 30, 0, 92, 41, 59, 62, 11, 8, 109, 70]\n",
      "layer2.1.conv1 [76, 126, 10, 39, 17, 84, 94, 95, 23, 2, 97, 89, 117]\n",
      "layer3.0.conv1 [64, 172, 174, 157, 168, 89, 150, 147, 248, 145, 143, 203, 201, 178, 179, 197, 107, 108, 110, 134, 187, 21, 230, 40, 24, 25]\n",
      "layer3.1.conv1 [71, 2, 230, 121, 131, 242, 186, 38, 187, 243, 60, 13, 158, 19, 44, 174, 172, 169, 113, 143, 232, 117, 155, 241, 88, 171]\n",
      "layer4.0.conv1 [64, 363, 2, 362, 189, 5, 361, 7, 187, 9, 464, 457, 358, 13, 454, 354, 16, 453, 18, 19, 351, 451, 448, 23, 186, 284, 26, 27, 440, 437, 183, 429, 32, 178, 426, 424, 422, 37, 419, 39, 283, 41, 416, 411, 44, 45, 46, 410, 409, 344, 406, 404]\n",
      "layer4.1.conv1 [128, 234, 138, 71, 351, 231, 74, 141, 225, 224, 458, 279, 304, 503, 283, 501, 142, 17, 498, 19, 20, 495, 22, 484, 398, 198, 193, 86, 189, 29, 472, 255, 115, 309, 34, 177, 247, 180, 319, 436, 174, 41, 239, 43, 321, 362, 325, 328, 386, 49, 426, 163]\n",
      "Pruning step: 1 multiply–accumulate (macs): 1474518396.0 number of parameters 9481588\n",
      "layer4.0.downsample.0 [273, 380, 72, 259, 106, 271, 267, 299, 436, 442, 385, 459, 57, 367, 427, 136, 58, 308, 351, 426, 69, 113, 8, 439, 256, 135, 258, 154, 242, 444, 116, 15, 279, 364, 366, 266, 286, 117, 74, 328, 112, 255, 25, 181, 446, 395, 48, 35, 329, 215, 244]\n",
      "layer3.0.downsample.0 [74, 59, 115, 20, 172, 52, 147, 67, 195, 27, 168, 79, 99, 113, 19, 107, 61, 182, 83, 194, 211, 5, 220, 200, 48, 111]\n",
      "layer2.0.downsample.0 [18, 43, 51, 30, 27, 15, 92, 11, 49, 114, 82, 52, 21]\n",
      "conv1 [10, 3, 31, 27, 13, 14]\n",
      "layer1.0.conv1 [20, 21, 31, 47, 53, 17]\n",
      "layer1.1.conv1 [27, 47, 40, 24, 15, 19]\n",
      "layer2.0.conv1 [26, 38, 44, 78, 13, 11, 58, 70, 54, 4, 105, 46, 74]\n",
      "layer2.1.conv1 [94, 110, 66, 41, 6, 64, 74, 102, 65, 16, 109, 24, 71]\n",
      "layer3.0.conv1 [122, 23, 113, 227, 116, 191, 118, 121, 138, 182, 171, 57, 163, 40, 26, 124, 144, 213, 17, 151, 130, 222, 64, 147, 152, 59]\n",
      "layer3.1.conv1 [161, 213, 105, 160, 50, 43, 93, 205, 30, 27, 17, 29, 158, 204, 221, 74, 108, 41, 51, 223, 66, 210, 71, 179, 7, 112]\n",
      "layer4.0.conv1 [52, 97, 98, 135, 356, 216, 365, 134, 137, 219, 242, 307, 240, 143, 301, 145, 300, 132, 299, 297, 228, 232, 293, 457, 455, 451, 106, 450, 268, 443, 440, 439, 438, 290, 282, 130, 36, 280, 127, 169, 431, 41, 42, 430, 426, 45, 279, 425, 423, 172, 126]\n",
      "layer4.1.conv1 [367, 316, 44, 117, 308, 120, 121, 370, 320, 372, 380, 49, 296, 139, 412, 252, 455, 4, 311, 179, 6, 181, 82, 297, 230, 207, 162, 388, 406, 80, 300, 342, 341, 98, 3, 128, 442, 61, 52, 420, 361, 450, 313, 432, 233, 299, 40, 170, 398, 383, 453]\n",
      "Pruning step: 2 multiply–accumulate (macs): 1178780522.0 number of parameters 7534380\n",
      "layer4.0.downsample.0 [188, 107, 104, 330, 195, 222, 187, 282, 239, 309, 374, 214, 27, 83, 185, 164, 259, 167, 115, 300, 75, 1, 182, 346, 69, 289, 205, 231, 77, 56, 373, 267, 298, 142, 406, 137, 234, 371, 117, 199, 398, 181, 31, 91, 230, 5, 73, 286, 265, 349, 229]\n",
      "layer3.0.downsample.0 [165, 168, 196, 131, 97, 27, 111, 96, 18, 146, 179, 189, 195, 62, 10, 198, 175, 151, 121, 75, 20, 158, 177, 105, 191]\n",
      "layer2.0.downsample.0 [22, 89, 64, 13, 86, 92, 53, 78, 52, 33, 36, 12, 2]\n",
      "conv1 [2, 1, 43, 8, 49, 24, 18]\n",
      "layer1.0.conv1 [23, 19, 31, 21, 15, 40, 14]\n",
      "layer1.1.conv1 [45, 35, 48, 19, 30, 46, 21]\n",
      "layer2.0.conv1 [72, 91, 67, 31, 84, 101, 62, 36, 6, 99, 92, 30, 38]\n",
      "layer2.1.conv1 [84, 88, 100, 32, 29, 5, 60, 21, 55, 64, 80, 13, 96]\n",
      "layer3.0.conv1 [107, 148, 150, 97, 25, 34, 123, 199, 137, 102, 112, 177, 19, 4, 6, 21, 84, 95, 171, 20, 13, 122, 136, 70, 195]\n",
      "layer3.1.conv1 [158, 152, 185, 144, 44, 50, 35, 12, 123, 173, 10, 46, 86, 47, 11, 101, 145, 65, 95, 6, 164, 121, 107, 91, 151]\n",
      "layer4.0.conv1 [102, 379, 300, 91, 101, 103, 60, 190, 167, 187, 186, 184, 164, 183, 363, 162, 176, 225, 161, 359, 160, 175, 158, 157, 236, 258, 57, 58, 59, 75, 61, 62, 63, 64, 318, 70, 84, 86, 301, 89, 141, 306, 378, 215, 16, 246, 45, 284, 204, 72, 269]\n",
      "layer4.1.conv1 [290, 266, 176, 404, 4, 163, 0, 200, 258, 294, 51, 36, 75, 187, 120, 395, 226, 354, 81, 80, 193, 91, 54, 108, 27, 275, 177, 47, 210, 212, 218, 274, 339, 128, 367, 17, 345, 71, 254, 31, 208, 205, 197, 227, 276, 312, 88, 304, 324, 328, 57]\n",
      "Pruning step: 3 multiply–accumulate (macs): 905775022.0 number of parameters 5820556\n",
      "layer4.0.downsample.0 [175, 68, 346, 87, 7, 260, 73, 104, 95, 232, 238, 181, 182, 200, 171, 69, 215, 281, 333, 209, 233, 228, 139, 138, 336, 320, 245, 214, 180, 108, 81, 261, 259, 9, 1, 90, 335, 205, 306, 13, 332, 307, 276, 176, 322, 63, 203, 86, 14, 131, 231]\n",
      "layer3.0.downsample.0 [39, 73, 44, 116, 150, 152, 174, 25, 178, 167, 45, 75, 112, 49, 166, 96, 54, 120, 53, 79, 34, 43, 102, 70, 20, 14]\n",
      "layer2.0.downsample.0 [82, 38, 50, 53, 4, 44, 61, 51, 77, 43, 56, 49, 47]\n",
      "conv1 [20, 18, 21, 9, 1, 6]\n",
      "layer1.0.conv1 [39, 43, 38, 25, 7, 10]\n",
      "layer1.1.conv1 [3, 29, 33, 43, 28, 4]\n",
      "layer2.0.conv1 [87, 37, 9, 33, 46, 27, 77, 62, 13, 68, 51, 30, 88]\n",
      "layer2.1.conv1 [7, 13, 26, 83, 80, 11, 52, 58, 9, 0, 54, 67, 39]\n",
      "layer3.0.conv1 [41, 66, 176, 156, 78, 121, 151, 94, 42, 53, 136, 168, 119, 120, 6, 159, 23, 19, 138, 33, 122, 17, 144, 29, 140, 71]\n",
      "layer3.1.conv1 [34, 159, 104, 151, 114, 51, 60, 7, 109, 47, 44, 59, 118, 132, 63, 96, 94, 133, 20, 64, 125, 87, 26, 100, 42, 43]\n",
      "layer4.0.conv1 [18, 119, 225, 140, 45, 254, 141, 41, 138, 56, 263, 319, 134, 178, 144, 285, 199, 212, 231, 275, 191, 286, 311, 261, 112, 82, 356, 190, 33, 167, 325, 267, 163, 266, 169, 271, 347, 192, 36, 326, 160, 343, 269, 59, 238, 317, 58, 96, 62, 25, 52]\n",
      "layer4.1.conv1 [76, 334, 72, 13, 108, 239, 306, 333, 282, 41, 183, 208, 252, 39, 298, 163, 94, 68, 36, 265, 329, 343, 49, 95, 244, 340, 315, 158, 33, 190, 222, 40, 121, 83, 268, 326, 316, 202, 131, 56, 24, 330, 322, 174, 253, 100, 175, 350, 52, 14, 57]\n",
      "Pruning step: 4 multiply–accumulate (macs): 677503778.0 number of parameters 4318898\n",
      "layer4.0.downsample.0 [154, 122, 284, 283, 77, 37, 2, 55, 226, 258, 104, 296, 90, 276, 62, 68, 266, 11, 185, 86, 50, 152, 54, 80, 234, 236, 148, 66, 121, 248, 52, 222, 124, 240, 262, 145, 183, 164, 213, 79, 264, 129, 228, 220, 31, 256, 65, 160, 35, 225, 168]\n",
      "layer3.0.downsample.0 [92, 141, 81, 82, 91, 22, 42, 55, 71, 45, 19, 88, 125, 58, 20, 121, 129, 146, 64, 2, 131, 8, 78, 103, 46]\n",
      "layer2.0.downsample.0 [61, 14, 67, 68, 17, 13, 11, 73, 7, 46, 29, 0]\n",
      "conv1 [7, 20, 33, 29, 21, 3]\n",
      "layer1.0.conv1 [3, 4, 29, 15, 23, 12]\n",
      "layer1.1.conv1 [30, 9, 32, 6, 23, 22]\n",
      "layer2.0.conv1 [31, 15, 57, 23, 43, 12, 58, 74, 36, 27, 25, 75]\n",
      "layer2.1.conv1 [44, 27, 2, 60, 74, 14, 32, 67, 15, 39, 4, 38]\n",
      "layer3.0.conv1 [126, 80, 102, 34, 142, 146, 121, 129, 127, 51, 135, 54, 134, 44, 12, 21, 7, 40, 22, 105, 16, 65, 26, 147, 37]\n",
      "layer3.1.conv1 [110, 116, 131, 137, 139, 143, 78, 114, 2, 122, 127, 42, 48, 107, 53, 76, 124, 69, 79, 150, 45, 85, 39, 113, 84]\n",
      "layer4.0.conv1 [213, 243, 298, 54, 159, 294, 166, 260, 143, 108, 16, 66, 106, 160, 135, 123, 109, 187, 102, 6, 271, 33, 26, 133, 211, 265, 186, 142, 264, 124, 24, 50, 188, 20, 274, 119, 268, 70, 59, 158, 296, 31, 217, 167, 234, 9, 82, 141, 68, 190, 192]\n",
      "layer4.1.conv1 [121, 125, 192, 13, 226, 211, 16, 8, 258, 1, 95, 51, 293, 114, 9, 164, 302, 80, 76, 17, 150, 301, 259, 50, 148, 290, 241, 162, 40, 232, 84, 146, 182, 70, 111, 177, 87, 255, 305, 280, 39, 155, 65, 118, 106, 205, 174, 63, 116, 14, 166]\n",
      "Pruning step: 5 multiply–accumulate (macs): 487202536.0 number of parameters 3055880\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchinfo\\torchinfo.py:288\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 288\u001b[0m     _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)(\u001b[39m*\u001b[39mx, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:269\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    268\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[1;32m--> 269\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x)\n\u001b[0;32m    270\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n",
      "\u001b[1;31mValueError\u001b[0m: expected 4D input (got 3D input)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 1\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m tp\u001b[39m.\u001b[39mload_state_dict(new_model, state_dict\u001b[39m=\u001b[39mloaded_state_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# print(new_model) # This will be a pruned model.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#W0sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m summary(new_model, (\u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchinfo\\torchinfo.py:218\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m validate_user_params(\n\u001b[0;32m    212\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    213\u001b[0m )\n\u001b[0;32m    215\u001b[0m x, correct_input_size \u001b[39m=\u001b[39m process_input(\n\u001b[0;32m    216\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    217\u001b[0m )\n\u001b[1;32m--> 218\u001b[0m summary_list \u001b[39m=\u001b[39m forward_pass(\n\u001b[0;32m    219\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    220\u001b[0m )\n\u001b[0;32m    221\u001b[0m formatting \u001b[39m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    222\u001b[0m results \u001b[39m=\u001b[39m ModelStatistics(\n\u001b[0;32m    223\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    224\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchinfo\\torchinfo.py:297\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    296\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[1;32m--> 297\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{\u001b[39;00mexecuted_layers\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[39mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2d: 1]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "from torchstat import stat\n",
    "from torchinfo import summary\n",
    "import os\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filepath, name):\n",
    "    torch.save(state, os.path.join(filepath, name+'checkpoint.pth'))\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "# Importance criteria\n",
    "example_inputs = torch.randn(1, 3, 224, 224)\n",
    "imp = tp.importance.TaylorImportance()\n",
    "\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 1000:\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "iterative_steps = 5 # progressive pruning\n",
    "current_step = 1\n",
    "prune_amounts = [x / 64 for x in range(48)]\n",
    "\n",
    "# 0.015625 -> 1 0.03125 -> 2 0.0625 -> 4 0.125 -> 8 0.25 -> 16 0.5 -> 32\n",
    "pruner = tp.pruner.MagnitudePruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    iterative_steps=iterative_steps,\n",
    "    ch_sparsity=0.5, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "    ignored_layers=ignored_layers,\n",
    ")\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "\n",
    "\n",
    "for i in range(iterative_steps):\n",
    "    if isinstance(imp, tp.importance.TaylorImportance):\n",
    "        # Taylor expansion requires gradients for importance estimation\n",
    "        loss = model(example_inputs).sum() # a dummy loss for TaylorImportance\n",
    "        loss.backward() # before pruner.step()\n",
    "    pruner.step()\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(\"Pruning step:\", current_step, \"multiply–accumulate (macs):\", macs, \"number of parameters\", nparams)\n",
    "    current_step += 1\n",
    "\n",
    "    # finetune your model here\n",
    "    # finetune(model)\n",
    "    # ...\n",
    "# save the pruned state_dict, which includes both pruned parameters and modified attributes\n",
    "state_dict = tp.state_dict(model) # the pruned model, e.g., a resnet-18-half\n",
    "torch.save(state_dict, 'pruned.pth')\n",
    "\n",
    "# create a new model, e.g. resnet18\n",
    "new_model = resnet18().eval()\n",
    "\n",
    "# load the pruned state_dict into the unpruned model.\n",
    "loaded_state_dict = torch.load('pruned.pth', map_location='cpu')\n",
    "tp.load_state_dict(new_model, state_dict=loaded_state_dict)\n",
    "# print(new_model) # This will be a pruned model.\n",
    "\n",
    "summary(new_model, (3, 224, 224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight [32, 3, 7, 7]\n",
      "bn1.weight [32]\n",
      "bn1.bias [32]\n",
      "layer1.0.conv1.weight [32, 32, 3, 3]\n",
      "layer1.0.bn1.weight [32]\n",
      "layer1.0.bn1.bias [32]\n",
      "layer1.0.conv2.weight [32, 32, 3, 3]\n",
      "layer1.0.bn2.weight [32]\n",
      "layer1.0.bn2.bias [32]\n",
      "layer1.1.conv1.weight [32, 32, 3, 3]\n",
      "layer1.1.bn1.weight [32]\n",
      "layer1.1.bn1.bias [32]\n",
      "layer1.1.conv2.weight [32, 32, 3, 3]\n",
      "layer1.1.bn2.weight [32]\n",
      "layer1.1.bn2.bias [32]\n",
      "layer2.0.conv1.weight [64, 32, 3, 3]\n",
      "layer2.0.bn1.weight [64]\n",
      "layer2.0.bn1.bias [64]\n",
      "layer2.0.conv2.weight [64, 64, 3, 3]\n",
      "layer2.0.bn2.weight [64]\n",
      "layer2.0.bn2.bias [64]\n",
      "layer2.0.downsample.0.weight [64, 32, 1, 1]\n",
      "layer2.0.downsample.1.weight [64]\n",
      "layer2.0.downsample.1.bias [64]\n",
      "layer2.1.conv1.weight [64, 64, 3, 3]\n",
      "layer2.1.bn1.weight [64]\n",
      "layer2.1.bn1.bias [64]\n",
      "layer2.1.conv2.weight [64, 64, 3, 3]\n",
      "layer2.1.bn2.weight [64]\n",
      "layer2.1.bn2.bias [64]\n",
      "layer3.0.conv1.weight [128, 64, 3, 3]\n",
      "layer3.0.bn1.weight [128]\n",
      "layer3.0.bn1.bias [128]\n",
      "layer3.0.conv2.weight [128, 128, 3, 3]\n",
      "layer3.0.bn2.weight [128]\n",
      "layer3.0.bn2.bias [128]\n",
      "layer3.0.downsample.0.weight [128, 64, 1, 1]\n",
      "layer3.0.downsample.1.weight [128]\n",
      "layer3.0.downsample.1.bias [128]\n",
      "layer3.1.conv1.weight [128, 128, 3, 3]\n",
      "layer3.1.bn1.weight [128]\n",
      "layer3.1.bn1.bias [128]\n",
      "layer3.1.conv2.weight [128, 128, 3, 3]\n",
      "layer3.1.bn2.weight [128]\n",
      "layer3.1.bn2.bias [128]\n",
      "layer4.0.conv1.weight [256, 128, 3, 3]\n",
      "layer4.0.bn1.weight [256]\n",
      "layer4.0.bn1.bias [256]\n",
      "layer4.0.conv2.weight [256, 256, 3, 3]\n",
      "layer4.0.bn2.weight [256]\n",
      "layer4.0.bn2.bias [256]\n",
      "layer4.0.downsample.0.weight [256, 128, 1, 1]\n",
      "layer4.0.downsample.1.weight [256]\n",
      "layer4.0.downsample.1.bias [256]\n",
      "layer4.1.conv1.weight [256, 256, 3, 3]\n",
      "layer4.1.bn1.weight [256]\n",
      "layer4.1.bn1.bias [256]\n",
      "layer4.1.conv2.weight [256, 256, 3, 3]\n",
      "layer4.1.bn2.weight [256]\n",
      "layer4.1.bn2.bias [256]\n",
      "fc.weight [1000, 256]\n",
      "fc.bias [1000]\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, list(param.data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 3\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m summary(model, (\u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchstat\\model_hook.py:47\u001b[0m, in \u001b[0;36mModelHook._sub_module_call_hook.<locals>.wrap_call\u001b[1;34m(module, *input, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39massert\u001b[39;00m module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_origin_call\n\u001b[0;32m     46\u001b[0m \u001b[39m# Itemsize for memory\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m itemsize \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitemsize\n\u001b[0;32m     49\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     50\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_origin_call[module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m](module, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.cpu()\n",
    "\n",
    "# summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'summary.<locals>.register_hook.<locals>.hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 4\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# model.zero_grad() # We don't want to store gradient information\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# torch.save(model, 'model.pth') # without .state_dict\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# model = torch.load('model.pth') # load the pruned model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# save the pruned state_dict, which includes both pruned parameters and modified attributes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m state_dict \u001b[39m=\u001b[39m tp\u001b[39m.\u001b[39mstate_dict(model) \u001b[39m# the pruned model, e.g., a resnet-18-half\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(state_dict, \u001b[39m'\u001b[39;49m\u001b[39mpruned.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# create a new model, e.g. resnet18\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m new_model \u001b[39m=\u001b[39m resnet18()\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\serialization.py:653\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    651\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[0;32m    652\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[1;32m--> 653\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m    654\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    655\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'summary.<locals>.register_hook.<locals>.hook'"
     ]
    }
   ],
   "source": [
    "# model.zero_grad() # We don't want to store gradient information\n",
    "# torch.save(model, 'model.pth') # without .state_dict\n",
    "# model = torch.load('model.pth') # load the pruned model\n",
    "\n",
    "# save the pruned state_dict, which includes both pruned parameters and modified attributes\n",
    "state_dict = tp.state_dict(model) # the pruned model, e.g., a resnet-18-half\n",
    "torch.save(state_dict, 'pruned.pth')\n",
    "\n",
    "# create a new model, e.g. resnet18\n",
    "new_model = resnet18().eval()\n",
    "\n",
    "# load the pruned state_dict into the unpruned model.\n",
    "loaded_state_dict = torch.load('pruned.pth', map_location='cpu')\n",
    "tp.load_state_dict(new_model, state_dict=loaded_state_dict)\n",
    "print(new_model) # This will be a pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 2\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmodels\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# model = models.resnet50()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m stat(model, input_size\u001b[39m=\u001b[39;49mexample_inputs)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchstat\\statistics.py:70\u001b[0m, in \u001b[0;36mstat\u001b[1;34m(model, input_size, query_granularity)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstat\u001b[39m(model, input_size, query_granularity\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m     ms \u001b[39m=\u001b[39m ModelStat(model, input_size, query_granularity)\n\u001b[0;32m     71\u001b[0m     ms\u001b[39m.\u001b[39mshow_report()\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchstat\\statistics.py:51\u001b[0m, in \u001b[0;36mModelStat.__init__\u001b[1;34m(self, model, input_size, query_granularity)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, input_size, query_granularity\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     50\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, nn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m---> 51\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(input_size, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(input_size) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m model\n\u001b[0;32m     53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_size \u001b[39m=\u001b[39m input_size\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchstat import stat\n",
    "import torchvision.models as models\n",
    "\n",
    "# model = models.resnet50()\n",
    "stat(model, input_size=example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "Shape: torch.Size([16, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, -32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 208, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 208, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 464, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 464, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 464, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 464, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 464, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "\n",
    "model = resnet18(pretrained=True).eval()\n",
    "\n",
    "# 1. build dependency graph for resnet18\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224))\n",
    "\n",
    "# # 2. Specify the to-be-pruned channels. Here we prune those channels indexed by [2, 6, 9].\n",
    "# group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[1] )\n",
    "\n",
    "# # 3. prune all grouped layers that are coupled with model.conv1 (included).\n",
    "# if DG.check_pruning_group(group): # avoid full pruning, i.e., channels=0.\n",
    "#     group.prune()\n",
    "\n",
    "\n",
    "# prune of channel 1 happens from start of conv.weights 6 groups for each channel\n",
    "channels_to_prune = [x for x in range(48)]\n",
    "tp.prune_conv_out_channels( model.conv1, idxs = channels_to_prune )\n",
    "tp.prune_batchnorm_out_channels( model.bn1, idxs= channels_to_prune )\n",
    "tp.prune_conv_in_channels(model.layer1[0].conv1, idxs= channels_to_prune )\n",
    "\n",
    "print(\"Shape:\", model.conv1.weight.shape)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        tp.prune_conv_out_channels( module, idxs = channels_to_prune )\n",
    "                \n",
    "\n",
    "\n",
    "# stat(model, (3, 224, 224))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "# print(\"Pruning step:\", current_step, \"multiply–accumulate (macs):\", macs, \"number of parameters\", nparams)\n",
    "# tp.prune_conv_out_channels( model.conv1, idxs=[0] )\n",
    "\n",
    "# tp.prune_batchnorm_out_channels( model.bn1, idxs=[1] )\n",
    "\n",
    "# tp.prune_conv_in_channels( model.layer1[0].conv1, idxs=[1] )\n",
    "\n",
    "# model\n",
    "# model.conv1.weight.shape\n",
    "# 1 , 3 , 7, 7\n",
    "model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "conv1 [2, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(pretrained=True).eval()\n",
    "example_inputs = torch.randn(1,3,224,224)\n",
    "\n",
    "# 1. build dependency graph for resnet18\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "# 2. Select some channels to prune. Here we prune the channels indexed by [2, 6, 9].\n",
    "pruning_idxs = pruning_idxs=[2, 6, 9]\n",
    "group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "\n",
    "group.prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(61, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(61, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.0.conv1 [2, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "# 2. Select some channels to prune. Here we prune the channels indexed by [2, 6, 9].\n",
    "pruning_idxs = pruning_idxs=[2, 6, 9]\n",
    "group = DG.get_pruning_group(model.layer1[0].conv1, tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "group.prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(61, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(61, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.015625,\n",
       " 0.03125,\n",
       " 0.046875,\n",
       " 0.0625,\n",
       " 0.078125,\n",
       " 0.09375,\n",
       " 0.109375,\n",
       " 0.125,\n",
       " 0.140625,\n",
       " 0.15625,\n",
       " 0.171875,\n",
       " 0.1875,\n",
       " 0.203125,\n",
       " 0.21875,\n",
       " 0.234375,\n",
       " 0.25,\n",
       " 0.265625,\n",
       " 0.28125,\n",
       " 0.296875,\n",
       " 0.3125,\n",
       " 0.328125,\n",
       " 0.34375,\n",
       " 0.359375,\n",
       " 0.375,\n",
       " 0.390625,\n",
       " 0.40625,\n",
       " 0.421875,\n",
       " 0.4375,\n",
       " 0.453125,\n",
       " 0.46875,\n",
       " 0.484375,\n",
       " 0.5,\n",
       " 0.515625,\n",
       " 0.53125,\n",
       " 0.546875,\n",
       " 0.5625,\n",
       " 0.578125,\n",
       " 0.59375,\n",
       " 0.609375,\n",
       " 0.625,\n",
       " 0.640625,\n",
       " 0.65625,\n",
       " 0.671875,\n",
       " 0.6875,\n",
       " 0.703125,\n",
       " 0.71875,\n",
       " 0.734375,\n",
       " 0.75,\n",
       " 0.765625,\n",
       " 0.78125,\n",
       " 0.796875,\n",
       " 0.8125,\n",
       " 0.828125,\n",
       " 0.84375,\n",
       " 0.859375,\n",
       " 0.875,\n",
       " 0.890625,\n",
       " 0.90625,\n",
       " 0.921875,\n",
       " 0.9375,\n",
       " 0.953125,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 1.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_amounts = [x / 64 for x in range(65)]\n",
    "prune_amounts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
