{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\35679/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, AlexNet\n",
    "import torch_pruning as tp\n",
    "from torchstat import stat\n",
    "from torchinfo import summary\n",
    "import os\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filepath, name):\n",
    "    torch.save(state, os.path.join(filepath, name+'checkpoint.pth'))\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.4 [0, 1, 2, 3828, 4, 3830, 6, 7, 8, 9, 10, 11, 3831]\n",
      "classifier.1 [0, 1, 2, 2828, 2829, 5, 2830, 2831, 8, 2833, 2834, 2836, 12]\n",
      "features.10 [0]\n",
      "features.8 [26]\n",
      "features.6 [48, 83]\n",
      "features.3 [68]\n",
      "features.0 [36]\n",
      "Pruning step: 1 multiply–accumulate (macs): 706115361.0 number of parameters 60688408\n",
      "classifier.4 [510, 3814, 3815, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "classifier.1 [416, 2471, 2470, 2469, 2487, 2466, 2486, 7, 2465, 9, 2485, 11, 12]\n",
      "features.10 [62]\n",
      "features.8 [251]\n",
      "features.6 [15]\n",
      "features.3 [102]\n",
      "Pruning step: 2 multiply–accumulate (macs): 701967322.0 number of parameters 60286501\n",
      "classifier.4 [508, 3801, 3802, 3, 4, 5, 6, 7, 8, 9, 10, 11, 3803]\n",
      "classifier.1 [508, 2795, 2797, 2798, 2800, 2801, 2806, 2811, 2813, 2814, 2815, 2817, 12]\n",
      "features.10 [62]\n",
      "features.8 [249]\n",
      "features.6 [42]\n",
      "Pruning step: 3 multiply–accumulate (macs): 699558025.0 number of parameters 59890918\n",
      "classifier.4 [507, 3789, 3790, 3791, 4, 5, 6, 7, 8, 9, 3793, 11, 12]\n",
      "classifier.1 [2778, 2781, 2782, 2783, 2786, 2788, 2789, 2790, 2791, 2792, 2805, 2808, 2810]\n",
      "features.10 [62]\n",
      "features.8 [207]\n",
      "features.6 [119]\n",
      "features.3 [154]\n",
      "Pruning step: 4 multiply–accumulate (macs): 695429265.0 number of parameters 59491658\n",
      "classifier.4 [505, 3775, 3776, 3778, 3779, 5, 6, 7, 8, 9, 10, 11]\n",
      "classifier.1 [505, 2765, 2766, 2767, 2768, 2769, 2770, 2772, 2773, 2774, 2775, 2777]\n",
      "features.6 [257]\n",
      "Pruning step: 5 multiply–accumulate (macs): 694540342.0 number of parameters 59269888\n"
     ]
    }
   ],
   "source": [
    "# Importance criteria\n",
    "example_inputs = torch.randn(1, 3, 224, 224)\n",
    "imp = tp.importance.TaylorImportance()\n",
    "\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 1000:\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "iterative_steps = 5 # progressive pruning\n",
    "current_step = 1\n",
    "\n",
    "\n",
    "# 0.015625 -> 1 0.03125 -> 2 0.0625 -> 4 0.125 -> 8 0.25 -> 16 0.5 -> 32\n",
    "pruner = tp.pruner.MagnitudePruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    round_to=None,\n",
    "    unwrapped_parameters=None,\n",
    "    importance=imp,\n",
    "    iterative_steps=iterative_steps,\n",
    "    ch_sparsity = 0.015625, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "    ignored_layers=ignored_layers,\n",
    ")\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "\n",
    "\n",
    "for i in range(iterative_steps):\n",
    "    if isinstance(imp, tp.importance.TaylorImportance):\n",
    "        # Taylor expansion requires gradients for importance estimation\n",
    "        loss = model(example_inputs).sum() # a dummy loss for TaylorImportance\n",
    "        loss.backward() # before pruner.step()\n",
    "    pruner.step()\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(\"Pruning step:\", current_step, \"multiply–accumulate (macs):\", macs, \"number of parameters\", nparams)\n",
    "    current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, param in enumerate(model.features[0].parameters()):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, param in enumerate(model.conv.parameters()):\n",
    "    if idx in feature_map_indices_to_freeze:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MagnitudePruner' object has no attribute 'get_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\torchpruningtest.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/torchpruningtest.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pruner\u001b[39m.\u001b[39;49mget_history()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MagnitudePruner' object has no attribute 'get_history'"
     ]
    }
   ],
   "source": [
    "pruner.get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # finetune your model here\n",
    "    # finetune(model)\n",
    "    # ...\n",
    "# save the pruned state_dict, which includes both pruned parameters and modified attributes\n",
    "state_dict = tp.state_dict(model) # the pruned model, e.g., a resnet-18-half\n",
    "torch.save(state_dict, 'pruned.pth')\n",
    "\n",
    "# create a new model, e.g. resnet18\n",
    "new_model = resnet18().eval()\n",
    "\n",
    "# load the pruned state_dict into the unpruned model.\n",
    "loaded_state_dict = torch.load('pruned.pth', map_location='cpu')\n",
    "tp.load_state_dict(new_model, state_dict=loaded_state_dict)\n",
    "# print(new_model) # This will be a pruned model.\n",
    "\n",
    "summary(new_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight [32, 3, 7, 7]\n",
      "bn1.weight [32]\n",
      "bn1.bias [32]\n",
      "layer1.0.conv1.weight [32, 32, 3, 3]\n",
      "layer1.0.bn1.weight [32]\n",
      "layer1.0.bn1.bias [32]\n",
      "layer1.0.conv2.weight [32, 32, 3, 3]\n",
      "layer1.0.bn2.weight [32]\n",
      "layer1.0.bn2.bias [32]\n",
      "layer1.1.conv1.weight [32, 32, 3, 3]\n",
      "layer1.1.bn1.weight [32]\n",
      "layer1.1.bn1.bias [32]\n",
      "layer1.1.conv2.weight [32, 32, 3, 3]\n",
      "layer1.1.bn2.weight [32]\n",
      "layer1.1.bn2.bias [32]\n",
      "layer2.0.conv1.weight [64, 32, 3, 3]\n",
      "layer2.0.bn1.weight [64]\n",
      "layer2.0.bn1.bias [64]\n",
      "layer2.0.conv2.weight [64, 64, 3, 3]\n",
      "layer2.0.bn2.weight [64]\n",
      "layer2.0.bn2.bias [64]\n",
      "layer2.0.downsample.0.weight [64, 32, 1, 1]\n",
      "layer2.0.downsample.1.weight [64]\n",
      "layer2.0.downsample.1.bias [64]\n",
      "layer2.1.conv1.weight [64, 64, 3, 3]\n",
      "layer2.1.bn1.weight [64]\n",
      "layer2.1.bn1.bias [64]\n",
      "layer2.1.conv2.weight [64, 64, 3, 3]\n",
      "layer2.1.bn2.weight [64]\n",
      "layer2.1.bn2.bias [64]\n",
      "layer3.0.conv1.weight [128, 64, 3, 3]\n",
      "layer3.0.bn1.weight [128]\n",
      "layer3.0.bn1.bias [128]\n",
      "layer3.0.conv2.weight [128, 128, 3, 3]\n",
      "layer3.0.bn2.weight [128]\n",
      "layer3.0.bn2.bias [128]\n",
      "layer3.0.downsample.0.weight [128, 64, 1, 1]\n",
      "layer3.0.downsample.1.weight [128]\n",
      "layer3.0.downsample.1.bias [128]\n",
      "layer3.1.conv1.weight [128, 128, 3, 3]\n",
      "layer3.1.bn1.weight [128]\n",
      "layer3.1.bn1.bias [128]\n",
      "layer3.1.conv2.weight [128, 128, 3, 3]\n",
      "layer3.1.bn2.weight [128]\n",
      "layer3.1.bn2.bias [128]\n",
      "layer4.0.conv1.weight [256, 128, 3, 3]\n",
      "layer4.0.bn1.weight [256]\n",
      "layer4.0.bn1.bias [256]\n",
      "layer4.0.conv2.weight [256, 256, 3, 3]\n",
      "layer4.0.bn2.weight [256]\n",
      "layer4.0.bn2.bias [256]\n",
      "layer4.0.downsample.0.weight [256, 128, 1, 1]\n",
      "layer4.0.downsample.1.weight [256]\n",
      "layer4.0.downsample.1.bias [256]\n",
      "layer4.1.conv1.weight [256, 256, 3, 3]\n",
      "layer4.1.bn1.weight [256]\n",
      "layer4.1.bn1.bias [256]\n",
      "layer4.1.conv2.weight [256, 256, 3, 3]\n",
      "layer4.1.bn2.weight [256]\n",
      "layer4.1.bn2.bias [256]\n",
      "fc.weight [1000, 256]\n",
      "fc.bias [1000]\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, list(param.data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 3\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m summary(model, (\u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[0;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchstat\\model_hook.py:47\u001b[0m, in \u001b[0;36mModelHook._sub_module_call_hook.<locals>.wrap_call\u001b[1;34m(module, *input, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39massert\u001b[39;00m module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_origin_call\n\u001b[0;32m     46\u001b[0m \u001b[39m# Itemsize for memory\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m itemsize \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitemsize\n\u001b[0;32m     49\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     50\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_origin_call[module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m](module, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.cpu()\n",
    "\n",
    "# summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'summary.<locals>.register_hook.<locals>.hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 4\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# model.zero_grad() # We don't want to store gradient information\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# torch.save(model, 'model.pth') # without .state_dict\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# model = torch.load('model.pth') # load the pruned model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# save the pruned state_dict, which includes both pruned parameters and modified attributes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m state_dict \u001b[39m=\u001b[39m tp\u001b[39m.\u001b[39mstate_dict(model) \u001b[39m# the pruned model, e.g., a resnet-18-half\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(state_dict, \u001b[39m'\u001b[39;49m\u001b[39mpruned.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# create a new model, e.g. resnet18\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m new_model \u001b[39m=\u001b[39m resnet18()\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch\\serialization.py:653\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    651\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[0;32m    652\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[1;32m--> 653\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m    654\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    655\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'summary.<locals>.register_hook.<locals>.hook'"
     ]
    }
   ],
   "source": [
    "# model.zero_grad() # We don't want to store gradient information\n",
    "# torch.save(model, 'model.pth') # without .state_dict\n",
    "# model = torch.load('model.pth') # load the pruned model\n",
    "\n",
    "# save the pruned state_dict, which includes both pruned parameters and modified attributes\n",
    "state_dict = tp.state_dict(model) # the pruned model, e.g., a resnet-18-half\n",
    "torch.save(state_dict, 'pruned.pth')\n",
    "\n",
    "# create a new model, e.g. resnet18\n",
    "new_model = resnet18().eval()\n",
    "\n",
    "# load the pruned state_dict into the unpruned model.\n",
    "loaded_state_dict = torch.load('pruned.pth', map_location='cpu')\n",
    "tp.load_state_dict(new_model, state_dict=loaded_state_dict)\n",
    "print(new_model) # This will be a pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\hope.ipynb Cell 2\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmodels\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# model = models.resnet50()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/hope.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m stat(model, input_size\u001b[39m=\u001b[39;49mexample_inputs)\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchstat\\statistics.py:70\u001b[0m, in \u001b[0;36mstat\u001b[1;34m(model, input_size, query_granularity)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstat\u001b[39m(model, input_size, query_granularity\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m     ms \u001b[39m=\u001b[39m ModelStat(model, input_size, query_granularity)\n\u001b[0;32m     71\u001b[0m     ms\u001b[39m.\u001b[39mshow_report()\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchstat\\statistics.py:51\u001b[0m, in \u001b[0;36mModelStat.__init__\u001b[1;34m(self, model, input_size, query_granularity)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, input_size, query_granularity\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     50\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, nn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m---> 51\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(input_size, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(input_size) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m model\n\u001b[0;32m     53\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_size \u001b[39m=\u001b[39m input_size\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchstat import stat\n",
    "import torchvision.models as models\n",
    "\n",
    "# model = models.resnet50()\n",
    "stat(model, input_size=example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "Shape: torch.Size([16, 3, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, -32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 208, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 208, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 464, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 464, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 464, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 464, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 464, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "\n",
    "model = resnet18(pretrained=True).eval()\n",
    "\n",
    "# 1. build dependency graph for resnet18\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224))\n",
    "\n",
    "# # 2. Specify the to-be-pruned channels. Here we prune those channels indexed by [2, 6, 9].\n",
    "# group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[1] )\n",
    "\n",
    "# # 3. prune all grouped layers that are coupled with model.conv1 (included).\n",
    "# if DG.check_pruning_group(group): # avoid full pruning, i.e., channels=0.\n",
    "#     group.prune()\n",
    "\n",
    "\n",
    "# prune of channel 1 happens from start of conv.weights 6 groups for each channel\n",
    "channels_to_prune = [x for x in range(48)]\n",
    "tp.prune_conv_out_channels( model.conv1, idxs = channels_to_prune )\n",
    "tp.prune_batchnorm_out_channels( model.bn1, idxs= channels_to_prune )\n",
    "tp.prune_conv_in_channels(model.layer1[0].conv1, idxs= channels_to_prune )\n",
    "\n",
    "print(\"Shape:\", model.conv1.weight.shape)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        tp.prune_conv_out_channels( module, idxs = channels_to_prune )\n",
    "                \n",
    "\n",
    "\n",
    "# stat(model, (3, 224, 224))\n",
    "# macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "# print(\"Pruning step:\", current_step, \"multiply–accumulate (macs):\", macs, \"number of parameters\", nparams)\n",
    "# tp.prune_conv_out_channels( model.conv1, idxs=[0] )\n",
    "\n",
    "# tp.prune_batchnorm_out_channels( model.bn1, idxs=[1] )\n",
    "\n",
    "# tp.prune_conv_in_channels( model.layer1[0].conv1, idxs=[1] )\n",
    "\n",
    "# model\n",
    "# model.conv1.weight.shape\n",
    "# 1 , 3 , 7, 7\n",
    "model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Module Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) is not in the dependency graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\35679\\Documents\\GitHub\\msc-thesis-self-adaptive-neural-networks\\notebooks\\torchpruningtest.ipynb Cell 11\u001b[0m in \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/torchpruningtest.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m resnet18(pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/torchpruningtest.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pruning_idxs \u001b[39m=\u001b[39m pruning_idxs\u001b[39m=\u001b[39m[\u001b[39m2\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m9\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/torchpruningtest.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m group \u001b[39m=\u001b[39m DG\u001b[39m.\u001b[39;49mget_pruning_group(model\u001b[39m.\u001b[39;49mconv1, tp\u001b[39m.\u001b[39;49mprune_conv_out_channels, idxs\u001b[39m=\u001b[39;49mpruning_idxs )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/35679/Documents/GitHub/msc-thesis-self-adaptive-neural-networks/notebooks/torchpruningtest.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m group\u001b[39m.\u001b[39mprune()\n",
      "File \u001b[1;32mc:\\Users\\35679\\anaconda3\\lib\\site-packages\\torch_pruning\\dependency.py:449\u001b[0m, in \u001b[0;36mDependencyGraph.get_pruning_group\u001b[1;34m(self, module, pruning_fn, idxs)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39m\"\"\"Get the pruning group of pruning_fn.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m    module (nn.Module): the to-be-pruned module/layer.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39m    pruning_fn (Callable): the pruning function.\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m    idxs (list or tuple): the indices of channels/dimensions.\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m module \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule2node:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModule \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not in the dependency graph.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(module)\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, ops\u001b[39m.\u001b[39mTORCH_CONV) \u001b[39mand\u001b[39;00m module\u001b[39m.\u001b[39mgroups \u001b[39m==\u001b[39m module\u001b[39m.\u001b[39mout_channels \u001b[39mand\u001b[39;00m module\u001b[39m.\u001b[39mout_channels\u001b[39m>\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m     pruning_fn \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mprune_depthwise_conv_out_channels\n",
      "\u001b[1;31mValueError\u001b[0m: Module Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) is not in the dependency graph."
     ]
    }
   ],
   "source": [
    "# 2. Select some channels to prune. Here we prune the channels indexed by [2, 6, 9].\n",
    "model = resnet18(pretrained=True).eval()\n",
    "pruning_idxs = pruning_idxs=[2, 6, 9]\n",
    "group = DG.get_pruning_group(model.conv1, tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "group.prune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 [2, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "DG.pruning_history()\n",
    "\n",
    "for name, bl, ids in DG.pruning_history():\n",
    "    print(name, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(61, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(61, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.015625,\n",
       " 0.03125,\n",
       " 0.046875,\n",
       " 0.0625,\n",
       " 0.078125,\n",
       " 0.09375,\n",
       " 0.109375,\n",
       " 0.125,\n",
       " 0.140625,\n",
       " 0.15625,\n",
       " 0.171875,\n",
       " 0.1875,\n",
       " 0.203125,\n",
       " 0.21875,\n",
       " 0.234375,\n",
       " 0.25,\n",
       " 0.265625,\n",
       " 0.28125,\n",
       " 0.296875,\n",
       " 0.3125,\n",
       " 0.328125,\n",
       " 0.34375,\n",
       " 0.359375,\n",
       " 0.375,\n",
       " 0.390625,\n",
       " 0.40625,\n",
       " 0.421875,\n",
       " 0.4375,\n",
       " 0.453125,\n",
       " 0.46875,\n",
       " 0.484375,\n",
       " 0.5,\n",
       " 0.515625,\n",
       " 0.53125,\n",
       " 0.546875,\n",
       " 0.5625,\n",
       " 0.578125,\n",
       " 0.59375,\n",
       " 0.609375,\n",
       " 0.625,\n",
       " 0.640625,\n",
       " 0.65625,\n",
       " 0.671875,\n",
       " 0.6875,\n",
       " 0.703125,\n",
       " 0.71875,\n",
       " 0.734375,\n",
       " 0.75,\n",
       " 0.765625,\n",
       " 0.78125,\n",
       " 0.796875,\n",
       " 0.8125,\n",
       " 0.828125,\n",
       " 0.84375,\n",
       " 0.859375,\n",
       " 0.875,\n",
       " 0.890625,\n",
       " 0.90625,\n",
       " 0.921875,\n",
       " 0.9375,\n",
       " 0.953125,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 1.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_amounts = [x / 64 for x in range(65)]\n",
    "prune_amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 [2, 6, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\35679\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[GroupItem(dep=prune_out_channels on conv1 (Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)) => prune_out_channels on conv1 (Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on conv1 (Conv2d(3, 61, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)) => prune_out_channels on bn1 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on bn1 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on _ElementWiseOp_20(ReluBackward0), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_20(ReluBackward0) => prune_out_channels on _ElementWiseOp_19(MaxPool2DWithIndicesBackward0), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_19(MaxPool2DWithIndicesBackward0) => prune_out_channels on _ElementWiseOp_18(AddBackward0), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_19(MaxPool2DWithIndicesBackward0) => prune_in_channels on layer1.0.conv1 (Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_18(AddBackward0) => prune_out_channels on layer1.0.bn2 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_18(AddBackward0) => prune_out_channels on _ElementWiseOp_17(ReluBackward0), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_17(ReluBackward0) => prune_out_channels on _ElementWiseOp_16(AddBackward0), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_17(ReluBackward0) => prune_in_channels on layer1.1.conv1 (Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_16(AddBackward0) => prune_out_channels on layer1.1.bn2 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_16(AddBackward0) => prune_out_channels on _ElementWiseOp_15(ReluBackward0), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_15(ReluBackward0) => prune_in_channels on layer2.0.downsample.0 (Conv2d(61, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on _ElementWiseOp_15(ReluBackward0) => prune_in_channels on layer2.0.conv1 (Conv2d(61, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on layer1.1.bn2 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer1.1.conv2 (Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), idxs=[2, 6, 9]),\n",
       " GroupItem(dep=prune_out_channels on layer1.0.bn2 (BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) => prune_out_channels on layer1.0.conv2 (Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)), idxs=[2, 6, 9])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18(pretrained=True).eval()\n",
    "example_inputs = torch.randn(1,3,224,224)\n",
    "pruned_layer = model.conv1\n",
    "\n",
    "# 1. build dependency graph for resnet18\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "# 2. Select some channels to prune. Here we prune the channels indexed by [2, 6, 9].\n",
    "pruning_idxs = pruning_idxs=[2, 6, 9]\n",
    "group = DG.get_pruning_group(model.conv1, tp.prune_conv_out_channels, idxs=pruning_idxs )\n",
    "\n",
    "group.prune()\n",
    "\n",
    "group.items\n",
    "\n",
    "group = DG.get_pruning_group(model.conv1, tp.prune_conv_out_channels, idxs=pruning_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you have a model with the pruned convolutional layer and dependent layers\n",
    "\n",
    "# Get the pruned convolutional layer\n",
    "pruned_layer = model.pruned_layer\n",
    "\n",
    "# Get the number of filters in the pruned layer\n",
    "num_filters_pruned = pruned_layer.out_channels\n",
    "\n",
    "# Define the channel positions to add\n",
    "channel_positions_to_add = [0, 2, 5, 8]  # Adjust this list as needed\n",
    "\n",
    "# Calculate the number of filters to be added\n",
    "new_filters = len(channel_positions_to_add)\n",
    "\n",
    "# Create a new convolutional layer with the desired number of filters\n",
    "new_conv_layer = nn.Conv2d(in_channels=num_filters_pruned + new_filters,\n",
    "                           out_channels=num_filters_pruned,\n",
    "                           kernel_size=pruned_layer.kernel_size,\n",
    "                           stride=pruned_layer.stride,\n",
    "                           padding=pruned_layer.padding,\n",
    "                           bias=pruned_layer.bias is not None)\n",
    "\n",
    "# Copy the weights from the pruned layer to the new layer\n",
    "new_conv_layer.weight.data[:, :num_filters_pruned, :, :] = pruned_layer.weight.data.clone()\n",
    "\n",
    "if pruned_layer.bias is not None:\n",
    "    new_conv_layer.bias.data = pruned_layer.bias.data.clone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
