(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.4 --global-pruning --total-epochs 80
Files already downloaded and verified
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: mode: prune
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: model: resnet56
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: verbose: False
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: method: group_norm
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: sparsity: 0.4
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: seed: None
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: device: cuda
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/09 23:09:29 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/09 23:09:56 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 [3, 4, 5, 6, 7, 10, 11, 15, 16, 18, 23, 25, 27, 28, 29, 30, 32, 33, 34, 47, 54, 57, 60]
layer2.0.downsample.0 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
conv1 [1, 4, 6, 7, 8, 9, 10, 13, 15]
layer1.0.conv1 [0, 3, 5, 6, 7, 15]
layer1.1.conv1 [0, 3, 5, 6, 7, 12, 13]
layer1.2.conv1 [0, 3, 6, 8, 9, 11, 13, 14]
layer1.3.conv1 [3, 4, 5, 6, 10, 11, 12]
layer1.4.conv1 [1, 2, 6, 7, 8, 11]
layer1.5.conv1 [0, 2, 6, 11, 13]
layer1.6.conv1 [0, 1, 2, 4, 5, 7, 13, 14]
layer1.7.conv1 [1, 3, 5, 7, 9, 12]
layer1.8.conv1 [0, 5, 6, 7, 8, 9, 11, 12, 15]
layer2.0.conv1 [1, 3, 5, 8, 9, 11, 13, 20, 26, 27, 30, 31]
layer2.1.conv1 [1, 2, 3, 4, 11, 16, 19, 22, 25, 26, 29, 30, 31]
layer2.2.conv1 [8, 12, 13, 16, 20, 21, 23, 26, 27, 30]
layer2.3.conv1 [11, 12, 17, 18, 20, 26, 27, 28]
layer2.4.conv1 [0, 1, 5, 8, 9, 15, 16, 18, 22, 23, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29, 31]
layer2.6.conv1 [2, 4, 5, 6, 7, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 [1, 2, 4, 6, 7, 9, 10, 12, 22, 26, 29, 34, 36, 38, 39, 40, 42, 44, 46, 47, 49, 50, 51, 57, 58, 60]
layer3.1.conv1 [2, 3, 5, 10, 11, 21, 23, 25, 27, 28, 29, 33, 37, 41, 45, 46, 55, 56, 57, 60, 61, 63]
layer3.2.conv1 [2, 3, 6, 11, 20, 27, 29, 30, 31, 33, 36, 38, 42, 43, 44, 45, 48, 49, 57]
layer3.3.conv1 [2, 4, 6, 7, 8, 9, 10, 15, 16, 21, 24, 30, 31, 33, 39, 40, 44, 46, 47, 49, 55, 58, 60, 61]
layer3.4.conv1 [0, 3, 14, 18, 20, 26, 27, 30, 31, 32, 35, 41, 42, 43, 45, 49, 50, 51, 52, 54, 59, 60]
layer3.5.conv1 [0, 1, 3, 5, 6, 8, 9, 10, 12, 14, 18, 19, 20, 26, 28, 38, 40, 45, 46, 49, 52, 59, 60, 63]
layer3.6.conv1 [3, 9, 12, 14, 22, 24, 26, 31, 37, 39, 45, 47, 49, 56, 57, 60, 61, 63]
layer3.7.conv1 [0, 2, 4, 8, 9, 18, 19, 21, 23, 24, 27, 31, 33, 34, 36, 37, 39, 42, 46, 50, 51, 52, 54, 55, 57, 59, 63]
layer3.8.conv1 [0, 4, 5, 17, 18, 23, 24, 31, 34, 35, 36, 38, 40, 41, 42, 43, 45, 46, 51, 55, 56, 57, 59, 60, 62]
=> Start history generation
layer3.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 [3, 4, 5, 6, 7, 10, 11, 15, 16, 18, 23, 25, 27, 28, 29, 30, 32, 33, 34, 47, 54, 57, 60]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 [3, 4, 5, 6, 7, 10, 11, 15, 16, 18, 23, 25, 27, 28, 29, 30, 32, 33, 34, 47, 54, 57, 60]
layer3.2.conv1 [3, 4, 5, 6, 7, 10, 11, 15, 16, 18, 23, 25, 27, 28, 29, 30, 32, 33, 34, 47, 54, 57, 60]
layer3.1.conv1 [3, 4, 5, 6, 7, 10, 11, 15, 16, 18, 23, 25, 27, 28, 29, 30, 32, 33, 34, 47, 54, 57, 60]
layer3.0.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.8.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.7.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.6.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.5.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31]
layer2.3.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.2.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.1.conv1 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
layer2.0.conv1 [1, 4, 6, 7, 8, 9, 10, 13, 15]
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 [1, 4, 6, 7, 8, 9, 10, 13, 15]
layer1.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 [1, 4, 6, 7, 8, 9, 10, 13, 15]
layer1.3.conv1 [1, 4, 6, 7, 8, 9, 10, 13, 15]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer3.0.downsample.0 [0, 1, 7, 8, 9, 11, 12, 13, 14, 18, 22, 25, 29]
[08/09 23:09:59 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 19, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(19, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(19, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(19, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(19, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(19, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(19, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(19, 38, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(38, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(19, 41, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(41, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(41, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(45, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(41, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(41, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(41, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(41, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(41, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(41, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(39, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=41, out_features=10, bias=True)
)
[08/09 23:09:59 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/09 23:10:09 cifar10-global-group_norm-resnet56]: Acc: 0.0996 Val Loss: 5.8944

[08/09 23:10:09 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 [2, 3, 4, 7, 8, 14, 16, 31, 37, 38]
layer2.0.downsample.0 [2, 3, 6, 7, 10, 14, 15]
conv1 [0, 1, 3, 6]
layer1.1.conv1 [3, 5, 6]
layer1.2.conv1 [0, 1, 5]
layer1.3.conv1 [2, 3, 4, 7, 8]
layer1.5.conv1 [0, 5, 7, 8, 10]
layer1.6.conv1 [1, 2, 3, 4]
layer1.8.conv1 [1, 5]
layer2.0.conv1 [0, 3, 4, 7, 12, 14, 19]
layer2.1.conv1 [3, 4, 6, 8, 9, 11, 12, 14, 15, 16, 17]
layer2.2.conv1 [0, 6, 8, 9, 10, 11, 17, 21]
layer2.3.conv1 [1, 5, 7, 8, 9, 11, 17, 19, 20, 21]
layer2.4.conv1 [2, 5, 6, 7, 11, 13, 14, 15, 17, 19]
layer2.5.conv1 [0, 1, 2, 3, 5, 6, 8]
layer2.6.conv1 [1, 4, 7, 9, 11, 12, 13]
layer2.7.conv1 [0, 2, 3, 5, 8, 9, 10]
layer2.8.conv1 [1, 2, 5]
layer3.0.conv1 [2, 6, 9, 10, 13, 16, 18, 21, 27, 32, 34, 36]
layer3.1.conv1 [1, 3, 9, 15, 19, 20, 21, 23, 25, 27, 28, 29, 30, 33, 35, 36, 37, 40]
layer3.2.conv1 [0, 2, 6, 9, 10, 13, 14, 16, 21, 26, 28, 33, 42, 43]
layer3.3.conv1 [0, 4, 5, 6, 7, 9, 15, 16, 20, 21, 24, 25, 29, 31, 32, 38, 39]
layer3.4.conv1 [0, 2, 4, 11, 14, 15, 16, 25, 28, 29, 33, 34, 38, 41]
layer3.5.conv1 [2, 5, 7, 8, 17, 18, 21, 23, 30, 31, 33, 34, 36, 37, 39]
layer3.6.conv1 [1, 3, 8, 9, 11, 12, 14, 15, 16, 17, 18, 20, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]
layer3.7.conv1 [0, 5, 6, 8, 13, 14, 15, 20, 22, 23, 24, 27, 30, 31, 33, 34, 35]
layer3.8.conv1 [0, 2, 7, 13, 14, 16, 17, 18, 23, 24, 27, 33, 34, 35]
=> Start history generation
layer3.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
layer3.6.conv1 [2, 3, 4, 7, 8, 14, 16, 31, 37, 38]
layer3.5.conv1 [2, 3, 4, 7, 8, 14, 16, 31, 37, 38]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
layer3.3.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
layer3.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]
layer3.1.conv1 [2, 3, 4, 7, 8, 14, 16, 31, 37, 38]
layer3.0.conv1 [2, 3, 6, 7, 10, 14, 15]
layer2.8.conv1 [2, 3, 6, 7, 10, 14, 15]
layer2.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
layer2.6.conv1 [2, 3, 6, 7, 10, 14, 15]
layer2.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
layer2.4.conv1 [2, 3, 6, 7, 10, 14, 15]
layer2.3.conv1 [2, 3, 6, 7, 10, 14, 15]
layer2.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]
layer2.1.conv1 [2, 3, 6, 7, 10, 14, 15]
layer2.0.conv1 [0, 1, 2, 3, 4, 5, 6]
layer1.8.conv1 [0, 1, 3, 6]
layer1.6.conv1 [0, 1, 3, 6]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6]
layer1.3.conv1 [0, 1, 3, 6]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6]
layer1.1.conv1 [0, 1, 3, 6]
conv1 [0, 1, 2]
layer2.0.downsample.0 [0, 1, 3, 6]
layer3.0.downsample.0 [2, 3, 6, 7, 10, 14, 15]
[08/09 23:10:10 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(3, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(3, 12, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 26, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(31, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(31, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(31, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(31, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(31, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(31, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(31, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=31, out_features=10, bias=True)
)
[08/09 23:10:20 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.14 M (16.82%)
[08/09 23:10:20 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 16.02 M (12.60%, 7.94X )
[08/09 23:10:20 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.1000
[08/09 23:10:20 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 4.0872
[08/09 23:10:20 cifar10-global-group_norm-resnet56]: Finetuning...
[08/09 23:10:57 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.5455, Val Loss=1.2767, lr=0.0100
[08/09 23:11:33 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.6310, Val Loss=1.0524, lr=0.0100
[08/09 23:12:09 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.6295, Val Loss=1.0769, lr=0.0100
[08/09 23:12:46 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.6201, Val Loss=1.1266, lr=0.0100
[08/09 23:13:22 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.7485, Val Loss=0.7539, lr=0.0100
[08/09 23:13:59 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.7420, Val Loss=0.7685, lr=0.0100
[08/09 23:14:36 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.7305, Val Loss=0.8100, lr=0.0100
[08/09 23:15:13 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7554, Val Loss=0.7403, lr=0.0100
[08/09 23:15:49 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.6949, Val Loss=0.9482, lr=0.0100
[08/09 23:16:26 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7733, Val Loss=0.6924, lr=0.0100
[08/09 23:17:03 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.7722, Val Loss=0.6811, lr=0.0100
[08/09 23:17:39 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.7870, Val Loss=0.6313, lr=0.0100
[08/09 23:18:16 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7851, Val Loss=0.6275, lr=0.0100
[08/09 23:18:54 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8031, Val Loss=0.5978, lr=0.0100
[08/09 23:19:30 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8069, Val Loss=0.5727, lr=0.0100
[08/09 23:20:06 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.7500, Val Loss=0.8071, lr=0.0100
[08/09 23:20:43 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8067, Val Loss=0.5884, lr=0.0100
[08/09 23:21:19 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.7488, Val Loss=0.7957, lr=0.0100
[08/09 23:21:57 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.7909, Val Loss=0.6427, lr=0.0100
[08/09 23:22:35 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.7715, Val Loss=0.7192, lr=0.0100
[08/09 23:23:13 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8098, Val Loss=0.5737, lr=0.0100
[08/09 23:23:50 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.7962, Val Loss=0.6365, lr=0.0100
[08/09 23:24:28 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8072, Val Loss=0.5753, lr=0.0100
[08/09 23:25:06 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8049, Val Loss=0.5689, lr=0.0100
[08/09 23:25:43 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8196, Val Loss=0.5456, lr=0.0100
[08/09 23:26:20 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8216, Val Loss=0.5316, lr=0.0100
[08/09 23:26:59 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.7986, Val Loss=0.6128, lr=0.0100
[08/09 23:27:36 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.7960, Val Loss=0.6451, lr=0.0100
[08/09 23:28:19 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.7688, Val Loss=0.7376, lr=0.0100
[08/09 23:28:56 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8140, Val Loss=0.5717, lr=0.0100
[08/09 23:29:33 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.7935, Val Loss=0.6646, lr=0.0100
[08/09 23:30:12 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8110, Val Loss=0.5978, lr=0.0100
[08/09 23:30:50 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8277, Val Loss=0.5309, lr=0.0100
[08/09 23:31:27 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8211, Val Loss=0.5340, lr=0.0100
[08/09 23:32:06 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8191, Val Loss=0.5478, lr=0.0100
[08/09 23:32:44 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.7954, Val Loss=0.6456, lr=0.0100
[08/09 23:33:21 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8234, Val Loss=0.5414, lr=0.0100
[08/09 23:33:58 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8102, Val Loss=0.5634, lr=0.0100
[08/09 23:34:35 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8112, Val Loss=0.5990, lr=0.0100
[08/09 23:35:12 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8401, Val Loss=0.4909, lr=0.0100
[08/09 23:35:51 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8686, Val Loss=0.3915, lr=0.0010
[08/09 23:36:29 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8703, Val Loss=0.3886, lr=0.0010
[08/09 23:37:06 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8698, Val Loss=0.3893, lr=0.0010
[08/09 23:37:43 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8712, Val Loss=0.3917, lr=0.0010
[08/09 23:38:21 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8696, Val Loss=0.3896, lr=0.0010
[08/09 23:39:00 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8708, Val Loss=0.3936, lr=0.0010
[08/09 23:39:37 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8736, Val Loss=0.3895, lr=0.0010
[08/09 23:40:13 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8719, Val Loss=0.3911, lr=0.0010
[08/09 23:40:49 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8728, Val Loss=0.3871, lr=0.0010
[08/09 23:41:26 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8727, Val Loss=0.3923, lr=0.0010
[08/09 23:42:04 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8700, Val Loss=0.3914, lr=0.0010
[08/09 23:42:42 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8718, Val Loss=0.3991, lr=0.0010
[08/09 23:43:18 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8734, Val Loss=0.3902, lr=0.0010
[08/09 23:43:55 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8734, Val Loss=0.3898, lr=0.0010
[08/09 23:44:31 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8720, Val Loss=0.3956, lr=0.0010
[08/09 23:45:08 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8709, Val Loss=0.3990, lr=0.0010
[08/09 23:45:45 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8750, Val Loss=0.3962, lr=0.0010
[08/09 23:46:22 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8733, Val Loss=0.3970, lr=0.0010
[08/09 23:47:00 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8703, Val Loss=0.4040, lr=0.0010
[08/09 23:47:37 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8708, Val Loss=0.3998, lr=0.0010
[08/09 23:48:15 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8742, Val Loss=0.3940, lr=0.0001
[08/09 23:48:52 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8740, Val Loss=0.3932, lr=0.0001
[08/09 23:49:31 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8764, Val Loss=0.3926, lr=0.0001
[08/09 23:50:13 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8763, Val Loss=0.3916, lr=0.0001
[08/09 23:50:52 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8746, Val Loss=0.3922, lr=0.0001
[08/09 23:51:31 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8736, Val Loss=0.3910, lr=0.0001
[08/09 23:52:09 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8753, Val Loss=0.3923, lr=0.0001
[08/09 23:52:47 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8769, Val Loss=0.3929, lr=0.0001
[08/09 23:53:26 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8743, Val Loss=0.3936, lr=0.0001
[08/09 23:54:06 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8744, Val Loss=0.3945, lr=0.0001
[08/09 23:54:43 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8758, Val Loss=0.3936, lr=0.0001
[08/09 23:55:20 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8764, Val Loss=0.3924, lr=0.0001
[08/09 23:55:59 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8744, Val Loss=0.3979, lr=0.0001
[08/09 23:56:37 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8747, Val Loss=0.3953, lr=0.0001
[08/09 23:57:15 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8762, Val Loss=0.3933, lr=0.0001
[08/09 23:57:52 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8767, Val Loss=0.3914, lr=0.0001
[08/09 23:58:30 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8758, Val Loss=0.3960, lr=0.0001
[08/09 23:59:08 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8728, Val Loss=0.3972, lr=0.0001
[08/09 23:59:46 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8744, Val Loss=0.3948, lr=0.0001
[08/10 00:00:25 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.8742, Val Loss=0.3966, lr=0.0001
[08/10 00:00:25 cifar10-global-group_norm-resnet56]: Best Acc=0.8769
[08/10 00:00:25 cifar10-global-group_norm-resnet56]: Params: 0.14 M
[08/10 00:00:25 cifar10-global-group_norm-resnet56]: ops: 16.02 M
[08/10 00:00:35 cifar10-global-group_norm-resnet56]: Acc: 0.8742 Val Loss: 0.3966

[08/10 00:00:35 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 27) layer3.8.conv1 has been rebuilt.
(2 of 27) layer3.7.conv1 has been rebuilt.
(3 of 27) layer3.6.conv1 has been rebuilt.
(4 of 27) layer3.5.conv1 has been rebuilt.
(5 of 27) layer3.4.conv1 has been rebuilt.
(6 of 27) layer3.3.conv1 has been rebuilt.
(7 of 27) layer3.2.conv1 has been rebuilt.
(8 of 27) layer3.1.conv1 has been rebuilt.
(9 of 27) layer3.0.conv1 has been rebuilt.
(10 of 27) layer2.8.conv1 has been rebuilt.
(11 of 27) layer2.7.conv1 has been rebuilt.
(12 of 27) layer2.6.conv1 has been rebuilt.
(13 of 27) layer2.5.conv1 has been rebuilt.
(14 of 27) layer2.4.conv1 has been rebuilt.
(15 of 27) layer2.3.conv1 has been rebuilt.
(16 of 27) layer2.2.conv1 has been rebuilt.
(17 of 27) layer2.1.conv1 has been rebuilt.
(18 of 27) layer2.0.conv1 has been rebuilt.
(19 of 27) layer1.8.conv1 has been rebuilt.
(20 of 27) layer1.6.conv1 has been rebuilt.
(21 of 27) layer1.5.conv1 has been rebuilt.
(22 of 27) layer1.3.conv1 has been rebuilt.
(23 of 27) layer1.2.conv1 has been rebuilt.
(24 of 27) layer1.1.conv1 has been rebuilt.
(25 of 27) conv1 has been rebuilt.
(26 of 27) layer2.0.downsample.0 has been rebuilt.
(27 of 27) layer3.0.downsample.0 has been rebuilt.
[08/10 00:00:35 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/10 00:00:45 cifar10-global-group_norm-resnet56]: Acc: 0.0977 Val Loss: 3.7802

[08/10 00:00:45 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/10 00:01:46 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.4422, Val Loss=1.5120, lr=0.0100
[08/10 00:02:45 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.5463, Val Loss=1.2809, lr=0.0100
[08/10 00:03:44 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.5977, Val Loss=1.1451, lr=0.0100
[08/10 00:04:43 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.6297, Val Loss=1.0729, lr=0.0100
[08/10 00:05:45 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.6587, Val Loss=1.0427, lr=0.0100
[08/10 00:06:46 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.6885, Val Loss=0.9037, lr=0.0100
[08/10 00:07:44 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.7239, Val Loss=0.8028, lr=0.0100
[08/10 00:08:41 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7324, Val Loss=0.7545, lr=0.0100
[08/10 00:09:40 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.7151, Val Loss=0.8508, lr=0.0100
[08/10 00:10:38 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7396, Val Loss=0.7924, lr=0.0100
[08/10 00:11:38 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.7294, Val Loss=0.8392, lr=0.0100
[08/10 00:12:38 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.7210, Val Loss=0.8606, lr=0.0100
[08/10 00:13:38 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7650, Val Loss=0.7204, lr=0.0100
[08/10 00:14:39 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.7560, Val Loss=0.7395, lr=0.0100
[08/10 00:15:53 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.7771, Val Loss=0.6555, lr=0.0100
[08/10 00:16:51 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.7208, Val Loss=0.8878, lr=0.0100
[08/10 00:17:50 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.7928, Val Loss=0.6063, lr=0.0100
[08/10 00:18:48 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.7918, Val Loss=0.6085, lr=0.0100
[08/10 00:19:47 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.7961, Val Loss=0.5800, lr=0.0100
[08/10 00:20:52 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.7925, Val Loss=0.6540, lr=0.0100
[08/10 00:22:05 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.7726, Val Loss=0.6992, lr=0.0100
[08/10 00:23:05 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.7339, Val Loss=0.8345, lr=0.0100
[08/10 00:24:07 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8019, Val Loss=0.5882, lr=0.0100
[08/10 00:25:07 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8107, Val Loss=0.5571, lr=0.0100
[08/10 00:26:05 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.7865, Val Loss=0.6463, lr=0.0100
[08/10 00:27:04 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.7909, Val Loss=0.6240, lr=0.0100
[08/10 00:28:12 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.7227, Val Loss=0.9003, lr=0.0100
[08/10 00:29:17 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.7964, Val Loss=0.6019, lr=0.0100
[08/10 00:30:25 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8176, Val Loss=0.5556, lr=0.0100
[08/10 00:31:32 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8098, Val Loss=0.5831, lr=0.0100
[08/10 00:32:38 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8134, Val Loss=0.5444, lr=0.0100
[08/10 00:33:43 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8260, Val Loss=0.5271, lr=0.0100
[08/10 00:34:48 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.7959, Val Loss=0.6319, lr=0.0100
[08/10 00:35:54 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.7736, Val Loss=0.7555, lr=0.0100
[08/10 00:36:59 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8054, Val Loss=0.6067, lr=0.0100
[08/10 00:38:05 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8170, Val Loss=0.5562, lr=0.0100
[08/10 00:39:11 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.7885, Val Loss=0.6601, lr=0.0100
[08/10 00:40:17 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8138, Val Loss=0.5640, lr=0.0100
[08/10 00:41:24 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8079, Val Loss=0.5955, lr=0.0100
[08/10 00:42:30 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8253, Val Loss=0.5253, lr=0.0100
[08/10 00:43:35 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8696, Val Loss=0.3868, lr=0.0010
[08/10 00:45:32 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8698, Val Loss=0.3854, lr=0.0010
[08/10 00:46:52 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8689, Val Loss=0.3846, lr=0.0010
[08/10 00:48:23 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8684, Val Loss=0.3851, lr=0.0010
[08/10 00:49:54 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8712, Val Loss=0.3873, lr=0.0010
[08/10 00:51:32 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8709, Val Loss=0.3855, lr=0.0010
[08/10 00:53:20 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8728, Val Loss=0.3823, lr=0.0010
[08/10 00:55:06 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8739, Val Loss=0.3846, lr=0.0010
[08/10 00:56:48 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8726, Val Loss=0.3878, lr=0.0010
[08/10 00:58:27 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8722, Val Loss=0.3917, lr=0.0010
[08/10 01:00:03 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8733, Val Loss=0.3817, lr=0.0010
[08/10 01:01:45 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8705, Val Loss=0.3857, lr=0.0010
[08/10 01:03:29 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8727, Val Loss=0.3827, lr=0.0010
[08/10 01:05:12 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8739, Val Loss=0.3856, lr=0.0010
[08/10 01:06:58 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8724, Val Loss=0.3952, lr=0.0010
[08/10 01:08:42 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8705, Val Loss=0.3879, lr=0.0010
[08/10 01:10:23 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8746, Val Loss=0.3869, lr=0.0010
[08/10 01:12:01 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8749, Val Loss=0.3923, lr=0.0010
[08/10 01:13:36 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8706, Val Loss=0.3957, lr=0.0010
[08/10 01:15:11 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8730, Val Loss=0.3981, lr=0.0010
[08/10 01:16:51 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8745, Val Loss=0.3857, lr=0.0001
[08/10 01:18:33 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8753, Val Loss=0.3812, lr=0.0001
[08/10 01:20:19 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8752, Val Loss=0.3828, lr=0.0001
[08/10 01:22:03 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8743, Val Loss=0.3810, lr=0.0001
[08/10 01:23:44 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8736, Val Loss=0.3819, lr=0.0001
[08/10 01:25:22 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8753, Val Loss=0.3840, lr=0.0001
[08/10 01:26:58 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8760, Val Loss=0.3803, lr=0.0001
[08/10 01:28:37 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8768, Val Loss=0.3811, lr=0.0001
[08/10 01:30:19 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8760, Val Loss=0.3842, lr=0.0001
[08/10 01:32:02 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8749, Val Loss=0.3841, lr=0.0001
[08/10 01:33:49 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8769, Val Loss=0.3823, lr=0.0001
[08/10 01:35:35 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8776, Val Loss=0.3839, lr=0.0001
[08/10 01:37:18 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8772, Val Loss=0.3817, lr=0.0001
[08/10 01:38:58 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8768, Val Loss=0.3803, lr=0.0001
[08/10 01:40:34 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8765, Val Loss=0.3807, lr=0.0001
[08/10 01:42:11 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8763, Val Loss=0.3834, lr=0.0001
[08/10 01:43:52 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8770, Val Loss=0.3829, lr=0.0001
[08/10 01:45:35 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8757, Val Loss=0.3797, lr=0.0001
[08/10 01:47:23 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8767, Val Loss=0.3822, lr=0.0001
[08/10 01:49:09 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.8757, Val Loss=0.3820, lr=0.0001
[08/10 01:49:09 cifar10-global-group_norm-resnet56]: Best Acc=0.8776
[08/10 01:49:09 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
tensor([[[ 0.1228,  0.0727,  0.0732],
         [ 0.0848,  0.2313, -0.2693],
         [-0.1046, -0.0991, -0.1600]],

        [[ 0.0229, -0.0362, -0.2335],
         [ 0.1094,  0.3698, -0.3226],
         [ 0.0337,  0.0604, -0.0327]],

        [[-0.1383, -0.1189, -0.2413],
         [ 0.1768,  0.4497, -0.0848],
         [ 0.1095,  0.2068, -0.0082]]], device='cuda:0')
tensor([[[-0.0061,  0.3327,  0.1074],
         [ 0.0299,  0.5648,  0.4156],
         [-0.1807,  0.2087, -0.0403]],

        [[-0.1179, -0.3042, -0.0680],
         [-0.2496, -0.4310, -0.0393],
         [-0.0625, -0.0837,  0.0137]],

        [[ 0.1832, -0.0073, -0.0193],
         [ 0.1380, -0.2342, -0.1343],
         [ 0.1564,  0.0145, -0.0907]]], device='cuda:0')
[08/10 01:49:09 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/10 01:49:26 cifar10-global-group_norm-resnet56]: Acc: 0.1070 Val Loss: 3.4043

[08/10 01:49:26 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/10 01:51:40 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.5198, Val Loss=1.3090, lr=0.0100
[08/10 01:53:48 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.6234, Val Loss=1.0557, lr=0.0100
[08/10 01:56:01 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.7091, Val Loss=0.8422, lr=0.0100
[08/10 01:58:11 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.7344, Val Loss=0.7818, lr=0.0100
[08/10 02:00:24 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.7403, Val Loss=0.7551, lr=0.0100
[08/10 02:02:38 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.7772, Val Loss=0.6634, lr=0.0100
[08/10 02:04:50 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.7894, Val Loss=0.6122, lr=0.0100
[08/10 02:07:01 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7468, Val Loss=0.7845, lr=0.0100
[08/10 02:09:13 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8156, Val Loss=0.5398, lr=0.0100
[08/10 02:11:28 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8043, Val Loss=0.5853, lr=0.0100
[08/10 02:13:35 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.7998, Val Loss=0.6160, lr=0.0100
[08/10 02:15:47 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8141, Val Loss=0.5381, lr=0.0100
[08/10 02:17:58 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8194, Val Loss=0.5350, lr=0.0100
[08/10 02:20:09 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8232, Val Loss=0.5288, lr=0.0100
[08/10 02:22:19 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8217, Val Loss=0.5536, lr=0.0100
[08/10 02:24:31 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8444, Val Loss=0.4729, lr=0.0100
[08/10 02:26:45 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8198, Val Loss=0.5762, lr=0.0100
[08/10 02:28:52 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8539, Val Loss=0.4316, lr=0.0100
[08/10 02:31:05 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8345, Val Loss=0.4902, lr=0.0100
[08/10 02:33:15 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8165, Val Loss=0.5699, lr=0.0100
[08/10 02:35:27 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8558, Val Loss=0.4289, lr=0.0100
[08/10 02:37:38 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8541, Val Loss=0.4467, lr=0.0100
[08/10 02:39:50 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8512, Val Loss=0.4491, lr=0.0100
[08/10 02:42:00 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8477, Val Loss=0.4532, lr=0.0100
[08/10 02:44:13 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8338, Val Loss=0.5119, lr=0.0100
[08/10 02:46:28 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8376, Val Loss=0.4998, lr=0.0100
[08/10 02:48:25 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8349, Val Loss=0.5038, lr=0.0100
[08/10 02:50:28 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8406, Val Loss=0.4747, lr=0.0100
[08/10 02:52:30 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8476, Val Loss=0.4692, lr=0.0100
[08/10 02:54:33 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8699, Val Loss=0.3919, lr=0.0100
[08/10 02:56:37 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8532, Val Loss=0.4497, lr=0.0100
[08/10 02:58:44 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8517, Val Loss=0.4503, lr=0.0100
[08/10 03:00:50 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8683, Val Loss=0.4040, lr=0.0100
[08/10 03:02:57 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8457, Val Loss=0.4828, lr=0.0100
[08/10 03:05:12 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8684, Val Loss=0.4086, lr=0.0100
[08/10 03:07:24 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8556, Val Loss=0.4592, lr=0.0100
[08/10 03:09:31 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8661, Val Loss=0.4166, lr=0.0100
[08/10 03:11:41 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8457, Val Loss=0.4903, lr=0.0100
[08/10 03:13:49 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8419, Val Loss=0.5172, lr=0.0100
[08/10 03:15:53 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8569, Val Loss=0.4488, lr=0.0100
[08/10 03:17:56 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8947, Val Loss=0.3159, lr=0.0010
[08/10 03:20:00 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8982, Val Loss=0.3158, lr=0.0010
[08/10 03:22:02 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9005, Val Loss=0.3154, lr=0.0010
[08/10 03:24:06 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8991, Val Loss=0.3113, lr=0.0010
[08/10 03:26:11 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9006, Val Loss=0.3147, lr=0.0010
[08/10 03:28:16 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9014, Val Loss=0.3138, lr=0.0010
[08/10 03:30:21 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9011, Val Loss=0.3172, lr=0.0010
[08/10 03:32:24 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9013, Val Loss=0.3206, lr=0.0010
[08/10 03:34:29 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9000, Val Loss=0.3184, lr=0.0010
[08/10 03:36:35 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9038, Val Loss=0.3198, lr=0.0010
[08/10 03:38:43 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8993, Val Loss=0.3240, lr=0.0010
[08/10 03:40:49 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9016, Val Loss=0.3267, lr=0.0010
[08/10 03:43:01 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9014, Val Loss=0.3237, lr=0.0010
[08/10 03:45:13 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9016, Val Loss=0.3241, lr=0.0010
[08/10 03:47:22 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9017, Val Loss=0.3266, lr=0.0010
[08/10 03:49:30 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9003, Val Loss=0.3305, lr=0.0010
[08/10 03:51:39 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9018, Val Loss=0.3280, lr=0.0010
[08/10 03:53:44 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9046, Val Loss=0.3291, lr=0.0010
[08/10 03:55:47 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9021, Val Loss=0.3340, lr=0.0010
[08/10 03:57:49 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9036, Val Loss=0.3318, lr=0.0010
[08/10 03:59:51 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9032, Val Loss=0.3299, lr=0.0001
[08/10 04:01:53 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9035, Val Loss=0.3297, lr=0.0001
[08/10 04:03:58 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9047, Val Loss=0.3284, lr=0.0001
[08/10 04:06:02 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9047, Val Loss=0.3300, lr=0.0001
[08/10 04:08:05 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9048, Val Loss=0.3289, lr=0.0001
[08/10 04:10:10 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9048, Val Loss=0.3290, lr=0.0001
[08/10 04:12:13 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9037, Val Loss=0.3300, lr=0.0001
[08/10 04:14:16 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9026, Val Loss=0.3340, lr=0.0001
[08/10 04:16:24 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9035, Val Loss=0.3327, lr=0.0001
[08/10 04:18:31 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9028, Val Loss=0.3331, lr=0.0001
[08/10 04:20:38 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9026, Val Loss=0.3333, lr=0.0001
[08/10 04:22:53 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9029, Val Loss=0.3307, lr=0.0001
[08/10 04:25:04 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9042, Val Loss=0.3309, lr=0.0001
[08/10 04:27:13 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9030, Val Loss=0.3330, lr=0.0001
[08/10 04:29:23 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9041, Val Loss=0.3318, lr=0.0001
[08/10 04:31:32 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9031, Val Loss=0.3331, lr=0.0001
[08/10 04:33:38 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9036, Val Loss=0.3346, lr=0.0001
[08/10 04:35:41 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9030, Val Loss=0.3338, lr=0.0001
[08/10 04:37:43 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9030, Val Loss=0.3331, lr=0.0001
[08/10 04:39:47 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9042, Val Loss=0.3351, lr=0.0001
[08/10 04:39:47 cifar10-global-group_norm-resnet56]: Best Acc=0.9048

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>		S
