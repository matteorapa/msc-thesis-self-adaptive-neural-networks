Microsoft Windows [Version 10.0.22621.1992]
(c) Microsoft Corporation. All rights reserved.

C:\Users\35679>cd Downloads

C:\Users\35679\Downloads>
C:\Users\35679\Downloads>
C:\Users\35679\Downloads>cd Torch-Pruning-1.1.4

C:\Users\35679\Downloads\Torch-Pruning-1.1.4>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4>cd Torch-Pruning-1.1.4

C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4>cd benchmarks

C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.1 --global-pruning
--total-epochs 80
Traceback (most recent call last):
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\main.py", line 13, in <module>
    import engine.utils as utils
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\__init__.py", line 1, in <module>
    from . import models, utils
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\models\__init__.py", line 1, in <module>
    from . import cifar, imagenet, graph
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\models\cifar\__init__.py", line 1,in <module>
    from . import (
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\models\cifar\vit.py", line 5, in <module>
    from einops import rearrange, repeat
ModuleNotFoundError: No module named 'einops'

C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>conda activate base

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>conda activate base

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.1 --global-pruning --total-epochs 80
Files already downloaded and verified
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: mode: prune
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: model: resnet56
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: verbose: False
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: method: group_norm
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sparsity: 0.1
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: seed: None
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: device: cuda
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/14 14:59:17 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 []
layer2.0.downsample.0 []
conv1 []
layer1.0.conv1 [6, 7]
layer1.1.conv1 []
layer1.2.conv1 [6]
layer1.3.conv1 [12]
layer1.4.conv1 []
layer1.5.conv1 [2, 11]
layer1.6.conv1 []
layer1.7.conv1 []
layer1.8.conv1 [0]
layer2.0.conv1 []
layer2.1.conv1 [1, 3, 4, 19, 22, 26, 31]
layer2.2.conv1 [8, 20]
layer2.3.conv1 [26, 28]
layer2.4.conv1 [23, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29]
layer2.6.conv1 [2, 4, 5, 6, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 []
layer3.1.conv1 [56]
layer3.2.conv1 [29, 45, 49]
layer3.3.conv1 [33]
layer3.4.conv1 [45, 54]
layer3.5.conv1 [10, 63]
layer3.6.conv1 [3, 14, 26, 57]
layer3.7.conv1 [21, 55]
layer3.8.conv1 []
=> Start history generation
layer3.8.conv1 []
layer3.7.conv1 []
layer3.6.conv1 []
layer3.5.conv1 []
layer3.4.conv1 []
layer3.3.conv1 []
layer3.2.conv1 []
layer3.1.conv1 []
layer3.0.conv1 []
layer2.8.conv1 []
layer2.7.conv1 []
layer2.6.conv1 []
layer2.5.conv1 []
layer2.4.conv1 []
layer2.3.conv1 []
layer2.2.conv1 []
layer2.1.conv1 []
layer2.0.conv1 []
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 []
layer1.6.conv1 []
layer1.5.conv1 []
layer1.4.conv1 []
layer1.3.conv1 []
layer1.2.conv1 []
layer1.1.conv1 []
layer1.0.conv1 []
conv1 []
layer2.0.downsample.0 []
layer3.0.downsample.0 []
[08/14 14:59:23 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(32, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[08/14 14:59:23 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/14 14:59:34 cifar10-global-group_norm-resnet56]: Acc: 0.9344 Val Loss: 0.2649

[08/14 14:59:34 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 []
layer2.0.downsample.0 []
conv1 []
layer1.0.conv1 [0, 3, 5, 13]
layer1.1.conv1 [0, 3, 5, 7, 12]
layer1.2.conv1 [0, 3, 7, 8, 10, 13]
layer1.3.conv1 [3, 11]
layer1.4.conv1 [7]
layer1.5.conv1 [0, 5, 11]
layer1.6.conv1 [4, 7, 14]
layer1.7.conv1 [3, 5, 7, 9]
layer1.8.conv1 [4, 6, 8, 10, 14]
layer2.0.conv1 []
layer2.1.conv1 [1, 8, 13, 20, 23, 24]
layer2.2.conv1 [11, 19]
layer2.3.conv1 [11, 17]
layer2.4.conv1 [1, 9, 15, 22]
layer2.5.conv1 [5, 11]
layer2.6.conv1 [3, 5, 13]
layer2.7.conv1 [8, 10]
layer2.8.conv1 [1, 2]
layer3.0.conv1 []
layer3.1.conv1 [10, 37]
layer3.2.conv1 [3, 11, 35]
layer3.3.conv1 [2, 6, 24, 38, 43, 45, 46, 48, 54]
layer3.4.conv1 [0, 30, 51]
layer3.5.conv1 [3, 5, 6, 8, 9, 27, 37, 44, 48, 51]
layer3.6.conv1 [11, 20, 28, 42, 44, 46, 53, 56]
layer3.7.conv1 [0, 2, 22, 26, 30, 38, 45, 55]
layer3.8.conv1 [24, 45]
=> Start history generation
layer3.8.conv1 []
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 []
layer3.5.conv1 []
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 []
layer3.2.conv1 []
layer3.1.conv1 []
layer3.0.conv1 []
layer2.8.conv1 []
layer2.7.conv1 []
layer2.6.conv1 []
layer2.5.conv1 []
layer2.4.conv1 []
layer2.3.conv1 []
layer2.2.conv1 []
layer2.1.conv1 []
layer2.0.conv1 []
layer1.8.conv1 []
layer1.7.conv1 []
layer1.6.conv1 []
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 []
layer1.3.conv1 []
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 []
layer3.0.downsample.0 []
[08/14 14:59:39 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(32, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(32, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.71 M (82.92%)
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 94.01 M (73.96%, 1.35X )
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.9187
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 0.3601
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Finetuning...
[08/14 15:00:49 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8951, Val Loss=0.3752, lr=0.0100
[08/14 15:01:43 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.9133, Val Loss=0.3079, lr=0.0100
[08/14 15:02:38 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.9005, Val Loss=0.3409, lr=0.0100
[08/14 15:03:33 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.9040, Val Loss=0.3321, lr=0.0100
[08/14 15:04:27 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.9084, Val Loss=0.3103, lr=0.0100
[08/14 15:05:22 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.9049, Val Loss=0.3148, lr=0.0100
[08/14 15:06:16 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.9086, Val Loss=0.3104, lr=0.0100
[08/14 15:07:11 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.9108, Val Loss=0.3180, lr=0.0100
[08/14 15:08:05 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.9026, Val Loss=0.3437, lr=0.0100
[08/14 15:08:59 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.9042, Val Loss=0.3384, lr=0.0100
[08/14 15:09:53 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.9044, Val Loss=0.3325, lr=0.0100
[08/14 15:10:47 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.9015, Val Loss=0.3526, lr=0.0100
[08/14 15:11:40 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.9076, Val Loss=0.3278, lr=0.0100
[08/14 15:12:34 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.9024, Val Loss=0.3376, lr=0.0100
[08/14 15:13:27 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8955, Val Loss=0.3551, lr=0.0100
[08/14 15:14:21 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.9029, Val Loss=0.3595, lr=0.0100
[08/14 15:15:15 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9040, Val Loss=0.3256, lr=0.0100
[08/14 15:16:08 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.9022, Val Loss=0.3294, lr=0.0100
[08/14 15:17:02 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.9027, Val Loss=0.3345, lr=0.0100
[08/14 15:17:56 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.9011, Val Loss=0.3590, lr=0.0100
[08/14 15:18:49 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.9077, Val Loss=0.3273, lr=0.0100
[08/14 15:19:43 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9020, Val Loss=0.3422, lr=0.0100
[08/14 15:20:36 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.9160, Val Loss=0.2878, lr=0.0100
[08/14 15:21:30 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.9086, Val Loss=0.3200, lr=0.0100
[08/14 15:22:24 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.9069, Val Loss=0.3133, lr=0.0100
[08/14 15:23:18 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.9073, Val Loss=0.3145, lr=0.0100
[08/14 15:24:12 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.9061, Val Loss=0.3225, lr=0.0100
[08/14 15:25:05 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.9149, Val Loss=0.3027, lr=0.0100
[08/14 15:25:59 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8993, Val Loss=0.3544, lr=0.0100
[08/14 15:26:53 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.9136, Val Loss=0.2902, lr=0.0100
[08/14 15:27:46 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.9011, Val Loss=0.3420, lr=0.0100
[08/14 15:28:40 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9042, Val Loss=0.3444, lr=0.0100
[08/14 15:29:34 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.9071, Val Loss=0.3262, lr=0.0100
[08/14 15:30:28 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9023, Val Loss=0.3517, lr=0.0100
[08/14 15:31:21 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8943, Val Loss=0.3743, lr=0.0100
[08/14 15:32:15 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.9007, Val Loss=0.3477, lr=0.0100
[08/14 15:33:11 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.9059, Val Loss=0.3397, lr=0.0100
[08/14 15:34:05 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9050, Val Loss=0.3157, lr=0.0100
[08/14 15:35:00 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.9107, Val Loss=0.3154, lr=0.0100
[08/14 15:35:54 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.9114, Val Loss=0.2929, lr=0.0100
[08/14 15:36:49 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9328, Val Loss=0.2305, lr=0.0010
[08/14 15:37:43 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9355, Val Loss=0.2267, lr=0.0010
[08/14 15:38:38 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9341, Val Loss=0.2275, lr=0.0010
[08/14 15:39:32 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9354, Val Loss=0.2289, lr=0.0010
[08/14 15:40:27 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9363, Val Loss=0.2298, lr=0.0010
[08/14 15:41:22 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9344, Val Loss=0.2302, lr=0.0010
[08/14 15:42:16 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9349, Val Loss=0.2363, lr=0.0010
[08/14 15:43:10 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9365, Val Loss=0.2336, lr=0.0010
[08/14 15:44:05 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9358, Val Loss=0.2377, lr=0.0010
[08/14 15:44:59 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9352, Val Loss=0.2380, lr=0.0010
[08/14 15:45:54 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9363, Val Loss=0.2357, lr=0.0010
[08/14 15:46:49 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9361, Val Loss=0.2401, lr=0.0010
[08/14 15:47:43 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9370, Val Loss=0.2397, lr=0.0010
[08/14 15:48:39 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9350, Val Loss=0.2459, lr=0.0010
[08/14 15:49:35 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9363, Val Loss=0.2414, lr=0.0010
[08/14 15:50:31 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9364, Val Loss=0.2410, lr=0.0010
[08/14 15:51:25 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9348, Val Loss=0.2446, lr=0.0010
[08/14 15:52:19 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9371, Val Loss=0.2404, lr=0.0010
[08/14 15:53:14 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9373, Val Loss=0.2385, lr=0.0010
[08/14 15:54:09 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9361, Val Loss=0.2435, lr=0.0010
[08/14 15:55:03 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9360, Val Loss=0.2440, lr=0.0001
[08/14 15:55:57 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9363, Val Loss=0.2439, lr=0.0001
[08/14 15:56:51 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9372, Val Loss=0.2417, lr=0.0001
[08/14 15:57:46 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9374, Val Loss=0.2422, lr=0.0001
[08/14 15:58:41 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9376, Val Loss=0.2432, lr=0.0001
[08/14 15:59:36 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9367, Val Loss=0.2416, lr=0.0001
[08/14 16:00:30 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9369, Val Loss=0.2424, lr=0.0001
[08/14 16:01:25 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9376, Val Loss=0.2419, lr=0.0001
[08/14 16:02:20 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9367, Val Loss=0.2413, lr=0.0001
[08/14 16:03:15 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9374, Val Loss=0.2422, lr=0.0001
[08/14 16:04:09 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9370, Val Loss=0.2427, lr=0.0001
[08/14 16:05:04 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9372, Val Loss=0.2426, lr=0.0001
[08/14 16:05:58 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9373, Val Loss=0.2443, lr=0.0001
[08/14 16:06:53 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9385, Val Loss=0.2420, lr=0.0001
[08/14 16:07:47 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9374, Val Loss=0.2424, lr=0.0001
[08/14 16:08:42 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9385, Val Loss=0.2423, lr=0.0001
[08/14 16:09:36 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9384, Val Loss=0.2406, lr=0.0001
[08/14 16:10:31 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9378, Val Loss=0.2429, lr=0.0001
[08/14 16:11:25 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9375, Val Loss=0.2421, lr=0.0001
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9367, Val Loss=0.2440, lr=0.0001
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: Best Acc=0.9385
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: Params: 0.71 M
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: ops: 94.01 M
[08/14 16:12:31 cifar10-global-group_norm-resnet56]: Acc: 0.9367 Val Loss: 0.2440

[08/14 16:12:31 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/14 16:12:31 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/14 16:12:42 cifar10-global-group_norm-resnet56]: Acc: 0.8151 Val Loss: 0.6809

[08/14 16:12:42 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/14 16:13:53 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8177, Val Loss=0.5483, lr=0.0100
[08/14 16:15:04 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8283, Val Loss=0.5237, lr=0.0100
[08/14 16:16:14 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8628, Val Loss=0.4217, lr=0.0100
[08/14 16:17:25 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8820, Val Loss=0.3590, lr=0.0100
[08/14 16:18:36 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8760, Val Loss=0.3729, lr=0.0100
[08/14 16:19:48 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8791, Val Loss=0.3669, lr=0.0100
[08/14 16:20:58 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8762, Val Loss=0.3729, lr=0.0100
[08/14 16:22:09 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8848, Val Loss=0.3540, lr=0.0100
[08/14 16:23:20 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8881, Val Loss=0.3484, lr=0.0100
[08/14 16:24:31 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8930, Val Loss=0.3348, lr=0.0100
[08/14 16:25:42 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8842, Val Loss=0.3575, lr=0.0100
[08/14 16:26:53 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8888, Val Loss=0.3504, lr=0.0100
[08/14 16:28:03 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8905, Val Loss=0.3390, lr=0.0100
[08/14 16:29:13 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8958, Val Loss=0.3332, lr=0.0100
[08/14 16:30:25 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8731, Val Loss=0.4183, lr=0.0100
[08/14 16:31:36 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8915, Val Loss=0.3445, lr=0.0100
[08/14 16:32:48 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8995, Val Loss=0.3275, lr=0.0100
[08/14 16:33:58 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8978, Val Loss=0.3222, lr=0.0100
[08/14 16:35:09 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8818, Val Loss=0.3940, lr=0.0100
[08/14 16:36:22 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8937, Val Loss=0.3487, lr=0.0100
[08/14 16:37:44 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8953, Val Loss=0.3417, lr=0.0100
[08/14 16:38:57 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9021, Val Loss=0.3145, lr=0.0100
[08/14 16:40:10 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8950, Val Loss=0.3371, lr=0.0100
[08/14 16:41:27 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8971, Val Loss=0.3326, lr=0.0100
[08/14 16:42:41 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8990, Val Loss=0.3278, lr=0.0100
[08/14 16:43:54 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8896, Val Loss=0.3584, lr=0.0100
[08/14 16:45:20 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8898, Val Loss=0.3566, lr=0.0100
[08/14 16:46:33 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8973, Val Loss=0.3304, lr=0.0100
[08/14 16:47:47 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8936, Val Loss=0.3530, lr=0.0100
[08/14 16:49:09 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8988, Val Loss=0.3235, lr=0.0100
[08/14 16:50:24 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8929, Val Loss=0.3319, lr=0.0100
[08/14 16:51:41 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9042, Val Loss=0.3053, lr=0.0100
[08/14 16:53:03 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8907, Val Loss=0.3491, lr=0.0100
[08/14 16:54:17 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8979, Val Loss=0.3196, lr=0.0100
[08/14 16:55:33 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8912, Val Loss=0.3542, lr=0.0100
[08/14 16:56:47 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8991, Val Loss=0.3196, lr=0.0100
[08/14 16:58:04 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8999, Val Loss=0.3239, lr=0.0100
[08/14 16:59:19 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9001, Val Loss=0.3397, lr=0.0100
[08/14 17:00:40 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8934, Val Loss=0.3687, lr=0.0100
[08/14 17:02:01 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8978, Val Loss=0.3286, lr=0.0100
[08/14 17:03:16 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9152, Val Loss=0.2765, lr=0.0010
[08/14 17:04:34 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9142, Val Loss=0.2755, lr=0.0010
[08/14 17:05:48 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9159, Val Loss=0.2736, lr=0.0010
[08/14 17:07:01 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9169, Val Loss=0.2721, lr=0.0010
[08/14 17:08:46 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9179, Val Loss=0.2731, lr=0.0010
[08/14 17:10:29 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9173, Val Loss=0.2738, lr=0.0010
[08/14 17:12:02 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9166, Val Loss=0.2711, lr=0.0010
[08/14 17:13:30 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9184, Val Loss=0.2706, lr=0.0010
[08/14 17:14:43 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9187, Val Loss=0.2776, lr=0.0010
[08/14 17:15:54 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9178, Val Loss=0.2746, lr=0.0010
[08/14 17:17:06 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9183, Val Loss=0.2803, lr=0.0010
[08/14 17:18:17 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9185, Val Loss=0.2787, lr=0.0010
[08/14 17:19:28 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9194, Val Loss=0.2801, lr=0.0010
[08/14 17:20:39 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9179, Val Loss=0.2783, lr=0.0010
[08/14 17:21:50 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9176, Val Loss=0.2810, lr=0.0010
[08/14 17:23:01 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9174, Val Loss=0.2867, lr=0.0010
[08/14 17:24:10 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9172, Val Loss=0.2854, lr=0.0010
[08/14 17:25:22 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9182, Val Loss=0.2848, lr=0.0010
[08/14 17:26:34 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9162, Val Loss=0.2868, lr=0.0010
[08/14 17:27:45 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9162, Val Loss=0.2858, lr=0.0010
[08/14 17:28:55 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9185, Val Loss=0.2847, lr=0.0001
[08/14 17:30:05 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9177, Val Loss=0.2861, lr=0.0001
[08/14 17:31:15 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9176, Val Loss=0.2857, lr=0.0001
[08/14 17:32:25 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9189, Val Loss=0.2873, lr=0.0001
[08/14 17:33:35 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9180, Val Loss=0.2845, lr=0.0001
[08/14 17:34:44 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9182, Val Loss=0.2859, lr=0.0001
[08/14 17:35:54 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9188, Val Loss=0.2841, lr=0.0001
[08/14 17:37:04 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9183, Val Loss=0.2847, lr=0.0001
[08/14 17:38:13 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9180, Val Loss=0.2853, lr=0.0001
[08/14 17:39:23 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9172, Val Loss=0.2833, lr=0.0001
[08/14 17:40:32 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9196, Val Loss=0.2875, lr=0.0001
[08/14 17:41:42 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9175, Val Loss=0.2858, lr=0.0001
[08/14 17:42:51 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9178, Val Loss=0.2872, lr=0.0001
[08/14 17:44:01 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9195, Val Loss=0.2845, lr=0.0001
[08/14 17:45:10 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9190, Val Loss=0.2863, lr=0.0001
[08/14 17:46:19 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9174, Val Loss=0.2856, lr=0.0001
[08/14 17:47:29 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9180, Val Loss=0.2860, lr=0.0001
[08/14 17:48:38 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9189, Val Loss=0.2854, lr=0.0001
[08/14 17:49:48 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9187, Val Loss=0.2835, lr=0.0001
[08/14 17:51:00 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9182, Val Loss=0.2869, lr=0.0001
[08/14 17:51:00 cifar10-global-group_norm-resnet56]: Best Acc=0.9196
[08/14 17:51:00 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/14 17:51:01 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/14 17:51:12 cifar10-global-group_norm-resnet56]: Acc: 0.1003 Val Loss: 2.4542

[08/14 17:51:12 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/14 17:52:10 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8430, Val Loss=0.4533, lr=0.0100
[08/14 17:53:08 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8551, Val Loss=0.4500, lr=0.0100
[08/14 17:54:05 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8817, Val Loss=0.3566, lr=0.0100
[08/14 17:55:04 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8952, Val Loss=0.3155, lr=0.0100
[08/14 17:56:04 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8860, Val Loss=0.3389, lr=0.0100
[08/14 17:57:02 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8802, Val Loss=0.3744, lr=0.0100
[08/14 17:57:59 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8861, Val Loss=0.3472, lr=0.0100
[08/14 17:58:56 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8821, Val Loss=0.3822, lr=0.0100
[08/14 17:59:53 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8988, Val Loss=0.3127, lr=0.0100
[08/14 18:00:51 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8886, Val Loss=0.3708, lr=0.0100
[08/14 18:01:48 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8959, Val Loss=0.3320, lr=0.0100
[08/14 18:02:46 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8972, Val Loss=0.3341, lr=0.0100
[08/14 18:03:43 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.9017, Val Loss=0.3075, lr=0.0100
[08/14 18:04:40 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8809, Val Loss=0.4240, lr=0.0100
[08/14 18:05:38 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8970, Val Loss=0.3366, lr=0.0100
[08/14 18:06:35 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.9028, Val Loss=0.3161, lr=0.0100
[08/14 18:07:32 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9009, Val Loss=0.3127, lr=0.0100
[08/14 18:08:30 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8907, Val Loss=0.3477, lr=0.0100
[08/14 18:09:27 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8994, Val Loss=0.3333, lr=0.0100
[08/14 18:10:27 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8862, Val Loss=0.3933, lr=0.0100
[08/14 18:11:24 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8974, Val Loss=0.3453, lr=0.0100
[08/14 18:12:21 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9020, Val Loss=0.3262, lr=0.0100
[08/14 18:13:17 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8953, Val Loss=0.3416, lr=0.0100
[08/14 18:14:14 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8934, Val Loss=0.3526, lr=0.0100
[08/14 18:15:11 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.9010, Val Loss=0.3198, lr=0.0100
[08/14 18:16:07 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.9062, Val Loss=0.3193, lr=0.0100
[08/14 18:17:04 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.9048, Val Loss=0.2961, lr=0.0100
[08/14 18:18:01 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.9061, Val Loss=0.3016, lr=0.0100
[08/14 18:18:57 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8904, Val Loss=0.3789, lr=0.0100
[08/14 18:19:54 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8929, Val Loss=0.3654, lr=0.0100
[08/14 18:20:50 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8945, Val Loss=0.3502, lr=0.0100
[08/14 18:21:47 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9119, Val Loss=0.3028, lr=0.0100
[08/14 18:22:44 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.9062, Val Loss=0.3215, lr=0.0100
[08/14 18:23:40 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8998, Val Loss=0.3276, lr=0.0100
[08/14 18:24:36 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.9112, Val Loss=0.2985, lr=0.0100
[08/14 18:25:33 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.9030, Val Loss=0.3248, lr=0.0100
[08/14 18:26:29 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.9037, Val Loss=0.3397, lr=0.0100
[08/14 18:27:27 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9039, Val Loss=0.3453, lr=0.0100
[08/14 18:28:23 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8889, Val Loss=0.3942, lr=0.0100
[08/14 18:29:20 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8940, Val Loss=0.3600, lr=0.0100
[08/14 18:30:16 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9284, Val Loss=0.2403, lr=0.0010
[08/14 18:31:14 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9282, Val Loss=0.2433, lr=0.0010
[08/14 18:32:11 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9313, Val Loss=0.2409, lr=0.0010
[08/14 18:33:08 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9297, Val Loss=0.2439, lr=0.0010
[08/14 18:34:05 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9303, Val Loss=0.2428, lr=0.0010
[08/14 18:35:03 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9317, Val Loss=0.2393, lr=0.0010
[08/14 18:36:01 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9312, Val Loss=0.2429, lr=0.0010
[08/14 18:36:58 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9322, Val Loss=0.2431, lr=0.0010
[08/14 18:37:54 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9312, Val Loss=0.2445, lr=0.0010
[08/14 18:38:51 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9328, Val Loss=0.2456, lr=0.0010
[08/14 18:39:48 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9302, Val Loss=0.2474, lr=0.0010
[08/14 18:40:44 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9334, Val Loss=0.2454, lr=0.0010
[08/14 18:41:41 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9327, Val Loss=0.2453, lr=0.0010
[08/14 18:42:37 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9311, Val Loss=0.2495, lr=0.0010
[08/14 18:43:34 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9313, Val Loss=0.2517, lr=0.0010
[08/14 18:44:30 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9307, Val Loss=0.2548, lr=0.0010
[08/14 18:45:27 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9325, Val Loss=0.2541, lr=0.0010
[08/14 18:46:23 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9313, Val Loss=0.2539, lr=0.0010
[08/14 18:47:19 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9329, Val Loss=0.2485, lr=0.0010
[08/14 18:48:16 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9328, Val Loss=0.2541, lr=0.0010
[08/14 18:49:13 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9336, Val Loss=0.2506, lr=0.0001
[08/14 18:50:09 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9332, Val Loss=0.2508, lr=0.0001
[08/14 18:51:07 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9337, Val Loss=0.2514, lr=0.0001
[08/14 18:52:05 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9336, Val Loss=0.2518, lr=0.0001
[08/14 18:53:04 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9335, Val Loss=0.2502, lr=0.0001
[08/14 18:54:00 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9329, Val Loss=0.2505, lr=0.0001
[08/14 18:54:57 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9329, Val Loss=0.2508, lr=0.0001
[08/14 18:55:53 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9329, Val Loss=0.2492, lr=0.0001
[08/14 18:56:49 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9341, Val Loss=0.2514, lr=0.0001
[08/14 18:57:46 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9343, Val Loss=0.2517, lr=0.0001
[08/14 18:58:42 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9342, Val Loss=0.2494, lr=0.0001
[08/14 18:59:38 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9334, Val Loss=0.2491, lr=0.0001
[08/14 19:00:35 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9335, Val Loss=0.2526, lr=0.0001
[08/14 19:01:31 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9335, Val Loss=0.2526, lr=0.0001
[08/14 19:02:29 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9331, Val Loss=0.2505, lr=0.0001
[08/14 19:03:27 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9328, Val Loss=0.2529, lr=0.0001
[08/14 19:04:25 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9330, Val Loss=0.2517, lr=0.0001
[08/14 19:05:22 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9342, Val Loss=0.2505, lr=0.0001
[08/14 19:06:19 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9343, Val Loss=0.2498, lr=0.0001
[08/14 19:07:16 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9329, Val Loss=0.2513, lr=0.0001
[08/14 19:07:16 cifar10-global-group_norm-resnet56]: Best Acc=0.9343

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.2 --global-pruning --total-epochs 80
Files already downloaded and verified
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: mode: prune
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: model: resnet56
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: verbose: False
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: method: group_norm
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sparsity: 0.2
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: seed: None
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: device: cuda
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/14 19:11:06 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 [28, 30]
layer2.0.downsample.0 [1, 12]
conv1 [6, 7, 10, 13]
layer1.0.conv1 [0, 3, 5, 6, 7, 15]
layer1.1.conv1 [0, 3, 5, 7, 12]
layer1.2.conv1 [0, 3, 6, 8, 9, 11, 14]
layer1.3.conv1 [3, 11, 12]
layer1.4.conv1 [7, 11]
layer1.5.conv1 [0, 2, 11, 13]
layer1.6.conv1 [4, 7, 13, 14]
layer1.7.conv1 [3, 5, 7, 9]
layer1.8.conv1 [0, 5, 7, 9, 11, 15]
layer2.0.conv1 [9, 11]
layer2.1.conv1 [1, 3, 4, 11, 16, 19, 22, 25, 26, 29, 30, 31]
layer2.2.conv1 [8, 12, 20, 21]
layer2.3.conv1 [11, 17, 26, 28]
layer2.4.conv1 [9, 15, 22, 23, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29, 31]
layer2.6.conv1 [2, 4, 5, 6, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 [50]
layer3.1.conv1 [10, 37, 55, 56, 63]
layer3.2.conv1 [3, 11, 29, 36, 45, 49]
layer3.3.conv1 [2, 6, 24, 31, 33, 39, 44, 46, 47, 49, 55]
layer3.4.conv1 [0, 30, 45, 52, 54]
layer3.5.conv1 [0, 3, 5, 6, 8, 9, 10, 28, 38, 40, 45, 49, 52, 63]
layer3.6.conv1 [3, 12, 14, 26, 31, 45, 47, 49, 56, 57, 60]
layer3.7.conv1 [0, 2, 19, 21, 23, 27, 31, 39, 46, 51, 55, 57, 63]
layer3.8.conv1 [24, 41, 45]
=> Start history generation
layer3.8.conv1 [28, 30]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 [28, 30]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 [28, 30]
layer3.2.conv1 [28, 30]
layer3.1.conv1 [28, 30]
layer3.0.conv1 [1, 12]
layer2.8.conv1 [1, 12]
layer2.7.conv1 [1, 12]
layer2.6.conv1 [1, 12]
layer2.5.conv1 [1, 12]
layer2.4.conv1 [1, 12]
layer2.3.conv1 [1, 12]
layer2.2.conv1 [1, 12]
layer2.1.conv1 [1, 12]
layer2.0.conv1 [6, 7, 10, 13]
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 [6, 7, 10, 13]
layer1.6.conv1 [6, 7, 10, 13]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 [6, 7, 10, 13]
layer1.3.conv1 [6, 7, 10, 13]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 [6, 7, 10, 13]
layer3.0.downsample.0 [1, 12]
[08/14 19:11:09 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 30, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 30, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(30, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(30, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(30, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(30, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(30, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(30, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(30, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(30, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(30, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(30, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=10, bias=True)
)
[08/14 19:11:09 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/14 19:11:20 cifar10-global-group_norm-resnet56]: Acc: 0.2797 Val Loss: 6.9964

[08/14 19:11:20 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 [3, 16]
layer2.0.downsample.0 [0, 23]
conv1 [6, 7, 11]
layer1.1.conv1 [8, 9]
layer1.2.conv1 [0, 1, 5, 7]
layer1.3.conv1 [3, 4, 5, 9]
layer1.4.conv1 [1, 2, 6, 7, 13]
layer1.5.conv1 [4, 11]
layer1.6.conv1 [0, 1, 2, 4]
layer1.7.conv1 [1, 8]
layer2.0.conv1 [1, 28, 29]
layer2.1.conv1 [1, 4, 5, 9, 10, 15, 16, 17]
layer2.2.conv1 [8, 11, 14, 19, 22, 23, 26, 27]
layer2.3.conv1 [11, 16, 18, 24, 25]
layer2.4.conv1 [0, 1, 5, 8, 14, 16]
layer2.5.conv1 [0, 1, 2, 3, 5, 6]
layer2.6.conv1 [1, 3, 5, 12, 13]
layer2.7.conv1 [0, 2, 3, 5, 8, 10]
layer2.8.conv1 [1, 2, 5]
layer3.0.conv1 [4, 22, 29, 47]
layer3.1.conv1 [2, 20, 22, 24, 26, 28, 32, 44, 56]
layer3.2.conv1 [2, 5, 18, 25, 27, 30, 34, 38, 40, 43, 51]
layer3.3.conv1 [3, 5, 6, 7, 8, 14, 19, 27, 34, 47, 49, 50]
layer3.4.conv1 [2, 13, 17, 19, 25, 26, 29, 30, 41, 46, 47, 54]
layer3.5.conv1 [0, 5, 7, 11, 12, 13, 19, 35, 39, 46, 47]
layer3.6.conv1 [8, 10, 12, 13, 18, 19, 21, 32, 34, 38, 40, 41, 44, 48, 50, 52]
layer3.7.conv1 [2, 6, 7, 16, 19, 26, 27, 29, 30, 34, 40, 41, 42, 44, 47]
layer3.8.conv1 [17, 30, 40, 43, 48, 57]
=> Start history generation
layer3.8.conv1 [3, 16]
layer3.7.conv1 [3, 16]
layer3.6.conv1 [3, 16]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
layer3.4.conv1 [3, 16]
layer3.3.conv1 [3, 16]
layer3.2.conv1 [3, 16]
layer3.1.conv1 [3, 16]
layer3.0.conv1 [0, 23]
layer2.8.conv1 [0, 23]
layer2.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
layer2.6.conv1 [0, 23]
layer2.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
layer2.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
layer2.3.conv1 [0, 23]
layer2.2.conv1 [0, 23]
layer2.1.conv1 [0, 23]
layer2.0.conv1 [6, 7, 11]
layer1.7.conv1 [6, 7, 11]
layer1.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
layer1.5.conv1 [6, 7, 11]
layer1.4.conv1 [6, 7, 11]
layer1.3.conv1 [6, 7, 11]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
layer1.1.conv1 [6, 7, 11]
conv1 []
layer2.0.downsample.0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
layer3.0.downsample.0 [0, 23]
[08/14 19:11:23 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(9, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(9, 27, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(9, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(28, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(28, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(28, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(28, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(28, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(28, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(28, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(28, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(28, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(28, 60, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(60, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(60, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(60, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(60, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(60, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(39, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(60, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(60, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(60, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=60, out_features=10, bias=True)
)
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.51 M (59.33%)
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 57.71 M (45.40%, 2.20X )
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.1211
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 7.6279
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Finetuning...
[08/14 19:12:25 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8670, Val Loss=0.4191, lr=0.0100
[08/14 19:13:15 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8839, Val Loss=0.3759, lr=0.0100
[08/14 19:14:04 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8919, Val Loss=0.3411, lr=0.0100
[08/14 19:14:54 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8950, Val Loss=0.3413, lr=0.0100
[08/14 19:15:45 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8855, Val Loss=0.3607, lr=0.0100
[08/14 19:16:35 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8891, Val Loss=0.3521, lr=0.0100
[08/14 19:17:26 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8945, Val Loss=0.3373, lr=0.0100
[08/14 19:18:16 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8814, Val Loss=0.3867, lr=0.0100
[08/14 19:19:06 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8848, Val Loss=0.3879, lr=0.0100
[08/14 19:19:57 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8843, Val Loss=0.3834, lr=0.0100
[08/14 19:20:48 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8923, Val Loss=0.3539, lr=0.0100
[08/14 19:21:38 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8908, Val Loss=0.3736, lr=0.0100
[08/14 19:22:29 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8771, Val Loss=0.4110, lr=0.0100
[08/14 19:23:21 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8950, Val Loss=0.3530, lr=0.0100
[08/14 19:24:12 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8937, Val Loss=0.3519, lr=0.0100
[08/14 19:25:02 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8961, Val Loss=0.3495, lr=0.0100
[08/14 19:25:52 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9015, Val Loss=0.3289, lr=0.0100
[08/14 19:26:42 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8936, Val Loss=0.3418, lr=0.0100
[08/14 19:27:32 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8910, Val Loss=0.3715, lr=0.0100
[08/14 19:28:21 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.9078, Val Loss=0.2991, lr=0.0100
[08/14 19:29:11 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8970, Val Loss=0.3525, lr=0.0100
[08/14 19:30:00 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8838, Val Loss=0.3857, lr=0.0100
[08/14 19:30:50 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.9028, Val Loss=0.3263, lr=0.0100
[08/14 19:31:39 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8984, Val Loss=0.3473, lr=0.0100
[08/14 19:32:29 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8980, Val Loss=0.3424, lr=0.0100
[08/14 19:33:18 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.9045, Val Loss=0.3180, lr=0.0100
[08/14 19:34:08 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8952, Val Loss=0.3494, lr=0.0100
[08/14 19:34:57 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8992, Val Loss=0.3326, lr=0.0100
[08/14 19:35:46 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8907, Val Loss=0.3606, lr=0.0100
[08/14 19:36:36 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.9016, Val Loss=0.3351, lr=0.0100
[08/14 19:37:25 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8912, Val Loss=0.3629, lr=0.0100
[08/14 19:38:15 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9009, Val Loss=0.3238, lr=0.0100
[08/14 19:39:05 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8919, Val Loss=0.3688, lr=0.0100
[08/14 19:39:54 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9042, Val Loss=0.3270, lr=0.0100
[08/14 19:40:44 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.9006, Val Loss=0.3204, lr=0.0100
[08/14 19:41:33 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8937, Val Loss=0.3416, lr=0.0100
[08/14 19:42:22 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8954, Val Loss=0.3558, lr=0.0100
[08/14 19:43:12 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8989, Val Loss=0.3432, lr=0.0100
[08/14 19:44:01 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8945, Val Loss=0.3559, lr=0.0100
[08/14 19:44:50 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.9025, Val Loss=0.3336, lr=0.0100
[08/14 19:45:42 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9240, Val Loss=0.2551, lr=0.0010
[08/14 19:46:35 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9241, Val Loss=0.2582, lr=0.0010
[08/14 19:47:27 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9250, Val Loss=0.2586, lr=0.0010
[08/14 19:48:19 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9252, Val Loss=0.2580, lr=0.0010
[08/14 19:49:10 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9263, Val Loss=0.2560, lr=0.0010
[08/14 19:50:01 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9280, Val Loss=0.2544, lr=0.0010
[08/14 19:50:51 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9286, Val Loss=0.2596, lr=0.0010
[08/14 19:51:41 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9291, Val Loss=0.2599, lr=0.0010
[08/14 19:52:32 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9268, Val Loss=0.2632, lr=0.0010
[08/14 19:53:22 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9282, Val Loss=0.2678, lr=0.0010
[08/14 19:54:12 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9276, Val Loss=0.2643, lr=0.0010
[08/14 19:55:03 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9279, Val Loss=0.2640, lr=0.0010
[08/14 19:55:53 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9274, Val Loss=0.2680, lr=0.0010
[08/14 19:56:43 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9255, Val Loss=0.2689, lr=0.0010
[08/14 19:57:34 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9277, Val Loss=0.2717, lr=0.0010
[08/14 19:58:25 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9269, Val Loss=0.2720, lr=0.0010
[08/14 19:59:15 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9294, Val Loss=0.2701, lr=0.0010
[08/14 20:00:06 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9278, Val Loss=0.2720, lr=0.0010
[08/14 20:00:56 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9291, Val Loss=0.2726, lr=0.0010
[08/14 20:01:46 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9300, Val Loss=0.2717, lr=0.0010
[08/14 20:02:36 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9287, Val Loss=0.2725, lr=0.0001
[08/14 20:03:27 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9302, Val Loss=0.2696, lr=0.0001
[08/14 20:04:18 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9296, Val Loss=0.2724, lr=0.0001
[08/14 20:05:10 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9310, Val Loss=0.2705, lr=0.0001
[08/14 20:06:03 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9314, Val Loss=0.2707, lr=0.0001
[08/14 20:06:54 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9307, Val Loss=0.2703, lr=0.0001
[08/14 20:07:44 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9312, Val Loss=0.2693, lr=0.0001
[08/14 20:08:34 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9307, Val Loss=0.2708, lr=0.0001
[08/14 20:09:25 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9319, Val Loss=0.2712, lr=0.0001
[08/14 20:10:15 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9314, Val Loss=0.2719, lr=0.0001
[08/14 20:11:06 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9309, Val Loss=0.2706, lr=0.0001
[08/14 20:11:56 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9307, Val Loss=0.2716, lr=0.0001
[08/14 20:12:46 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9308, Val Loss=0.2740, lr=0.0001
[08/14 20:13:36 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9316, Val Loss=0.2732, lr=0.0001
[08/14 20:14:27 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9299, Val Loss=0.2735, lr=0.0001
[08/14 20:15:17 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9310, Val Loss=0.2700, lr=0.0001
[08/14 20:16:07 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9306, Val Loss=0.2723, lr=0.0001
[08/14 20:16:58 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9311, Val Loss=0.2711, lr=0.0001
[08/14 20:17:48 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9321, Val Loss=0.2727, lr=0.0001
[08/14 20:18:38 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9301, Val Loss=0.2726, lr=0.0001
[08/14 20:18:38 cifar10-global-group_norm-resnet56]: Best Acc=0.9321
[08/14 20:18:39 cifar10-global-group_norm-resnet56]: Params: 0.51 M
[08/14 20:18:39 cifar10-global-group_norm-resnet56]: ops: 57.71 M
[08/14 20:18:49 cifar10-global-group_norm-resnet56]: Acc: 0.9301 Val Loss: 0.2726

[08/14 20:18:49 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 28) layer3.8.conv1 has been rebuilt.
(2 of 28) layer3.7.conv1 has been rebuilt.
(3 of 28) layer3.6.conv1 has been rebuilt.
(4 of 28) layer3.5.conv1 has been rebuilt.
(5 of 28) layer3.4.conv1 has been rebuilt.
(6 of 28) layer3.3.conv1 has been rebuilt.
(7 of 28) layer3.2.conv1 has been rebuilt.
(8 of 28) layer3.1.conv1 has been rebuilt.
(9 of 28) layer3.0.conv1 has been rebuilt.
(10 of 28) layer2.8.conv1 has been rebuilt.
(11 of 28) layer2.7.conv1 has been rebuilt.
(12 of 28) layer2.6.conv1 has been rebuilt.
(13 of 28) layer2.5.conv1 has been rebuilt.
(14 of 28) layer2.4.conv1 has been rebuilt.
(15 of 28) layer2.3.conv1 has been rebuilt.
(16 of 28) layer2.2.conv1 has been rebuilt.
(17 of 28) layer2.1.conv1 has been rebuilt.
(18 of 28) layer2.0.conv1 has been rebuilt.
(19 of 28) layer1.7.conv1 has been rebuilt.
(20 of 28) layer1.6.conv1 has been rebuilt.
(21 of 28) layer1.5.conv1 has been rebuilt.
(22 of 28) layer1.4.conv1 has been rebuilt.
(23 of 28) layer1.3.conv1 has been rebuilt.
(24 of 28) layer1.2.conv1 has been rebuilt.
(25 of 28) layer1.1.conv1 has been rebuilt.
(26 of 28) conv1 has been rebuilt.
(27 of 28) layer2.0.downsample.0 has been rebuilt.
(28 of 28) layer3.0.downsample.0 has been rebuilt.
[08/14 20:18:50 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/14 20:19:01 cifar10-global-group_norm-resnet56]: Acc: 0.1943 Val Loss: 5.7395

[08/14 20:19:01 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/14 20:20:05 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.4448, Val Loss=1.5433, lr=0.0100
[08/14 20:21:09 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.5517, Val Loss=1.2586, lr=0.0100
[08/14 20:22:12 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.6136, Val Loss=1.0870, lr=0.0100
[08/14 20:23:15 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.6187, Val Loss=1.0698, lr=0.0100
[08/14 20:24:18 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.6298, Val Loss=1.0598, lr=0.0100
[08/14 20:25:21 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.6750, Val Loss=0.9442, lr=0.0100
[08/14 20:26:25 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.6564, Val Loss=0.9997, lr=0.0100
[08/14 20:27:29 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7144, Val Loss=0.8298, lr=0.0100
[08/14 20:28:33 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.7061, Val Loss=0.8907, lr=0.0100
[08/14 20:29:35 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7287, Val Loss=0.8097, lr=0.0100
[08/14 20:30:38 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.7574, Val Loss=0.7087, lr=0.0100
[08/14 20:31:41 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.7567, Val Loss=0.7305, lr=0.0100
[08/14 20:32:44 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7561, Val Loss=0.7285, lr=0.0100
[08/14 20:33:47 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.7705, Val Loss=0.6835, lr=0.0100
[08/14 20:35:02 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.7366, Val Loss=0.7849, lr=0.0100
[08/14 20:36:18 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.7773, Val Loss=0.6527, lr=0.0100
[08/14 20:37:21 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.7740, Val Loss=0.6888, lr=0.0100
[08/14 20:38:24 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.7579, Val Loss=0.7566, lr=0.0100
[08/14 20:39:28 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.7886, Val Loss=0.6339, lr=0.0100
[08/14 20:40:31 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8065, Val Loss=0.5794, lr=0.0100
[08/14 20:41:35 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.7979, Val Loss=0.6071, lr=0.0100
[08/14 20:42:38 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8049, Val Loss=0.5873, lr=0.0100
[08/14 20:43:41 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.7955, Val Loss=0.6307, lr=0.0100
[08/14 20:44:49 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.7962, Val Loss=0.6237, lr=0.0100
[08/14 20:45:51 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8133, Val Loss=0.5679, lr=0.0100
[08/14 20:46:55 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8028, Val Loss=0.5984, lr=0.0100
[08/14 20:47:58 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.7599, Val Loss=0.7445, lr=0.0100
[08/14 20:49:00 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.7918, Val Loss=0.6539, lr=0.0100
[08/14 20:50:04 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.7872, Val Loss=0.6643, lr=0.0100
[08/14 20:51:07 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.7995, Val Loss=0.6109, lr=0.0100
[08/14 20:52:11 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8171, Val Loss=0.5477, lr=0.0100
[08/14 20:53:13 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8256, Val Loss=0.5310, lr=0.0100
[08/14 20:54:16 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8053, Val Loss=0.6058, lr=0.0100
[08/14 20:55:20 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.7930, Val Loss=0.6490, lr=0.0100
[08/14 20:56:25 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8125, Val Loss=0.5540, lr=0.0100
[08/14 20:57:28 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8049, Val Loss=0.5996, lr=0.0100
[08/14 20:58:31 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8319, Val Loss=0.5050, lr=0.0100
[08/14 20:59:34 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8076, Val Loss=0.5860, lr=0.0100
[08/14 21:00:37 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8225, Val Loss=0.5292, lr=0.0100
[08/14 21:01:40 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8338, Val Loss=0.4986, lr=0.0100
[08/14 21:02:42 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8691, Val Loss=0.3925, lr=0.0010
[08/14 21:03:55 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8699, Val Loss=0.3892, lr=0.0010
[08/14 21:05:06 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8694, Val Loss=0.3857, lr=0.0010
[08/14 21:06:15 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8724, Val Loss=0.3874, lr=0.0010
[08/14 21:07:20 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8712, Val Loss=0.3836, lr=0.0010
[08/14 21:08:23 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8716, Val Loss=0.3862, lr=0.0010
[08/14 21:09:26 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8731, Val Loss=0.3894, lr=0.0010
[08/14 21:10:30 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8743, Val Loss=0.3861, lr=0.0010
[08/14 21:11:35 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8718, Val Loss=0.3878, lr=0.0010
[08/14 21:12:38 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8725, Val Loss=0.3889, lr=0.0010
[08/14 21:13:41 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8720, Val Loss=0.3889, lr=0.0010
[08/14 21:14:44 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8735, Val Loss=0.3866, lr=0.0010
[08/14 21:15:47 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8729, Val Loss=0.3932, lr=0.0010
[08/14 21:16:50 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8725, Val Loss=0.3887, lr=0.0010
[08/14 21:17:53 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8727, Val Loss=0.3951, lr=0.0010
[08/14 21:18:56 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8715, Val Loss=0.3924, lr=0.0010
[08/14 21:20:00 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8737, Val Loss=0.3969, lr=0.0010
[08/14 21:21:02 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8720, Val Loss=0.3929, lr=0.0010
[08/14 21:22:05 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8732, Val Loss=0.3963, lr=0.0010
[08/14 21:23:08 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8736, Val Loss=0.3967, lr=0.0010
[08/14 21:24:12 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8763, Val Loss=0.3901, lr=0.0001
[08/14 21:25:15 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8750, Val Loss=0.3892, lr=0.0001
[08/14 21:26:19 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8767, Val Loss=0.3885, lr=0.0001
[08/14 21:27:22 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8777, Val Loss=0.3884, lr=0.0001
[08/14 21:28:25 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8767, Val Loss=0.3913, lr=0.0001
[08/14 21:29:30 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8766, Val Loss=0.3894, lr=0.0001
[08/14 21:30:33 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8775, Val Loss=0.3891, lr=0.0001
[08/14 21:31:36 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8769, Val Loss=0.3900, lr=0.0001
[08/14 21:32:39 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8758, Val Loss=0.3905, lr=0.0001
[08/14 21:33:41 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8768, Val Loss=0.3912, lr=0.0001
[08/14 21:34:45 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8762, Val Loss=0.3885, lr=0.0001
[08/14 21:35:48 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8761, Val Loss=0.3885, lr=0.0001
[08/14 21:36:51 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8771, Val Loss=0.3875, lr=0.0001
[08/14 21:37:54 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8771, Val Loss=0.3903, lr=0.0001
[08/14 21:38:57 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8762, Val Loss=0.3903, lr=0.0001
[08/14 21:40:00 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8758, Val Loss=0.3890, lr=0.0001
[08/14 21:41:03 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8763, Val Loss=0.3891, lr=0.0001
[08/14 21:42:07 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8762, Val Loss=0.3897, lr=0.0001
[08/14 21:43:10 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8768, Val Loss=0.3917, lr=0.0001
[08/14 21:44:13 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.8766, Val Loss=0.3897, lr=0.0001
[08/14 21:44:13 cifar10-global-group_norm-resnet56]: Best Acc=0.8777
[08/14 21:44:13 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/14 21:44:14 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/14 21:44:25 cifar10-global-group_norm-resnet56]: Acc: 0.1000 Val Loss: 12.2184

[08/14 21:44:25 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/14 21:45:22 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.6368, Val Loss=1.0552, lr=0.0100
[08/14 21:46:21 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.7355, Val Loss=0.7960, lr=0.0100
[08/14 21:47:20 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.7856, Val Loss=0.6199, lr=0.0100
[08/14 21:48:19 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8050, Val Loss=0.5843, lr=0.0100
[08/14 21:49:16 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8335, Val Loss=0.4865, lr=0.0100
[08/14 21:50:14 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8478, Val Loss=0.4532, lr=0.0100
[08/14 21:51:11 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8549, Val Loss=0.4243, lr=0.0100
[08/14 21:52:08 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8397, Val Loss=0.4803, lr=0.0100
[08/14 21:53:05 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8391, Val Loss=0.4749, lr=0.0100
[08/14 21:54:03 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8589, Val Loss=0.4329, lr=0.0100
[08/14 21:55:00 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8543, Val Loss=0.4326, lr=0.0100
[08/14 21:55:57 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8671, Val Loss=0.4028, lr=0.0100
[08/14 21:56:54 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8773, Val Loss=0.3876, lr=0.0100
[08/14 21:57:51 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8801, Val Loss=0.3492, lr=0.0100
[08/14 21:58:48 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8863, Val Loss=0.3447, lr=0.0100
[08/14 21:59:45 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8662, Val Loss=0.4258, lr=0.0100
[08/14 22:00:43 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8671, Val Loss=0.3968, lr=0.0100
[08/14 22:01:41 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8866, Val Loss=0.3495, lr=0.0100
[08/14 22:02:40 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8913, Val Loss=0.3357, lr=0.0100
[08/14 22:03:38 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8773, Val Loss=0.3994, lr=0.0100
[08/14 22:04:36 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8798, Val Loss=0.3810, lr=0.0100
[08/14 22:05:35 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8797, Val Loss=0.3920, lr=0.0100
[08/14 22:06:33 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8857, Val Loss=0.3680, lr=0.0100
[08/14 22:07:32 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8930, Val Loss=0.3352, lr=0.0100
[08/14 22:08:30 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8880, Val Loss=0.3623, lr=0.0100
[08/14 22:09:29 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8893, Val Loss=0.3521, lr=0.0100
[08/14 22:10:27 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8889, Val Loss=0.3817, lr=0.0100
[08/14 22:11:26 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8767, Val Loss=0.3957, lr=0.0100
[08/14 22:12:25 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8863, Val Loss=0.3609, lr=0.0100
[08/14 22:13:23 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8695, Val Loss=0.4216, lr=0.0100
[08/14 22:14:22 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8915, Val Loss=0.3361, lr=0.0100
[08/14 22:15:20 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8800, Val Loss=0.3908, lr=0.0100
[08/14 22:16:19 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8963, Val Loss=0.3372, lr=0.0100
[08/14 22:17:17 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9007, Val Loss=0.3205, lr=0.0100
[08/14 22:18:18 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8872, Val Loss=0.3646, lr=0.0100
[08/14 22:19:19 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8974, Val Loss=0.3338, lr=0.0100
[08/14 22:20:20 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8922, Val Loss=0.3494, lr=0.0100
[08/14 22:21:20 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8892, Val Loss=0.3719, lr=0.0100
[08/14 22:22:18 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8947, Val Loss=0.3532, lr=0.0100
[08/14 22:23:17 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8958, Val Loss=0.3508, lr=0.0100
[08/14 22:24:15 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9241, Val Loss=0.2494, lr=0.0010
[08/14 22:25:14 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9271, Val Loss=0.2471, lr=0.0010
[08/14 22:26:12 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9274, Val Loss=0.2469, lr=0.0010
[08/14 22:27:11 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9294, Val Loss=0.2483, lr=0.0010
[08/14 22:28:09 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9278, Val Loss=0.2504, lr=0.0010
[08/14 22:29:08 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9276, Val Loss=0.2523, lr=0.0010
[08/14 22:30:06 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9282, Val Loss=0.2538, lr=0.0010
[08/14 22:31:05 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9273, Val Loss=0.2555, lr=0.0010
[08/14 22:32:03 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9287, Val Loss=0.2542, lr=0.0010
[08/14 22:33:03 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9297, Val Loss=0.2594, lr=0.0010
[08/14 22:34:01 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9295, Val Loss=0.2633, lr=0.0010
[08/14 22:35:00 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9280, Val Loss=0.2655, lr=0.0010
[08/14 22:35:59 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9254, Val Loss=0.2663, lr=0.0010
[08/14 22:36:57 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9275, Val Loss=0.2658, lr=0.0010
[08/14 22:37:56 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9302, Val Loss=0.2647, lr=0.0010
[08/14 22:38:54 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9295, Val Loss=0.2640, lr=0.0010
[08/14 22:39:54 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9290, Val Loss=0.2697, lr=0.0010
[08/14 22:40:53 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9295, Val Loss=0.2719, lr=0.0010
[08/14 22:41:51 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9281, Val Loss=0.2754, lr=0.0010
[08/14 22:42:50 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9283, Val Loss=0.2743, lr=0.0010
[08/14 22:43:48 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9296, Val Loss=0.2706, lr=0.0001
[08/14 22:44:46 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9301, Val Loss=0.2690, lr=0.0001
[08/14 22:45:45 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9302, Val Loss=0.2700, lr=0.0001
[08/14 22:46:43 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9303, Val Loss=0.2684, lr=0.0001
[08/14 22:47:41 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9301, Val Loss=0.2685, lr=0.0001
[08/14 22:48:39 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9306, Val Loss=0.2708, lr=0.0001
[08/14 22:49:41 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9306, Val Loss=0.2700, lr=0.0001
[08/14 22:50:42 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9310, Val Loss=0.2685, lr=0.0001
[08/14 22:51:42 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9299, Val Loss=0.2698, lr=0.0001
[08/14 22:52:40 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9306, Val Loss=0.2694, lr=0.0001
[08/14 22:53:39 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9308, Val Loss=0.2717, lr=0.0001
[08/14 22:54:37 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9317, Val Loss=0.2716, lr=0.0001
[08/14 22:55:43 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9295, Val Loss=0.2708, lr=0.0001
[08/14 22:56:47 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9307, Val Loss=0.2712, lr=0.0001
[08/14 22:57:45 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9300, Val Loss=0.2722, lr=0.0001
[08/14 22:58:43 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9309, Val Loss=0.2700, lr=0.0001
[08/14 22:59:41 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9304, Val Loss=0.2717, lr=0.0001
[08/14 23:00:39 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9307, Val Loss=0.2709, lr=0.0001
[08/14 23:01:37 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9289, Val Loss=0.2736, lr=0.0001
[08/14 23:02:36 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9301, Val Loss=0.2713, lr=0.0001
[08/14 23:02:36 cifar10-global-group_norm-resnet56]: Best Acc=0.9317

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.3 --global-pruning --total-epochs 80
Files already downloaded and verified
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: mode: prune
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: model: resnet56
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: verbose: False
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: method: group_norm
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: sparsity: 0.3
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: seed: None
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: device: cuda
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/14 23:53:51 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/14 23:54:17 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 [3, 5, 6, 16, 23, 27, 28, 30, 33, 47]
layer2.0.downsample.0 [0, 1, 9, 11, 12, 25]
conv1 [6, 7, 9, 10, 13]
layer1.0.conv1 [0, 3, 5, 6, 7, 15]
layer1.1.conv1 [0, 3, 5, 7, 12, 13]
layer1.2.conv1 [0, 3, 6, 8, 9, 11, 14]
layer1.3.conv1 [3, 5, 10, 11, 12]
layer1.4.conv1 [1, 2, 6, 7, 8, 11]
layer1.5.conv1 [0, 2, 6, 11, 13]
layer1.6.conv1 [0, 4, 5, 7, 13, 14]
layer1.7.conv1 [3, 5, 7, 9, 12]
layer1.8.conv1 [0, 5, 6, 7, 8, 9, 11, 15]
layer2.0.conv1 [1, 3, 9, 11, 13, 30, 31]
layer2.1.conv1 [1, 3, 4, 11, 16, 19, 22, 25, 26, 29, 30, 31]
layer2.2.conv1 [8, 12, 13, 16, 20, 21, 23]
layer2.3.conv1 [11, 17, 18, 26, 28]
layer2.4.conv1 [0, 1, 5, 9, 15, 16, 22, 23, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29, 31]
layer2.6.conv1 [2, 4, 5, 6, 7, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 [1, 4, 9, 22, 29, 47, 50, 57]
layer3.1.conv1 [2, 10, 21, 23, 25, 27, 29, 33, 37, 46, 55, 56, 60, 63]
layer3.2.conv1 [2, 3, 11, 20, 29, 30, 33, 36, 38, 42, 45, 48, 49, 57]
layer3.3.conv1 [2, 4, 6, 7, 8, 10, 21, 24, 31, 33, 39, 44, 46, 47, 49, 55, 58, 60, 61]
layer3.4.conv1 [0, 3, 14, 18, 26, 30, 32, 45, 49, 50, 52, 54, 59]
layer3.5.conv1 [0, 3, 5, 6, 8, 9, 10, 12, 14, 19, 20, 26, 28, 38, 40, 45, 46, 49, 52, 60, 63]
layer3.6.conv1 [3, 9, 12, 14, 22, 24, 26, 31, 45, 47, 49, 56, 57, 60, 63]
layer3.7.conv1 [0, 2, 8, 9, 19, 21, 23, 27, 31, 33, 34, 37, 39, 42, 46, 50, 51, 52, 54, 55, 57, 63]
layer3.8.conv1 [4, 17, 18, 24, 31, 34, 35, 38, 41, 42, 45, 46, 51, 56, 60]
=> Start history generation
layer3.8.conv1 [3, 5, 6, 16, 23, 27, 28, 30, 33, 47]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 [3, 5, 6, 16, 23, 27, 28, 30, 33, 47]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 [3, 5, 6, 16, 23, 27, 28, 30, 33, 47]
layer3.2.conv1 [3, 5, 6, 16, 23, 27, 28, 30, 33, 47]
layer3.1.conv1 [3, 5, 6, 16, 23, 27, 28, 30, 33, 47]
layer3.0.conv1 [0, 1, 9, 11, 12, 25]
layer2.8.conv1 [0, 1, 9, 11, 12, 25]
layer2.7.conv1 [0, 1, 9, 11, 12, 25]
layer2.6.conv1 [0, 1, 9, 11, 12, 25]
layer2.5.conv1 [0, 1, 9, 11, 12, 25]
layer2.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31]
layer2.3.conv1 [0, 1, 9, 11, 12, 25]
layer2.2.conv1 [0, 1, 9, 11, 12, 25]
layer2.1.conv1 [0, 1, 9, 11, 12, 25]
layer2.0.conv1 [6, 7, 9, 10, 13]
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 [6, 7, 9, 10, 13]
layer1.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 [6, 7, 9, 10, 13]
layer1.3.conv1 [6, 7, 9, 10, 13]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer3.0.downsample.0 [0, 1, 9, 11, 12, 25]
[08/14 23:54:21 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 26, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(26, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(26, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(26, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(26, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(26, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(26, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(26, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(26, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(26, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(26, 54, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(54, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(54, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(54, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(45, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(54, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(54, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(43, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(54, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(54, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(54, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=54, out_features=10, bias=True)
)
[08/14 23:54:21 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/14 23:54:32 cifar10-global-group_norm-resnet56]: Acc: 0.2011 Val Loss: 3.6678

[08/14 23:54:32 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 [3, 4, 7, 8, 12, 22, 25, 44, 50]
layer2.0.downsample.0 [8, 13, 17, 23]
conv1 [1, 4, 6, 9, 10]
layer1.2.conv1 [0, 1, 5, 7]
layer1.3.conv1 [3, 4, 6, 10]
layer1.5.conv1 [0, 1, 10]
layer1.7.conv1 [1, 9, 10]
layer1.8.conv1 [2, 5]
layer2.0.conv1 [0, 3, 4, 5, 9, 13, 15, 17, 22]
layer2.1.conv1 [1, 4, 5, 9, 10, 12, 15, 16, 17]
layer2.2.conv1 [0, 8, 9, 10, 11, 17, 19, 20, 23, 24]
layer2.3.conv1 [1, 11, 12, 17, 21, 23, 24]
layer2.4.conv1 [5, 6, 8, 12, 16, 21]
layer2.5.conv1 [0, 1, 2, 3, 5, 6, 8]
layer2.6.conv1 [1, 4, 7, 9, 11, 12, 13]
layer2.7.conv1 [0, 2, 3, 5, 8, 9, 10]
layer2.8.conv1 [1, 2, 5]
layer3.0.conv1 [1, 4, 7, 21, 22, 24, 31, 33, 34, 35, 39, 41, 44, 48, 50, 52]
layer3.1.conv1 [2, 4, 9, 18, 22, 23, 27, 29, 31, 32, 36, 45, 48]
layer3.2.conv1 [0, 4, 7, 14, 15, 17, 22, 23, 25, 33, 34, 38, 47, 48]
layer3.3.conv1 [4, 5, 9, 10, 18, 22, 24, 28, 29, 30, 36, 43]
layer3.4.conv1 [2, 14, 15, 16, 22, 25, 28, 34, 35, 36, 41, 47, 50]
layer3.5.conv1 [0, 6, 9, 19, 20, 32, 33, 40, 42]
layer3.6.conv1 [0, 8, 9, 11, 12, 14, 16, 17, 18, 20, 29, 31, 35, 37, 38, 39, 41, 43, 45, 46, 47]
layer3.7.conv1 [0, 2, 6, 9, 14, 15, 17, 23, 25, 26, 27, 31, 34, 35, 37, 38, 39]
layer3.8.conv1 [4, 16, 20, 26, 29, 32, 33, 42, 45, 47]
=> Start history generation
layer3.8.conv1 [3, 4, 7, 8, 12, 22, 25, 44, 50]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
layer3.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
layer3.4.conv1 [3, 4, 7, 8, 12, 22, 25, 44, 50]
layer3.3.conv1 [3, 4, 7, 8, 12, 22, 25, 44, 50]
layer3.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]
layer3.1.conv1 [3, 4, 7, 8, 12, 22, 25, 44, 50]
layer3.0.conv1 [8, 13, 17, 23]
layer2.8.conv1 [8, 13, 17, 23]
layer2.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
layer2.6.conv1 [8, 13, 17, 23]
layer2.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
layer2.4.conv1 [8, 13, 17, 23]
layer2.3.conv1 [8, 13, 17, 23]
layer2.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
layer2.1.conv1 [8, 13, 17, 23]
layer2.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
layer1.8.conv1 [1, 4, 6, 9, 10]
layer1.7.conv1 [1, 4, 6, 9, 10]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
layer1.3.conv1 [1, 4, 6, 9, 10]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
conv1 []
layer2.0.downsample.0 [1, 4, 6, 9, 10]
layer3.0.downsample.0 [8, 13, 17, 23]
[08/14 23:54:34 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(6, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(6, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(22, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(22, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(22, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(22, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(22, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(22, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(22, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(22, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(22, 45, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(45, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(45, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(45, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(45, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(38, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(45, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(34, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(45, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(45, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(45, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(39, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=45, out_features=10, bias=True)
)
[08/14 23:54:45 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.29 M (34.29%)
[08/14 23:54:45 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 34.51 M (27.15%, 3.68X )
[08/14 23:54:45 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.1148
[08/14 23:54:45 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 2.6658
[08/14 23:54:45 cifar10-global-group_norm-resnet56]: Finetuning...
[08/14 23:55:33 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8006, Val Loss=0.6090, lr=0.0100
[08/14 23:56:18 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8227, Val Loss=0.5369, lr=0.0100
[08/14 23:57:03 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8369, Val Loss=0.4848, lr=0.0100
[08/14 23:57:47 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8210, Val Loss=0.5774, lr=0.0100
[08/14 23:58:33 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8405, Val Loss=0.4950, lr=0.0100
[08/14 23:59:19 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8506, Val Loss=0.4505, lr=0.0100
[08/15 00:00:04 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8602, Val Loss=0.4271, lr=0.0100
[08/15 00:00:49 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8635, Val Loss=0.4040, lr=0.0100
[08/15 00:01:33 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8804, Val Loss=0.3666, lr=0.0100
[08/15 00:02:18 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8752, Val Loss=0.3830, lr=0.0100
[08/15 00:03:04 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8794, Val Loss=0.3847, lr=0.0100
[08/15 00:03:48 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8629, Val Loss=0.4184, lr=0.0100
[08/15 00:04:32 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8791, Val Loss=0.3717, lr=0.0100
[08/15 00:05:17 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8761, Val Loss=0.3782, lr=0.0100
[08/15 00:06:03 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8772, Val Loss=0.3852, lr=0.0100
[08/15 00:06:48 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8748, Val Loss=0.3918, lr=0.0100
[08/15 00:07:32 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8766, Val Loss=0.3904, lr=0.0100
[08/15 00:08:17 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8885, Val Loss=0.3387, lr=0.0100
[08/15 00:09:02 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8829, Val Loss=0.3687, lr=0.0100
[08/15 00:09:47 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8563, Val Loss=0.4737, lr=0.0100
[08/15 00:10:31 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8843, Val Loss=0.3655, lr=0.0100
[08/15 00:11:15 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8339, Val Loss=0.5762, lr=0.0100
[08/15 00:12:01 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8811, Val Loss=0.3732, lr=0.0100
[08/15 00:12:45 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8715, Val Loss=0.4071, lr=0.0100
[08/15 00:13:29 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8665, Val Loss=0.4463, lr=0.0100
[08/15 00:14:13 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8814, Val Loss=0.3787, lr=0.0100
[08/15 00:14:56 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8721, Val Loss=0.4040, lr=0.0100
[08/15 00:15:40 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8702, Val Loss=0.4123, lr=0.0100
[08/15 00:16:24 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8701, Val Loss=0.3969, lr=0.0100
[08/15 00:17:08 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8810, Val Loss=0.3909, lr=0.0100
[08/15 00:17:52 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8868, Val Loss=0.3610, lr=0.0100
[08/15 00:18:36 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8875, Val Loss=0.3631, lr=0.0100
[08/15 00:19:20 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8798, Val Loss=0.3889, lr=0.0100
[08/15 00:20:04 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8809, Val Loss=0.3787, lr=0.0100
[08/15 00:20:48 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8838, Val Loss=0.3672, lr=0.0100
[08/15 00:21:32 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8780, Val Loss=0.3968, lr=0.0100
[08/15 00:22:16 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8728, Val Loss=0.4234, lr=0.0100
[08/15 00:23:00 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8797, Val Loss=0.3946, lr=0.0100
[08/15 00:23:44 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8852, Val Loss=0.3872, lr=0.0100
[08/15 00:24:27 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8765, Val Loss=0.4246, lr=0.0100
[08/15 00:25:11 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9174, Val Loss=0.2662, lr=0.0010
[08/15 00:25:58 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9191, Val Loss=0.2611, lr=0.0010
[08/15 00:26:45 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9173, Val Loss=0.2696, lr=0.0010
[08/15 00:27:32 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9207, Val Loss=0.2671, lr=0.0010
[08/15 00:28:17 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9179, Val Loss=0.2689, lr=0.0010
[08/15 00:29:01 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9221, Val Loss=0.2664, lr=0.0010
[08/15 00:29:45 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9203, Val Loss=0.2741, lr=0.0010
[08/15 00:30:30 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9187, Val Loss=0.2775, lr=0.0010
[08/15 00:31:17 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9194, Val Loss=0.2745, lr=0.0010
[08/15 00:32:02 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9194, Val Loss=0.2781, lr=0.0010
[08/15 00:32:47 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9202, Val Loss=0.2798, lr=0.0010
[08/15 00:33:31 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9200, Val Loss=0.2773, lr=0.0010
[08/15 00:34:16 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9188, Val Loss=0.2830, lr=0.0010
[08/15 00:35:01 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9185, Val Loss=0.2845, lr=0.0010
[08/15 00:35:46 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9196, Val Loss=0.2861, lr=0.0010
[08/15 00:36:30 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9176, Val Loss=0.2944, lr=0.0010
[08/15 00:37:15 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9195, Val Loss=0.2874, lr=0.0010
[08/15 00:38:00 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9154, Val Loss=0.2954, lr=0.0010
[08/15 00:38:45 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9192, Val Loss=0.2972, lr=0.0010
[08/15 00:39:30 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9198, Val Loss=0.2937, lr=0.0010
[08/15 00:40:14 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9198, Val Loss=0.2901, lr=0.0001
[08/15 00:40:59 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9207, Val Loss=0.2911, lr=0.0001
[08/15 00:41:44 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9195, Val Loss=0.2894, lr=0.0001
[08/15 00:42:29 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9194, Val Loss=0.2916, lr=0.0001
[08/15 00:43:13 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9193, Val Loss=0.2907, lr=0.0001
[08/15 00:43:58 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9198, Val Loss=0.2900, lr=0.0001
[08/15 00:44:43 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9195, Val Loss=0.2902, lr=0.0001
[08/15 00:45:28 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9197, Val Loss=0.2877, lr=0.0001
[08/15 00:46:13 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9204, Val Loss=0.2905, lr=0.0001
[08/15 00:46:58 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9202, Val Loss=0.2889, lr=0.0001
[08/15 00:47:43 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9196, Val Loss=0.2894, lr=0.0001
[08/15 00:48:27 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9196, Val Loss=0.2905, lr=0.0001
[08/15 00:49:12 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9197, Val Loss=0.2894, lr=0.0001
[08/15 00:49:57 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9196, Val Loss=0.2906, lr=0.0001
[08/15 00:50:42 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9203, Val Loss=0.2892, lr=0.0001
[08/15 00:51:27 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9199, Val Loss=0.2899, lr=0.0001
[08/15 00:52:12 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9189, Val Loss=0.2905, lr=0.0001
[08/15 00:52:57 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9205, Val Loss=0.2905, lr=0.0001
[08/15 00:53:41 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9194, Val Loss=0.2912, lr=0.0001
[08/15 00:54:26 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9193, Val Loss=0.2894, lr=0.0001
[08/15 00:54:26 cifar10-global-group_norm-resnet56]: Best Acc=0.9221
[08/15 00:54:26 cifar10-global-group_norm-resnet56]: Params: 0.29 M
[08/15 00:54:26 cifar10-global-group_norm-resnet56]: ops: 34.51 M
[08/15 00:54:37 cifar10-global-group_norm-resnet56]: Acc: 0.9193 Val Loss: 0.2894

[08/15 00:54:37 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 26) layer3.8.conv1 has been rebuilt.
(2 of 26) layer3.7.conv1 has been rebuilt.
(3 of 26) layer3.6.conv1 has been rebuilt.
(4 of 26) layer3.5.conv1 has been rebuilt.
(5 of 26) layer3.4.conv1 has been rebuilt.
(6 of 26) layer3.3.conv1 has been rebuilt.
(7 of 26) layer3.2.conv1 has been rebuilt.
(8 of 26) layer3.1.conv1 has been rebuilt.
(9 of 26) layer3.0.conv1 has been rebuilt.
(10 of 26) layer2.8.conv1 has been rebuilt.
(11 of 26) layer2.7.conv1 has been rebuilt.
(12 of 26) layer2.6.conv1 has been rebuilt.
(13 of 26) layer2.5.conv1 has been rebuilt.
(14 of 26) layer2.4.conv1 has been rebuilt.
(15 of 26) layer2.3.conv1 has been rebuilt.
(16 of 26) layer2.2.conv1 has been rebuilt.
(17 of 26) layer2.1.conv1 has been rebuilt.
(18 of 26) layer2.0.conv1 has been rebuilt.
(19 of 26) layer1.8.conv1 has been rebuilt.
(20 of 26) layer1.7.conv1 has been rebuilt.
(21 of 26) layer1.5.conv1 has been rebuilt.
(22 of 26) layer1.3.conv1 has been rebuilt.
(23 of 26) layer1.2.conv1 has been rebuilt.
(24 of 26) conv1 has been rebuilt.
(25 of 26) layer2.0.downsample.0 has been rebuilt.
(26 of 26) layer3.0.downsample.0 has been rebuilt.
[08/15 00:54:38 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/15 00:54:49 cifar10-global-group_norm-resnet56]: Acc: 0.2818 Val Loss: 3.2069

[08/15 00:54:49 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/15 00:56:01 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.4854, Val Loss=1.4320, lr=0.0100
[08/15 00:57:19 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.6087, Val Loss=1.1087, lr=0.0100
[08/15 00:58:33 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.6557, Val Loss=1.0197, lr=0.0100
[08/15 00:59:49 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.7159, Val Loss=0.8190, lr=0.0100
[08/15 01:01:01 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.7293, Val Loss=0.7816, lr=0.0100
[08/15 01:02:13 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.7467, Val Loss=0.7569, lr=0.0100
[08/15 01:03:25 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.7317, Val Loss=0.8033, lr=0.0100
[08/15 01:04:37 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7842, Val Loss=0.6500, lr=0.0100
[08/15 01:05:49 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.7201, Val Loss=0.9314, lr=0.0100
[08/15 01:07:01 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7787, Val Loss=0.6637, lr=0.0100
[08/15 01:08:13 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8008, Val Loss=0.5957, lr=0.0100
[08/15 01:09:25 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8006, Val Loss=0.5883, lr=0.0100
[08/15 01:10:38 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7820, Val Loss=0.6831, lr=0.0100
[08/15 01:11:50 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8217, Val Loss=0.5395, lr=0.0100
[08/15 01:13:02 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8313, Val Loss=0.4997, lr=0.0100
[08/15 01:14:16 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8154, Val Loss=0.5782, lr=0.0100
[08/15 01:15:28 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8351, Val Loss=0.5016, lr=0.0100
[08/15 01:16:42 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8272, Val Loss=0.5314, lr=0.0100
[08/15 01:17:54 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8350, Val Loss=0.4796, lr=0.0100
[08/15 01:19:06 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8162, Val Loss=0.5575, lr=0.0100
[08/15 01:20:18 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8332, Val Loss=0.5157, lr=0.0100
[08/15 01:21:30 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8449, Val Loss=0.4775, lr=0.0100
[08/15 01:22:46 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8396, Val Loss=0.4738, lr=0.0100
[08/15 01:24:01 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8386, Val Loss=0.4992, lr=0.0100
[08/15 01:25:14 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8157, Val Loss=0.5630, lr=0.0100
[08/15 01:26:27 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8523, Val Loss=0.4571, lr=0.0100
[08/15 01:27:39 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8288, Val Loss=0.5392, lr=0.0100
[08/15 01:28:51 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8410, Val Loss=0.4838, lr=0.0100
[08/15 01:30:04 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8333, Val Loss=0.5097, lr=0.0100
[08/15 01:31:16 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8413, Val Loss=0.5113, lr=0.0100
[08/15 01:32:28 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8469, Val Loss=0.4838, lr=0.0100
[08/15 01:33:40 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8472, Val Loss=0.4443, lr=0.0100
[08/15 01:34:53 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8542, Val Loss=0.4517, lr=0.0100
[08/15 01:36:05 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8235, Val Loss=0.5597, lr=0.0100
[08/15 01:37:17 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8272, Val Loss=0.5313, lr=0.0100
[08/15 01:38:29 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8412, Val Loss=0.5066, lr=0.0100
[08/15 01:39:41 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8522, Val Loss=0.4464, lr=0.0100
[08/15 01:40:53 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8556, Val Loss=0.4564, lr=0.0100
[08/15 01:42:05 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.7933, Val Loss=0.6932, lr=0.0100
[08/15 01:43:18 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8555, Val Loss=0.4308, lr=0.0100
[08/15 01:44:30 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8935, Val Loss=0.3293, lr=0.0010
[08/15 01:45:42 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8950, Val Loss=0.3158, lr=0.0010
[08/15 01:46:54 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8955, Val Loss=0.3212, lr=0.0010
[08/15 01:48:06 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8965, Val Loss=0.3174, lr=0.0010
[08/15 01:49:22 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8989, Val Loss=0.3141, lr=0.0010
[08/15 01:50:42 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8963, Val Loss=0.3162, lr=0.0010
[08/15 01:51:55 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8991, Val Loss=0.3151, lr=0.0010
[08/15 01:53:08 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8971, Val Loss=0.3192, lr=0.0010
[08/15 01:54:23 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8989, Val Loss=0.3176, lr=0.0010
[08/15 01:55:36 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8982, Val Loss=0.3189, lr=0.0010
[08/15 01:56:50 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8975, Val Loss=0.3216, lr=0.0010
[08/15 01:58:03 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8977, Val Loss=0.3208, lr=0.0010
[08/15 01:59:16 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8975, Val Loss=0.3239, lr=0.0010
[08/15 02:00:28 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8953, Val Loss=0.3247, lr=0.0010
[08/15 02:01:41 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8985, Val Loss=0.3209, lr=0.0010
[08/15 02:02:53 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8969, Val Loss=0.3232, lr=0.0010
[08/15 02:04:04 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8977, Val Loss=0.3289, lr=0.0010
[08/15 02:05:16 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8976, Val Loss=0.3308, lr=0.0010
[08/15 02:06:27 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8949, Val Loss=0.3331, lr=0.0010
[08/15 02:07:38 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8951, Val Loss=0.3288, lr=0.0010
[08/15 02:08:49 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8970, Val Loss=0.3262, lr=0.0001
[08/15 02:10:00 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8981, Val Loss=0.3242, lr=0.0001
[08/15 02:11:11 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8987, Val Loss=0.3219, lr=0.0001
[08/15 02:12:22 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8981, Val Loss=0.3242, lr=0.0001
[08/15 02:13:33 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8984, Val Loss=0.3232, lr=0.0001
[08/15 02:14:44 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8985, Val Loss=0.3227, lr=0.0001
[08/15 02:15:55 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8993, Val Loss=0.3227, lr=0.0001
[08/15 02:17:07 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8986, Val Loss=0.3241, lr=0.0001
[08/15 02:18:18 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8992, Val Loss=0.3232, lr=0.0001
[08/15 02:19:29 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8991, Val Loss=0.3237, lr=0.0001
[08/15 02:20:40 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8989, Val Loss=0.3242, lr=0.0001
[08/15 02:21:51 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8991, Val Loss=0.3232, lr=0.0001
[08/15 02:23:03 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8992, Val Loss=0.3254, lr=0.0001
[08/15 02:24:14 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8984, Val Loss=0.3250, lr=0.0001
[08/15 02:25:28 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8993, Val Loss=0.3260, lr=0.0001
[08/15 02:26:40 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8985, Val Loss=0.3263, lr=0.0001
[08/15 02:27:53 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8982, Val Loss=0.3256, lr=0.0001
[08/15 02:29:06 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8994, Val Loss=0.3258, lr=0.0001
[08/15 02:30:21 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8993, Val Loss=0.3269, lr=0.0001
[08/15 02:31:39 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9003, Val Loss=0.3265, lr=0.0001
[08/15 02:31:39 cifar10-global-group_norm-resnet56]: Best Acc=0.9003
[08/15 02:31:39 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/15 02:31:40 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/15 02:31:52 cifar10-global-group_norm-resnet56]: Acc: 0.0980 Val Loss: 4.4072

[08/15 02:31:52 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/15 02:32:52 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8242, Val Loss=0.5479, lr=0.0100
[08/15 02:33:51 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8704, Val Loss=0.3834, lr=0.0100
[08/15 02:34:49 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8605, Val Loss=0.4323, lr=0.0100
[08/15 02:35:48 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8925, Val Loss=0.3386, lr=0.0100
[08/15 02:36:46 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8967, Val Loss=0.3236, lr=0.0100
[08/15 02:37:45 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8906, Val Loss=0.3523, lr=0.0100
[08/15 02:38:43 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8986, Val Loss=0.3122, lr=0.0100
[08/15 02:39:42 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.9033, Val Loss=0.3094, lr=0.0100
[08/15 02:40:40 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8978, Val Loss=0.3297, lr=0.0100
[08/15 02:41:38 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.9045, Val Loss=0.3051, lr=0.0100
[08/15 02:42:37 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8901, Val Loss=0.3565, lr=0.0100
[08/15 02:43:35 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8969, Val Loss=0.3297, lr=0.0100
[08/15 02:44:33 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.9021, Val Loss=0.3168, lr=0.0100
[08/15 02:45:31 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8976, Val Loss=0.3371, lr=0.0100
[08/15 02:46:30 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.9026, Val Loss=0.3344, lr=0.0100
[08/15 02:47:28 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.9048, Val Loss=0.3089, lr=0.0100
[08/15 02:48:27 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9142, Val Loss=0.2913, lr=0.0100
[08/15 02:49:26 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8897, Val Loss=0.3759, lr=0.0100
[08/15 02:50:25 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.9077, Val Loss=0.3146, lr=0.0100
[08/15 02:51:23 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.9087, Val Loss=0.3177, lr=0.0100
[08/15 02:52:21 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.9021, Val Loss=0.3417, lr=0.0100
[08/15 02:53:20 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9076, Val Loss=0.2935, lr=0.0100
[08/15 02:54:18 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.9055, Val Loss=0.3179, lr=0.0100
[08/15 02:55:17 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.9044, Val Loss=0.3219, lr=0.0100
[08/15 02:56:15 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.9064, Val Loss=0.3056, lr=0.0100
[08/15 02:57:14 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8954, Val Loss=0.3686, lr=0.0100
[08/15 02:58:12 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.9087, Val Loss=0.3093, lr=0.0100
[08/15 02:59:11 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.9023, Val Loss=0.3261, lr=0.0100
[08/15 03:00:09 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.9089, Val Loss=0.3277, lr=0.0100
[08/15 03:01:08 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8957, Val Loss=0.3602, lr=0.0100
[08/15 03:02:06 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.9042, Val Loss=0.3338, lr=0.0100
[08/15 03:03:05 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9036, Val Loss=0.3282, lr=0.0100
[08/15 03:04:03 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.9002, Val Loss=0.3451, lr=0.0100
[08/15 03:05:01 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9026, Val Loss=0.3192, lr=0.0100
[08/15 03:06:00 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.9026, Val Loss=0.3431, lr=0.0100
[08/15 03:06:58 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8996, Val Loss=0.3300, lr=0.0100
[08/15 03:07:57 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8972, Val Loss=0.3571, lr=0.0100
[08/15 03:08:55 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9145, Val Loss=0.2929, lr=0.0100
[08/15 03:09:55 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.9056, Val Loss=0.3386, lr=0.0100
[08/15 03:10:57 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8986, Val Loss=0.3513, lr=0.0100
[08/15 03:11:59 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9301, Val Loss=0.2331, lr=0.0010
[08/15 03:12:57 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9318, Val Loss=0.2310, lr=0.0010
[08/15 03:13:56 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9314, Val Loss=0.2354, lr=0.0010
[08/15 03:14:54 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9327, Val Loss=0.2356, lr=0.0010
[08/15 03:15:53 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9342, Val Loss=0.2369, lr=0.0010
[08/15 03:16:51 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9333, Val Loss=0.2427, lr=0.0010
[08/15 03:17:50 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9341, Val Loss=0.2420, lr=0.0010
[08/15 03:18:48 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9350, Val Loss=0.2393, lr=0.0010
[08/15 03:19:46 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9341, Val Loss=0.2450, lr=0.0010
[08/15 03:20:44 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9343, Val Loss=0.2460, lr=0.0010
[08/15 03:21:43 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9336, Val Loss=0.2484, lr=0.0010
[08/15 03:22:41 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9327, Val Loss=0.2491, lr=0.0010
[08/15 03:23:40 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9357, Val Loss=0.2471, lr=0.0010
[08/15 03:24:38 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9345, Val Loss=0.2449, lr=0.0010
[08/15 03:25:37 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9348, Val Loss=0.2476, lr=0.0010
[08/15 03:26:36 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9350, Val Loss=0.2468, lr=0.0010
[08/15 03:27:34 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9353, Val Loss=0.2483, lr=0.0010
[08/15 03:28:33 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9350, Val Loss=0.2502, lr=0.0010
[08/15 03:29:31 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9345, Val Loss=0.2527, lr=0.0010
[08/15 03:30:29 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9347, Val Loss=0.2529, lr=0.0010
[08/15 03:31:28 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9345, Val Loss=0.2529, lr=0.0001
[08/15 03:32:26 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9355, Val Loss=0.2522, lr=0.0001
[08/15 03:33:25 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9349, Val Loss=0.2543, lr=0.0001
[08/15 03:34:23 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9347, Val Loss=0.2511, lr=0.0001
[08/15 03:35:22 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9349, Val Loss=0.2516, lr=0.0001
[08/15 03:36:21 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9348, Val Loss=0.2549, lr=0.0001
[08/15 03:37:19 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9348, Val Loss=0.2522, lr=0.0001
[08/15 03:38:17 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9346, Val Loss=0.2523, lr=0.0001
[08/15 03:39:15 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9351, Val Loss=0.2525, lr=0.0001
[08/15 03:40:14 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9350, Val Loss=0.2521, lr=0.0001
[08/15 03:41:12 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9360, Val Loss=0.2523, lr=0.0001
[08/15 03:42:13 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9354, Val Loss=0.2514, lr=0.0001
[08/15 03:43:14 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9354, Val Loss=0.2520, lr=0.0001
[08/15 03:44:14 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9356, Val Loss=0.2526, lr=0.0001
[08/15 03:45:12 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9359, Val Loss=0.2516, lr=0.0001
[08/15 03:46:11 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9359, Val Loss=0.2524, lr=0.0001
[08/15 03:47:09 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9354, Val Loss=0.2522, lr=0.0001
[08/15 03:48:08 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9358, Val Loss=0.2513, lr=0.0001
[08/15 03:49:06 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9344, Val Loss=0.2520, lr=0.0001
[08/15 03:50:05 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9360, Val Loss=0.2520, lr=0.0001
[08/15 03:50:05 cifar10-global-group_norm-resnet56]: Best Acc=0.9360

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.5 --global-pruning --tot
al-epochs 80
Files already downloaded and verified
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: mode: prune
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: model: resnet56
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: verbose: False
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: method: group_norm
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: sparsity: 0.5
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: seed: None
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: device: cuda
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/15 10:03:24 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/15 10:03:54 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 47, 51, 53, 54, 57, 58, 59, 60, 61]
layer2.0.downsample.0 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
conv1 [1, 2, 4, 6, 7, 8, 9, 10, 13, 14, 15]
layer1.0.conv1 [0, 3, 5, 6, 7, 14, 15]
layer1.1.conv1 [0, 3, 5, 6, 7, 11, 12, 13, 14]
layer1.2.conv1 [0, 1, 2, 3, 6, 8, 9, 10, 11, 13, 14]
layer1.3.conv1 [3, 4, 5, 6, 8, 10, 11, 12]
layer1.4.conv1 [1, 2, 5, 6, 7, 8, 11, 13, 15]
layer1.5.conv1 [0, 2, 6, 11, 13]
layer1.6.conv1 [0, 1, 2, 4, 5, 6, 7, 9, 13, 14]
layer1.7.conv1 [1, 3, 5, 7, 9, 12]
layer1.8.conv1 [0, 5, 6, 7, 8, 9, 11, 12, 15]
layer2.0.conv1 [0, 1, 3, 5, 7, 8, 9, 11, 13, 14, 18, 19, 20, 25, 26, 27, 30, 31]
layer2.1.conv1 [1, 2, 3, 4, 11, 16, 19, 22, 25, 26, 29, 30, 31]
layer2.2.conv1 [0, 8, 9, 11, 12, 13, 16, 20, 21, 23, 26, 27, 30, 31]
layer2.3.conv1 [11, 12, 17, 18, 20, 24, 26, 27, 28, 29]
layer2.4.conv1 [0, 1, 5, 8, 9, 15, 16, 18, 22, 23, 24, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29, 31]
layer2.6.conv1 [2, 4, 5, 6, 7, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 [1, 2, 3, 4, 6, 7, 9, 10, 12, 17, 18, 21, 22, 25, 26, 28, 29, 32, 34, 36, 38, 39, 40, 42, 44, 45, 46, 47, 49, 50, 51, 55, 56, 57, 58, 59, 60, 62]
layer3.1.conv1 [2, 3, 5, 6, 10, 11, 16, 20, 21, 23, 25, 27, 28, 29, 30, 33, 35, 37, 38, 40, 41, 45, 46, 53, 55, 56, 57, 59, 60, 61, 63]
layer3.2.conv1 [2, 3, 6, 9, 11, 17, 18, 20, 26, 27, 29, 30, 31, 33, 36, 38, 42, 43, 44, 45, 48, 49, 51, 57, 62]
layer3.3.conv1 [2, 4, 6, 7, 8, 9, 10, 11, 15, 16, 21, 24, 26, 30, 31, 33, 34, 39, 40, 41, 44, 46, 47, 49, 51, 55, 58, 60, 61,62]
layer3.4.conv1 [0, 3, 4, 14, 18, 19, 20, 21, 26, 27, 30, 31, 32, 35, 36, 39, 41, 42, 43, 45, 49, 50, 51, 52, 53, 54, 58, 59, 60, 63]
layer3.5.conv1 [0, 1, 3, 5, 6, 8, 9, 10, 12, 14, 18, 19, 20, 26, 28, 33, 38, 40, 45, 46, 49, 51, 52, 59, 60, 62, 63]
layer3.6.conv1 [3, 9, 11, 12, 14, 15, 16, 21, 22, 24, 26, 31, 37, 39, 43, 45, 46, 47, 48, 49, 52, 56, 57, 60, 61, 63]
layer3.7.conv1 [0, 1, 2, 4, 8, 9, 13, 18, 19, 21, 23, 24, 27, 31, 33, 34, 36, 37, 39, 42, 45, 46, 49, 50, 51, 52, 54, 55, 57,58, 59, 63]
layer3.8.conv1 [0, 1, 4, 5, 15, 17, 18, 19, 21, 22, 23, 24, 25, 30, 31, 32, 34, 35, 36, 37, 38, 40, 41, 42, 43, 45, 46, 50, 51, 55, 56, 57, 59, 60, 62]
=> Start history generation
layer3.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 47, 51, 53, 54, 57, 58, 59, 60, 61]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 47, 51, 53, 54, 57, 58, 59, 60, 61]
layer3.2.conv1 [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 47, 51, 53, 54, 57, 58, 59, 60, 61]
layer3.1.conv1 [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 47, 51, 53, 54, 57, 58, 59, 60, 61]
layer3.0.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.8.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.7.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.6.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.5.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31]
layer2.3.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30, 31]
layer2.1.conv1 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
layer2.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 [1, 2, 4, 6, 7, 8, 9, 10, 13, 14, 15]
layer1.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 [1, 2, 4, 6, 7, 8, 9, 10, 13, 14, 15]
layer1.3.conv1 [1, 2, 4, 6, 7, 8, 9, 10, 13, 14, 15]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer3.0.downsample.0 [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 18, 20, 22, 25, 26, 27, 29]
[08/15 10:03:55 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(5, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(5, 14, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(5, 13, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(13, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(13, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(13, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(13, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(13, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 26, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(13, 29, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(29, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(29, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(39, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(29, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(34, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(29, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(34, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(29, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(29, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(38, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(29, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=29, out_features=10, bias=True)
)
[08/15 10:03:55 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/15 10:04:15 cifar10-global-group_norm-resnet56]: Acc: 0.0999 Val Loss: 3.5491

[08/15 10:04:15 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 [1, 2, 4, 5, 6, 8, 9, 11, 13, 16, 20, 21, 25, 27]
layer2.0.downsample.0 [1, 3, 4, 8, 9, 11, 12]
conv1 [0, 2]
layer1.0.conv1 [2, 3, 5, 7, 8]
layer1.1.conv1 [3, 4, 5]
layer1.2.conv1 [0, 1, 3, 4]
layer1.3.conv1 [2, 3, 6, 7]
layer1.4.conv1 [1, 2, 4, 5, 6]
layer1.5.conv1 [2, 3, 5, 6, 8, 10]
layer1.6.conv1 [1, 2, 3]
layer1.8.conv1 [0, 1, 3, 5]
layer2.0.conv1 [2, 4, 9, 11, 13]
layer2.1.conv1 [3, 4, 6, 8, 9, 11, 12, 14, 15, 16, 17]
layer2.2.conv1 [1, 2, 5, 7, 8, 14]
layer2.3.conv1 [1, 4, 5, 6, 7, 8, 9, 11, 16, 18, 19]
layer2.4.conv1 [0, 2, 5, 6, 7, 8, 11, 12, 13, 14, 17, 18]
layer2.5.conv1 [0, 1, 2, 5, 6, 8]
layer2.6.conv1 [1, 4, 7, 9, 10, 11, 12, 13]
layer2.7.conv1 [0, 1, 2, 3, 5, 8, 9, 10]
layer2.8.conv1 [1, 2, 5]
layer3.0.conv1 [1, 3, 5, 7, 9, 11, 13, 18, 19, 23, 24]
layer3.1.conv1 [1, 3, 8, 10, 11, 14, 16, 17, 21, 22, 23, 25, 26, 28, 29, 30]
layer3.2.conv1 [0, 1, 2, 5, 6, 8, 9, 13, 16, 18, 22, 24, 26, 31, 32, 35, 37]
layer3.3.conv1 [0, 4, 5, 6, 8, 10, 14, 15, 18, 21, 23, 25, 27, 28, 29, 33]
layer3.4.conv1 [0, 1, 3, 5, 6, 10, 12, 13, 14, 15, 16, 18, 19, 20, 23, 24, 25, 28]
layer3.5.conv1 [1, 2, 3, 5, 7, 8, 9, 11, 14, 17, 18, 19, 20, 21, 22, 25, 29, 32, 34, 35]
layer3.6.conv1 [0, 1, 3, 6, 8, 11, 12, 13, 14, 16, 20, 24, 25, 30, 31, 32, 33, 34, 35, 36, 37]
layer3.7.conv1 [1, 4, 5, 11, 12, 13, 14, 18, 20, 21, 22, 23, 27, 28, 29, 30]
layer3.8.conv1 [1, 6, 10, 11, 13, 15, 18, 21, 22, 23, 24, 25, 27]
=> Start history generation
layer3.8.conv1 [1, 2, 4, 5, 6, 8, 9, 11, 13, 16, 20, 21, 25, 27]
layer3.7.conv1 [1, 2, 4, 5, 6, 8, 9, 11, 13, 16, 20, 21, 25, 27]
layer3.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
layer3.5.conv1 [1, 2, 4, 5, 6, 8, 9, 11, 13, 16, 20, 21, 25, 27]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
layer3.3.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
layer3.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]
layer3.1.conv1 [1, 2, 4, 5, 6, 8, 9, 11, 13, 16, 20, 21, 25, 27]
layer3.0.conv1 [1, 3, 4, 8, 9, 11, 12]
layer2.8.conv1 [1, 3, 4, 8, 9, 11, 12]
layer2.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
layer2.6.conv1 [1, 3, 4, 8, 9, 11, 12]
layer2.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
layer2.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
layer2.3.conv1 [1, 3, 4, 8, 9, 11, 12]
layer2.2.conv1 [1, 3, 4, 8, 9, 11, 12]
layer2.1.conv1 [1, 3, 4, 8, 9, 11, 12]
layer2.0.conv1 [0, 2]
layer1.8.conv1 [0, 1, 2, 3, 4]
layer1.6.conv1 [0, 2]
layer1.5.conv1 [0, 2]
layer1.4.conv1 [0, 2]
layer1.3.conv1 [0, 2]
layer1.2.conv1 [0, 1, 2, 3, 4]
layer1.1.conv1 [0, 2]
layer1.0.conv1 [0, 2]
conv1 [0, 1, 2]
layer2.0.downsample.0 [0, 2]
layer3.0.downsample.0 [1, 3, 4, 8, 9, 11, 12]
[08/15 10:04:15 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(3, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(3, 6, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(6, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=15, out_features=10, bias=True)
)
[08/15 10:04:25 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.05 M (5.97%)
[08/15 10:04:25 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 6.88 M (5.41%, 18.48X )
[08/15 10:04:25 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.1000
[08/15 10:04:25 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 2.4350
[08/15 10:04:26 cifar10-global-group_norm-resnet56]: Finetuning...
[08/15 10:05:11 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.3929, Val Loss=1.6799, lr=0.0100
[08/15 10:05:49 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.4662, Val Loss=1.5486, lr=0.0100
[08/15 10:06:29 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.3991, Val Loss=2.0228, lr=0.0100
[08/15 10:07:08 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.6176, Val Loss=1.1386, lr=0.0100
[08/15 10:07:48 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.6669, Val Loss=0.9412, lr=0.0100
[08/15 10:08:27 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.6254, Val Loss=1.0953, lr=0.0100
[08/15 10:09:06 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.6822, Val Loss=0.9120, lr=0.0100
[08/15 10:09:46 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.6490, Val Loss=0.9888, lr=0.0100
[08/15 10:10:25 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.6478, Val Loss=1.0369, lr=0.0100
[08/15 10:11:04 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7006, Val Loss=0.8590, lr=0.0100
[08/15 10:11:44 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.5963, Val Loss=1.2144, lr=0.0100
[08/15 10:12:23 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.6091, Val Loss=1.2819, lr=0.0100
[08/15 10:13:03 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7058, Val Loss=0.8641, lr=0.0100
[08/15 10:13:43 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.6555, Val Loss=1.0562, lr=0.0100
[08/15 10:14:22 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.5020, Val Loss=1.7511, lr=0.0100
[08/15 10:15:01 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.7385, Val Loss=0.7570, lr=0.0100
[08/15 10:15:40 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.7115, Val Loss=0.8793, lr=0.0100
[08/15 10:16:20 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.6856, Val Loss=0.9485, lr=0.0100
[08/15 10:16:59 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.7157, Val Loss=0.8924, lr=0.0100
[08/15 10:17:39 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.7522, Val Loss=0.7296, lr=0.0100
[08/15 10:18:19 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.6793, Val Loss=0.9589, lr=0.0100
[08/15 10:18:57 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.7472, Val Loss=0.7563, lr=0.0100
[08/15 10:19:36 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.6788, Val Loss=0.9638, lr=0.0100
[08/15 10:20:16 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.7371, Val Loss=0.7948, lr=0.0100
[08/15 10:20:55 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.7516, Val Loss=0.7293, lr=0.0100
[08/15 10:21:34 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.7186, Val Loss=0.8823, lr=0.0100
[08/15 10:22:13 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.7051, Val Loss=0.9322, lr=0.0100
[08/15 10:22:52 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.7415, Val Loss=0.7864, lr=0.0100
[08/15 10:23:31 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.7193, Val Loss=0.8527, lr=0.0100
[08/15 10:24:11 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.6846, Val Loss=1.0064, lr=0.0100
[08/15 10:24:50 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.6929, Val Loss=1.0161, lr=0.0100
[08/15 10:25:29 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.7775, Val Loss=0.6587, lr=0.0100
[08/15 10:26:09 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.7835, Val Loss=0.6391, lr=0.0100
[08/15 10:26:48 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.6855, Val Loss=1.0228, lr=0.0100
[08/15 10:27:27 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.6989, Val Loss=0.9411, lr=0.0100
[08/15 10:28:08 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.7220, Val Loss=0.8990, lr=0.0100
[08/15 10:28:47 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.7420, Val Loss=0.7455, lr=0.0100
[08/15 10:29:25 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.7797, Val Loss=0.6552, lr=0.0100
[08/15 10:30:05 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.6984, Val Loss=0.9716, lr=0.0100
[08/15 10:30:44 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.7403, Val Loss=0.7801, lr=0.0100
[08/15 10:31:23 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8263, Val Loss=0.5188, lr=0.0010
[08/15 10:32:03 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8290, Val Loss=0.5151, lr=0.0010
[08/15 10:32:42 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8307, Val Loss=0.5059, lr=0.0010
[08/15 10:33:21 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8246, Val Loss=0.5145, lr=0.0010
[08/15 10:34:01 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8298, Val Loss=0.5065, lr=0.0010
[08/15 10:34:40 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8303, Val Loss=0.5032, lr=0.0010
[08/15 10:35:19 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8322, Val Loss=0.5060, lr=0.0010
[08/15 10:35:58 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8304, Val Loss=0.5108, lr=0.0010
[08/15 10:36:38 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8274, Val Loss=0.5128, lr=0.0010
[08/15 10:37:21 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8327, Val Loss=0.5003, lr=0.0010
[08/15 10:38:02 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8319, Val Loss=0.5021, lr=0.0010
[08/15 10:38:41 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8334, Val Loss=0.5032, lr=0.0010
[08/15 10:39:20 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8345, Val Loss=0.4974, lr=0.0010
[08/15 10:40:00 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8335, Val Loss=0.5027, lr=0.0010
[08/15 10:40:39 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8354, Val Loss=0.5038, lr=0.0010
[08/15 10:41:18 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8322, Val Loss=0.5070, lr=0.0010
[08/15 10:41:57 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8336, Val Loss=0.5016, lr=0.0010
[08/15 10:42:37 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8346, Val Loss=0.4994, lr=0.0010
[08/15 10:43:16 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8332, Val Loss=0.5031, lr=0.0010
[08/15 10:43:55 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8354, Val Loss=0.4993, lr=0.0010
[08/15 10:44:35 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8396, Val Loss=0.4898, lr=0.0001
[08/15 10:45:14 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8401, Val Loss=0.4883, lr=0.0001
[08/15 10:45:53 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8389, Val Loss=0.4919, lr=0.0001
[08/15 10:46:33 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8410, Val Loss=0.4886, lr=0.0001
[08/15 10:47:12 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8392, Val Loss=0.4886, lr=0.0001
[08/15 10:47:51 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8402, Val Loss=0.4886, lr=0.0001
[08/15 10:48:30 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8390, Val Loss=0.4890, lr=0.0001
[08/15 10:49:10 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8401, Val Loss=0.4887, lr=0.0001
[08/15 10:49:49 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8396, Val Loss=0.4884, lr=0.0001
[08/15 10:50:28 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8394, Val Loss=0.4892, lr=0.0001
[08/15 10:51:07 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8408, Val Loss=0.4873, lr=0.0001
[08/15 10:51:47 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8407, Val Loss=0.4872, lr=0.0001
[08/15 10:52:26 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8415, Val Loss=0.4887, lr=0.0001
[08/15 10:53:09 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8389, Val Loss=0.4883, lr=0.0001
[08/15 10:53:51 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8419, Val Loss=0.4877, lr=0.0001
[08/15 10:54:31 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8405, Val Loss=0.4871, lr=0.0001
[08/15 10:55:10 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8395, Val Loss=0.4904, lr=0.0001
[08/15 10:55:50 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8387, Val Loss=0.4882, lr=0.0001
[08/15 10:56:29 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8418, Val Loss=0.4871, lr=0.0001
[08/15 10:57:08 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.8393, Val Loss=0.4889, lr=0.0001
[08/15 10:57:08 cifar10-global-group_norm-resnet56]: Best Acc=0.8419
[08/15 10:57:08 cifar10-global-group_norm-resnet56]: Params: 0.05 M
[08/15 10:57:08 cifar10-global-group_norm-resnet56]: ops: 6.88 M
[08/15 10:57:18 cifar10-global-group_norm-resnet56]: Acc: 0.8393 Val Loss: 0.4889

[08/15 10:57:18 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 29) layer3.8.conv1 has been rebuilt.
(2 of 29) layer3.7.conv1 has been rebuilt.
(3 of 29) layer3.6.conv1 has been rebuilt.
(4 of 29) layer3.5.conv1 has been rebuilt.
(5 of 29) layer3.4.conv1 has been rebuilt.
(6 of 29) layer3.3.conv1 has been rebuilt.
(7 of 29) layer3.2.conv1 has been rebuilt.
(8 of 29) layer3.1.conv1 has been rebuilt.
(9 of 29) layer3.0.conv1 has been rebuilt.
(10 of 29) layer2.8.conv1 has been rebuilt.
(11 of 29) layer2.7.conv1 has been rebuilt.
(12 of 29) layer2.6.conv1 has been rebuilt.
(13 of 29) layer2.5.conv1 has been rebuilt.
(14 of 29) layer2.4.conv1 has been rebuilt.
(15 of 29) layer2.3.conv1 has been rebuilt.
(16 of 29) layer2.2.conv1 has been rebuilt.
(17 of 29) layer2.1.conv1 has been rebuilt.
(18 of 29) layer2.0.conv1 has been rebuilt.
(19 of 29) layer1.8.conv1 has been rebuilt.
(20 of 29) layer1.6.conv1 has been rebuilt.
(21 of 29) layer1.5.conv1 has been rebuilt.
(22 of 29) layer1.4.conv1 has been rebuilt.
(23 of 29) layer1.3.conv1 has been rebuilt.
(24 of 29) layer1.2.conv1 has been rebuilt.
(25 of 29) layer1.1.conv1 has been rebuilt.
(26 of 29) layer1.0.conv1 has been rebuilt.
(27 of 29) conv1 has been rebuilt.
(28 of 29) layer2.0.downsample.0 has been rebuilt.
(29 of 29) layer3.0.downsample.0 has been rebuilt.
[08/15 10:57:18 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/15 10:57:29 cifar10-global-group_norm-resnet56]: Acc: 0.0869 Val Loss: 7.1779

[08/15 10:57:29 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/15 10:58:35 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.3892, Val Loss=1.7283, lr=0.0100
[08/15 10:59:41 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.4393, Val Loss=1.6394, lr=0.0100
[08/15 11:00:41 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.5205, Val Loss=1.4395, lr=0.0100
[08/15 11:01:40 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.6483, Val Loss=1.0096, lr=0.0100
[08/15 11:02:39 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.6172, Val Loss=1.1176, lr=0.0100
[08/15 11:03:42 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.6991, Val Loss=0.8557, lr=0.0100
[08/15 11:04:43 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.6496, Val Loss=1.0717, lr=0.0100
[08/15 11:05:44 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7087, Val Loss=0.8477, lr=0.0100
[08/15 11:06:42 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.7183, Val Loss=0.8358, lr=0.0100
[08/15 11:07:39 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7561, Val Loss=0.7235, lr=0.0100
[08/15 11:08:37 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.6850, Val Loss=1.0054, lr=0.0100
[08/15 11:09:35 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.7410, Val Loss=0.7569, lr=0.0100
[08/15 11:10:34 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7666, Val Loss=0.6994, lr=0.0100
[08/15 11:11:32 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.7852, Val Loss=0.6212, lr=0.0100
[08/15 11:12:30 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.7736, Val Loss=0.6800, lr=0.0100
[08/15 11:13:27 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.7742, Val Loss=0.6619, lr=0.0100
[08/15 11:14:25 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.7505, Val Loss=0.7640, lr=0.0100
[08/15 11:15:23 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.7636, Val Loss=0.7068, lr=0.0100
[08/15 11:16:21 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.7499, Val Loss=0.7389, lr=0.0100
[08/15 11:17:19 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.7439, Val Loss=0.8183, lr=0.0100
[08/15 11:18:16 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.7861, Val Loss=0.6431, lr=0.0100
[08/15 11:19:14 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.7867, Val Loss=0.6307, lr=0.0100
[08/15 11:20:12 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.7816, Val Loss=0.6466, lr=0.0100
[08/15 11:21:10 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.7764, Val Loss=0.6813, lr=0.0100
[08/15 11:22:08 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.7624, Val Loss=0.7153, lr=0.0100
[08/15 11:23:06 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.7557, Val Loss=0.7581, lr=0.0100
[08/15 11:24:06 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.7906, Val Loss=0.6253, lr=0.0100
[08/15 11:25:06 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.7955, Val Loss=0.6076, lr=0.0100
[08/15 11:26:05 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.7976, Val Loss=0.6025, lr=0.0100
[08/15 11:27:04 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.7866, Val Loss=0.6179, lr=0.0100
[08/15 11:28:03 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.7911, Val Loss=0.6141, lr=0.0100
[08/15 11:29:02 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8023, Val Loss=0.5938, lr=0.0100
[08/15 11:30:01 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.7901, Val Loss=0.6589, lr=0.0100
[08/15 11:30:59 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.7962, Val Loss=0.6093, lr=0.0100
[08/15 11:31:58 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8196, Val Loss=0.5398, lr=0.0100
[08/15 11:32:56 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.7886, Val Loss=0.6259, lr=0.0100
[08/15 11:33:55 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8018, Val Loss=0.5889, lr=0.0100
[08/15 11:34:54 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.6961, Val Loss=1.0447, lr=0.0100
[08/15 11:35:53 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.7960, Val Loss=0.6122, lr=0.0100
[08/15 11:36:52 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8061, Val Loss=0.5818, lr=0.0100
[08/15 11:37:50 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8557, Val Loss=0.4217, lr=0.0010
[08/15 11:38:53 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8587, Val Loss=0.4133, lr=0.0010
[08/15 11:39:57 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8628, Val Loss=0.4086, lr=0.0010
[08/15 11:40:59 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8615, Val Loss=0.4092, lr=0.0010
[08/15 11:42:00 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8586, Val Loss=0.4156, lr=0.0010
[08/15 11:43:11 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8625, Val Loss=0.4107, lr=0.0010
[08/15 11:44:17 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8629, Val Loss=0.4111, lr=0.0010
[08/15 11:45:22 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8622, Val Loss=0.4147, lr=0.0010
[08/15 11:46:29 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8611, Val Loss=0.4138, lr=0.0010
[08/15 11:47:32 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8623, Val Loss=0.4174, lr=0.0010
[08/15 11:48:35 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8640, Val Loss=0.4117, lr=0.0010
[08/15 11:49:43 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8644, Val Loss=0.4106, lr=0.0010
[08/15 11:50:51 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8627, Val Loss=0.4155, lr=0.0010
[08/15 11:52:00 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8629, Val Loss=0.4130, lr=0.0010
[08/15 11:53:07 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8623, Val Loss=0.4219, lr=0.0010
[08/15 11:54:13 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8639, Val Loss=0.4126, lr=0.0010
[08/15 11:55:15 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8643, Val Loss=0.4177, lr=0.0010
[08/15 11:56:15 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8629, Val Loss=0.4091, lr=0.0010
[08/15 11:57:16 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8644, Val Loss=0.4146, lr=0.0010
[08/15 11:58:17 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8613, Val Loss=0.4232, lr=0.0010
[08/15 11:59:17 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8654, Val Loss=0.4092, lr=0.0001
[08/15 12:00:19 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8647, Val Loss=0.4067, lr=0.0001
[08/15 12:01:20 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8645, Val Loss=0.4064, lr=0.0001
[08/15 12:02:20 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8662, Val Loss=0.4047, lr=0.0001
[08/15 12:03:24 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8658, Val Loss=0.4074, lr=0.0001
[08/15 12:04:28 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8656, Val Loss=0.4074, lr=0.0001
[08/15 12:05:29 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8647, Val Loss=0.4064, lr=0.0001
[08/15 12:06:30 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8668, Val Loss=0.4033, lr=0.0001
[08/15 12:07:32 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8651, Val Loss=0.4107, lr=0.0001
[08/15 12:08:33 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8667, Val Loss=0.4052, lr=0.0001
[08/15 12:09:36 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8670, Val Loss=0.4048, lr=0.0001
[08/15 12:10:37 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8665, Val Loss=0.4071, lr=0.0001
[08/15 12:11:39 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8675, Val Loss=0.4067, lr=0.0001
[08/15 12:12:42 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8661, Val Loss=0.4050, lr=0.0001
[08/15 12:13:43 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8669, Val Loss=0.4062, lr=0.0001
[08/15 12:14:47 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8649, Val Loss=0.4057, lr=0.0001
[08/15 12:15:50 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8661, Val Loss=0.4083, lr=0.0001
[08/15 12:16:51 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8666, Val Loss=0.4058, lr=0.0001
[08/15 12:17:52 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8663, Val Loss=0.4062, lr=0.0001
[08/15 12:18:54 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.8662, Val Loss=0.4054, lr=0.0001
[08/15 12:18:54 cifar10-global-group_norm-resnet56]: Best Acc=0.8675
[08/15 12:18:54 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/15 12:18:55 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/15 12:19:07 cifar10-global-group_norm-resnet56]: Acc: 0.1285 Val Loss: 6.1681

[08/15 12:19:07 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/15 12:20:07 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8897, Val Loss=0.3530, lr=0.0100
[08/15 12:21:06 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8905, Val Loss=0.3525, lr=0.0100
[08/15 12:22:07 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.9041, Val Loss=0.3079, lr=0.0100
[08/15 12:23:13 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.9038, Val Loss=0.3119, lr=0.0100
[08/15 12:24:19 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.9024, Val Loss=0.3132, lr=0.0100
[08/15 12:25:25 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.9056, Val Loss=0.3201, lr=0.0100
[08/15 12:26:29 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8981, Val Loss=0.3472, lr=0.0100
[08/15 12:27:30 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.9036, Val Loss=0.3216, lr=0.0100
[08/15 12:28:39 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.9113, Val Loss=0.2829, lr=0.0100
[08/15 12:29:45 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.9113, Val Loss=0.2963, lr=0.0100
[08/15 12:30:52 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8894, Val Loss=0.3772, lr=0.0100
[08/15 12:31:53 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8887, Val Loss=0.3916, lr=0.0100
[08/15 12:32:53 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.9092, Val Loss=0.3129, lr=0.0100
[08/15 12:33:52 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.9138, Val Loss=0.2818, lr=0.0100
[08/15 12:34:51 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.9032, Val Loss=0.3276, lr=0.0100
[08/15 12:35:51 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.9044, Val Loss=0.3249, lr=0.0100
[08/15 12:36:50 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9052, Val Loss=0.3177, lr=0.0100
[08/15 12:37:49 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.9026, Val Loss=0.3511, lr=0.0100
[08/15 12:38:48 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.9120, Val Loss=0.3039, lr=0.0100
[08/15 12:39:47 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.9171, Val Loss=0.2778, lr=0.0100
[08/15 12:40:46 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.9140, Val Loss=0.2986, lr=0.0100
[08/15 12:41:44 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9018, Val Loss=0.3529, lr=0.0100
[08/15 12:42:47 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.9036, Val Loss=0.3265, lr=0.0100
[08/15 12:43:45 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.9046, Val Loss=0.3225, lr=0.0100
[08/15 12:44:43 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.9067, Val Loss=0.3208, lr=0.0100
[08/15 12:45:42 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8972, Val Loss=0.3629, lr=0.0100
[08/15 12:46:42 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.9079, Val Loss=0.3036, lr=0.0100
[08/15 12:47:39 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8982, Val Loss=0.3761, lr=0.0100
[08/15 12:48:37 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.9055, Val Loss=0.3385, lr=0.0100
[08/15 12:49:34 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.9040, Val Loss=0.3386, lr=0.0100
[08/15 12:50:32 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.9060, Val Loss=0.3337, lr=0.0100
[08/15 12:51:29 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8985, Val Loss=0.3661, lr=0.0100
[08/15 12:52:27 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.9075, Val Loss=0.3390, lr=0.0100
[08/15 12:53:25 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8975, Val Loss=0.3702, lr=0.0100
[08/15 12:54:22 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.9144, Val Loss=0.3074, lr=0.0100
[08/15 12:55:20 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.9065, Val Loss=0.3515, lr=0.0100
[08/15 12:56:17 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.9083, Val Loss=0.3201, lr=0.0100
[08/15 12:57:15 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9002, Val Loss=0.3531, lr=0.0100
[08/15 12:58:13 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.9156, Val Loss=0.2996, lr=0.0100
[08/15 12:59:10 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.9042, Val Loss=0.3327, lr=0.0100
[08/15 13:00:08 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9322, Val Loss=0.2349, lr=0.0010
[08/15 13:01:06 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9337, Val Loss=0.2354, lr=0.0010
[08/15 13:02:04 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9339, Val Loss=0.2380, lr=0.0010
[08/15 13:03:01 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9344, Val Loss=0.2354, lr=0.0010
[08/15 13:04:03 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9345, Val Loss=0.2430, lr=0.0010
[08/15 13:05:02 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9365, Val Loss=0.2366, lr=0.0010
[08/15 13:06:01 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9363, Val Loss=0.2421, lr=0.0010
[08/15 13:07:02 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9353, Val Loss=0.2406, lr=0.0010
[08/15 13:08:03 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9362, Val Loss=0.2378, lr=0.0010
[08/15 13:09:01 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9363, Val Loss=0.2430, lr=0.0010
[08/15 13:10:00 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9356, Val Loss=0.2434, lr=0.0010
[08/15 13:10:59 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9362, Val Loss=0.2432, lr=0.0010
[08/15 13:11:57 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9362, Val Loss=0.2412, lr=0.0010
[08/15 13:12:55 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9367, Val Loss=0.2447, lr=0.0010
[08/15 13:13:54 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9367, Val Loss=0.2444, lr=0.0010
[08/15 13:14:53 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9372, Val Loss=0.2423, lr=0.0010
[08/15 13:15:51 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9364, Val Loss=0.2490, lr=0.0010
[08/15 13:16:50 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9359, Val Loss=0.2484, lr=0.0010
[08/15 13:17:49 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9370, Val Loss=0.2476, lr=0.0010
[08/15 13:18:47 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9372, Val Loss=0.2500, lr=0.0010
[08/15 13:19:46 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9368, Val Loss=0.2543, lr=0.0001
[08/15 13:20:44 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9374, Val Loss=0.2493, lr=0.0001
[08/15 13:21:43 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9374, Val Loss=0.2477, lr=0.0001
[08/15 13:22:41 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9363, Val Loss=0.2469, lr=0.0001
[08/15 13:23:40 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9368, Val Loss=0.2475, lr=0.0001
[08/15 13:24:39 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9364, Val Loss=0.2467, lr=0.0001
[08/15 13:25:39 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9372, Val Loss=0.2480, lr=0.0001
[08/15 13:26:39 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9369, Val Loss=0.2472, lr=0.0001
[08/15 13:27:38 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9376, Val Loss=0.2479, lr=0.0001
[08/15 13:28:37 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9371, Val Loss=0.2492, lr=0.0001
[08/15 13:29:35 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9379, Val Loss=0.2476, lr=0.0001
[08/15 13:30:33 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9376, Val Loss=0.2476, lr=0.0001
[08/15 13:31:31 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9376, Val Loss=0.2475, lr=0.0001
[08/15 13:32:29 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9366, Val Loss=0.2482, lr=0.0001
[08/15 13:33:27 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9375, Val Loss=0.2477, lr=0.0001
[08/15 13:34:25 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9366, Val Loss=0.2489, lr=0.0001
[08/15 13:35:24 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9367, Val Loss=0.2463, lr=0.0001
[08/15 13:36:22 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9376, Val Loss=0.2475, lr=0.0001
[08/15 13:37:21 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9375, Val Loss=0.2470, lr=0.0001
[08/15 13:38:20 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9371, Val Loss=0.2479, lr=0.0001
[08/15 13:38:20 cifar10-global-group_norm-resnet56]: Best Acc=0.9379

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
