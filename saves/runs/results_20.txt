Microsoft Windows [Version 10.0.22621.1992]
(c) Microsoft Corporation. All rights reserved.

C:\Users\35679>cd Downloads

C:\Users\35679\Downloads>
C:\Users\35679\Downloads>
C:\Users\35679\Downloads>cd Torch-Pruning-1.1.4

C:\Users\35679\Downloads\Torch-Pruning-1.1.4>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4>cd Torch-Pruning-1.1.4

C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4>cd benchmarks

C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.1 --global-pruning
--total-epochs 80
Traceback (most recent call last):
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\main.py", line 13, in <module>
    import engine.utils as utils
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\__init__.py", line 1, in <module>
    from . import models, utils
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\models\__init__.py", line 1,in <module>
    from . import cifar, imagenet, graph
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\models\cifar\__init__.py", line 1, in <module>
    from . import (
  File "C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks\engine\models\cifar\vit.py", line 5, in <module>
    from einops import rearrange, repeat
ModuleNotFoundError: No module named 'einops'

C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>conda activate base

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>conda activate base

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.1 --global-pruning --total-epochs 80
Files already downloaded and verified
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: mode: prune
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: model: resnet56
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: verbose: False
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: method: group_norm
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sparsity: 0.1
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: seed: None
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: device: cuda
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/14 14:58:02 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/14 14:59:17 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 []
layer2.0.downsample.0 []
conv1 []
layer1.0.conv1 [6, 7]
layer1.1.conv1 []
layer1.2.conv1 [6]
layer1.3.conv1 [12]
layer1.4.conv1 []
layer1.5.conv1 [2, 11]
layer1.6.conv1 []
layer1.7.conv1 []
layer1.8.conv1 [0]
layer2.0.conv1 []
layer2.1.conv1 [1, 3, 4, 19, 22, 26, 31]
layer2.2.conv1 [8, 20]
layer2.3.conv1 [26, 28]
layer2.4.conv1 [23, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29]
layer2.6.conv1 [2, 4, 5, 6, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 []
layer3.1.conv1 [56]
layer3.2.conv1 [29, 45, 49]
layer3.3.conv1 [33]
layer3.4.conv1 [45, 54]
layer3.5.conv1 [10, 63]
layer3.6.conv1 [3, 14, 26, 57]
layer3.7.conv1 [21, 55]
layer3.8.conv1 []
=> Start history generation
layer3.8.conv1 []
layer3.7.conv1 []
layer3.6.conv1 []
layer3.5.conv1 []
layer3.4.conv1 []
layer3.3.conv1 []
layer3.2.conv1 []
layer3.1.conv1 []
layer3.0.conv1 []
layer2.8.conv1 []
layer2.7.conv1 []
layer2.6.conv1 []
layer2.5.conv1 []
layer2.4.conv1 []
layer2.3.conv1 []
layer2.2.conv1 []
layer2.1.conv1 []
layer2.0.conv1 []
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 []
layer1.6.conv1 []
layer1.5.conv1 []
layer1.4.conv1 []
layer1.3.conv1 []
layer1.2.conv1 []
layer1.1.conv1 []
layer1.0.conv1 []
conv1 []
layer2.0.downsample.0 []
layer3.0.downsample.0 []
[08/14 14:59:23 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(32, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(32, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(32, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(32, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[08/14 14:59:23 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/14 14:59:34 cifar10-global-group_norm-resnet56]: Acc: 0.9344 Val Loss: 0.2649

[08/14 14:59:34 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 []
layer2.0.downsample.0 []
conv1 []
layer1.0.conv1 [0, 3, 5, 13]
layer1.1.conv1 [0, 3, 5, 7, 12]
layer1.2.conv1 [0, 3, 7, 8, 10, 13]
layer1.3.conv1 [3, 11]
layer1.4.conv1 [7]
layer1.5.conv1 [0, 5, 11]
layer1.6.conv1 [4, 7, 14]
layer1.7.conv1 [3, 5, 7, 9]
layer1.8.conv1 [4, 6, 8, 10, 14]
layer2.0.conv1 []
layer2.1.conv1 [1, 8, 13, 20, 23, 24]
layer2.2.conv1 [11, 19]
layer2.3.conv1 [11, 17]
layer2.4.conv1 [1, 9, 15, 22]
layer2.5.conv1 [5, 11]
layer2.6.conv1 [3, 5, 13]
layer2.7.conv1 [8, 10]
layer2.8.conv1 [1, 2]
layer3.0.conv1 []
layer3.1.conv1 [10, 37]
layer3.2.conv1 [3, 11, 35]
layer3.3.conv1 [2, 6, 24, 38, 43, 45, 46, 48, 54]
layer3.4.conv1 [0, 30, 51]
layer3.5.conv1 [3, 5, 6, 8, 9, 27, 37, 44, 48, 51]
layer3.6.conv1 [11, 20, 28, 42, 44, 46, 53, 56]
layer3.7.conv1 [0, 2, 22, 26, 30, 38, 45, 55]
layer3.8.conv1 [24, 45]
=> Start history generation
layer3.8.conv1 []
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 []
layer3.5.conv1 []
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 []
layer3.2.conv1 []
layer3.1.conv1 []
layer3.0.conv1 []
layer2.8.conv1 []
layer2.7.conv1 []
layer2.6.conv1 []
layer2.5.conv1 []
layer2.4.conv1 []
layer2.3.conv1 []
layer2.2.conv1 []
layer2.1.conv1 []
layer2.0.conv1 []
layer1.8.conv1 []
layer1.7.conv1 []
layer1.6.conv1 []
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 []
layer1.3.conv1 []
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 []
layer3.0.downsample.0 []
[08/14 14:59:39 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(32, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(32, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(32, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(32, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(32, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(32, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.71 M (82.92%)
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 94.01 M (73.96%, 1.35X )
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.9187
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 0.3601
[08/14 14:59:50 cifar10-global-group_norm-resnet56]: Finetuning...
[08/14 15:00:49 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8951, Val Loss=0.3752, lr=0.0100
[08/14 15:01:43 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.9133, Val Loss=0.3079, lr=0.0100
[08/14 15:02:38 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.9005, Val Loss=0.3409, lr=0.0100
[08/14 15:03:33 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.9040, Val Loss=0.3321, lr=0.0100
[08/14 15:04:27 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.9084, Val Loss=0.3103, lr=0.0100
[08/14 15:05:22 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.9049, Val Loss=0.3148, lr=0.0100
[08/14 15:06:16 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.9086, Val Loss=0.3104, lr=0.0100
[08/14 15:07:11 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.9108, Val Loss=0.3180, lr=0.0100
[08/14 15:08:05 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.9026, Val Loss=0.3437, lr=0.0100
[08/14 15:08:59 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.9042, Val Loss=0.3384, lr=0.0100
[08/14 15:09:53 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.9044, Val Loss=0.3325, lr=0.0100
[08/14 15:10:47 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.9015, Val Loss=0.3526, lr=0.0100
[08/14 15:11:40 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.9076, Val Loss=0.3278, lr=0.0100
[08/14 15:12:34 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.9024, Val Loss=0.3376, lr=0.0100
[08/14 15:13:27 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8955, Val Loss=0.3551, lr=0.0100
[08/14 15:14:21 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.9029, Val Loss=0.3595, lr=0.0100
[08/14 15:15:15 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9040, Val Loss=0.3256, lr=0.0100
[08/14 15:16:08 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.9022, Val Loss=0.3294, lr=0.0100
[08/14 15:17:02 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.9027, Val Loss=0.3345, lr=0.0100
[08/14 15:17:56 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.9011, Val Loss=0.3590, lr=0.0100
[08/14 15:18:49 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.9077, Val Loss=0.3273, lr=0.0100
[08/14 15:19:43 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9020, Val Loss=0.3422, lr=0.0100
[08/14 15:20:36 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.9160, Val Loss=0.2878, lr=0.0100
[08/14 15:21:30 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.9086, Val Loss=0.3200, lr=0.0100
[08/14 15:22:24 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.9069, Val Loss=0.3133, lr=0.0100
[08/14 15:23:18 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.9073, Val Loss=0.3145, lr=0.0100
[08/14 15:24:12 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.9061, Val Loss=0.3225, lr=0.0100
[08/14 15:25:05 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.9149, Val Loss=0.3027, lr=0.0100
[08/14 15:25:59 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8993, Val Loss=0.3544, lr=0.0100
[08/14 15:26:53 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.9136, Val Loss=0.2902, lr=0.0100
[08/14 15:27:46 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.9011, Val Loss=0.3420, lr=0.0100
[08/14 15:28:40 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9042, Val Loss=0.3444, lr=0.0100
[08/14 15:29:34 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.9071, Val Loss=0.3262, lr=0.0100
[08/14 15:30:28 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9023, Val Loss=0.3517, lr=0.0100
[08/14 15:31:21 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8943, Val Loss=0.3743, lr=0.0100
[08/14 15:32:15 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.9007, Val Loss=0.3477, lr=0.0100
[08/14 15:33:11 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.9059, Val Loss=0.3397, lr=0.0100
[08/14 15:34:05 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9050, Val Loss=0.3157, lr=0.0100
[08/14 15:35:00 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.9107, Val Loss=0.3154, lr=0.0100
[08/14 15:35:54 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.9114, Val Loss=0.2929, lr=0.0100
[08/14 15:36:49 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9328, Val Loss=0.2305, lr=0.0010
[08/14 15:37:43 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9355, Val Loss=0.2267, lr=0.0010
[08/14 15:38:38 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9341, Val Loss=0.2275, lr=0.0010
[08/14 15:39:32 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9354, Val Loss=0.2289, lr=0.0010
[08/14 15:40:27 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9363, Val Loss=0.2298, lr=0.0010
[08/14 15:41:22 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9344, Val Loss=0.2302, lr=0.0010
[08/14 15:42:16 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9349, Val Loss=0.2363, lr=0.0010
[08/14 15:43:10 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9365, Val Loss=0.2336, lr=0.0010
[08/14 15:44:05 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9358, Val Loss=0.2377, lr=0.0010
[08/14 15:44:59 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9352, Val Loss=0.2380, lr=0.0010
[08/14 15:45:54 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9363, Val Loss=0.2357, lr=0.0010
[08/14 15:46:49 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9361, Val Loss=0.2401, lr=0.0010
[08/14 15:47:43 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9370, Val Loss=0.2397, lr=0.0010
[08/14 15:48:39 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9350, Val Loss=0.2459, lr=0.0010
[08/14 15:49:35 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9363, Val Loss=0.2414, lr=0.0010
[08/14 15:50:31 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9364, Val Loss=0.2410, lr=0.0010
[08/14 15:51:25 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9348, Val Loss=0.2446, lr=0.0010
[08/14 15:52:19 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9371, Val Loss=0.2404, lr=0.0010
[08/14 15:53:14 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9373, Val Loss=0.2385, lr=0.0010
[08/14 15:54:09 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9361, Val Loss=0.2435, lr=0.0010
[08/14 15:55:03 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9360, Val Loss=0.2440, lr=0.0001
[08/14 15:55:57 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9363, Val Loss=0.2439, lr=0.0001
[08/14 15:56:51 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9372, Val Loss=0.2417, lr=0.0001
[08/14 15:57:46 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9374, Val Loss=0.2422, lr=0.0001
[08/14 15:58:41 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9376, Val Loss=0.2432, lr=0.0001
[08/14 15:59:36 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9367, Val Loss=0.2416, lr=0.0001
[08/14 16:00:30 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9369, Val Loss=0.2424, lr=0.0001
[08/14 16:01:25 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9376, Val Loss=0.2419, lr=0.0001
[08/14 16:02:20 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9367, Val Loss=0.2413, lr=0.0001
[08/14 16:03:15 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9374, Val Loss=0.2422, lr=0.0001
[08/14 16:04:09 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9370, Val Loss=0.2427, lr=0.0001
[08/14 16:05:04 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9372, Val Loss=0.2426, lr=0.0001
[08/14 16:05:58 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9373, Val Loss=0.2443, lr=0.0001
[08/14 16:06:53 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9385, Val Loss=0.2420, lr=0.0001
[08/14 16:07:47 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9374, Val Loss=0.2424, lr=0.0001
[08/14 16:08:42 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9385, Val Loss=0.2423, lr=0.0001
[08/14 16:09:36 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9384, Val Loss=0.2406, lr=0.0001
[08/14 16:10:31 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9378, Val Loss=0.2429, lr=0.0001
[08/14 16:11:25 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9375, Val Loss=0.2421, lr=0.0001
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9367, Val Loss=0.2440, lr=0.0001
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: Best Acc=0.9385
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: Params: 0.71 M
[08/14 16:12:20 cifar10-global-group_norm-resnet56]: ops: 94.01 M
[08/14 16:12:31 cifar10-global-group_norm-resnet56]: Acc: 0.9367 Val Loss: 0.2440

[08/14 16:12:31 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/14 16:12:31 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/14 16:12:42 cifar10-global-group_norm-resnet56]: Acc: 0.8151 Val Loss: 0.6809

[08/14 16:12:42 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/14 16:13:53 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8177, Val Loss=0.5483, lr=0.0100
[08/14 16:15:04 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8283, Val Loss=0.5237, lr=0.0100
[08/14 16:16:14 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8628, Val Loss=0.4217, lr=0.0100
[08/14 16:17:25 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8820, Val Loss=0.3590, lr=0.0100
[08/14 16:18:36 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8760, Val Loss=0.3729, lr=0.0100
[08/14 16:19:48 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8791, Val Loss=0.3669, lr=0.0100
[08/14 16:20:58 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8762, Val Loss=0.3729, lr=0.0100
[08/14 16:22:09 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8848, Val Loss=0.3540, lr=0.0100
[08/14 16:23:20 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8881, Val Loss=0.3484, lr=0.0100
[08/14 16:24:31 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8930, Val Loss=0.3348, lr=0.0100
[08/14 16:25:42 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8842, Val Loss=0.3575, lr=0.0100
[08/14 16:26:53 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8888, Val Loss=0.3504, lr=0.0100
[08/14 16:28:03 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8905, Val Loss=0.3390, lr=0.0100
[08/14 16:29:13 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8958, Val Loss=0.3332, lr=0.0100
[08/14 16:30:25 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8731, Val Loss=0.4183, lr=0.0100
[08/14 16:31:36 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8915, Val Loss=0.3445, lr=0.0100
[08/14 16:32:48 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8995, Val Loss=0.3275, lr=0.0100
[08/14 16:33:58 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8978, Val Loss=0.3222, lr=0.0100
[08/14 16:35:09 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8818, Val Loss=0.3940, lr=0.0100
[08/14 16:36:22 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8937, Val Loss=0.3487, lr=0.0100
[08/14 16:37:44 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8953, Val Loss=0.3417, lr=0.0100
[08/14 16:38:57 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9021, Val Loss=0.3145, lr=0.0100
[08/14 16:40:10 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8950, Val Loss=0.3371, lr=0.0100
[08/14 16:41:27 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8971, Val Loss=0.3326, lr=0.0100
[08/14 16:42:41 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8990, Val Loss=0.3278, lr=0.0100
[08/14 16:43:54 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8896, Val Loss=0.3584, lr=0.0100
[08/14 16:45:20 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8898, Val Loss=0.3566, lr=0.0100
[08/14 16:46:33 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8973, Val Loss=0.3304, lr=0.0100
[08/14 16:47:47 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8936, Val Loss=0.3530, lr=0.0100
[08/14 16:49:09 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8988, Val Loss=0.3235, lr=0.0100
[08/14 16:50:24 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8929, Val Loss=0.3319, lr=0.0100
[08/14 16:51:41 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9042, Val Loss=0.3053, lr=0.0100
[08/14 16:53:03 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8907, Val Loss=0.3491, lr=0.0100
[08/14 16:54:17 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8979, Val Loss=0.3196, lr=0.0100
[08/14 16:55:33 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8912, Val Loss=0.3542, lr=0.0100
[08/14 16:56:47 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8991, Val Loss=0.3196, lr=0.0100
[08/14 16:58:04 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8999, Val Loss=0.3239, lr=0.0100
[08/14 16:59:19 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9001, Val Loss=0.3397, lr=0.0100
[08/14 17:00:40 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8934, Val Loss=0.3687, lr=0.0100
[08/14 17:02:01 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8978, Val Loss=0.3286, lr=0.0100
[08/14 17:03:16 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9152, Val Loss=0.2765, lr=0.0010
[08/14 17:04:34 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9142, Val Loss=0.2755, lr=0.0010
[08/14 17:05:48 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9159, Val Loss=0.2736, lr=0.0010
[08/14 17:07:01 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9169, Val Loss=0.2721, lr=0.0010
[08/14 17:08:46 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9179, Val Loss=0.2731, lr=0.0010
[08/14 17:10:29 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9173, Val Loss=0.2738, lr=0.0010
[08/14 17:12:02 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9166, Val Loss=0.2711, lr=0.0010
[08/14 17:13:30 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9184, Val Loss=0.2706, lr=0.0010
[08/14 17:14:43 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9187, Val Loss=0.2776, lr=0.0010
[08/14 17:15:54 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9178, Val Loss=0.2746, lr=0.0010
[08/14 17:17:06 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9183, Val Loss=0.2803, lr=0.0010
[08/14 17:18:17 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9185, Val Loss=0.2787, lr=0.0010
[08/14 17:19:28 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9194, Val Loss=0.2801, lr=0.0010
[08/14 17:20:39 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9179, Val Loss=0.2783, lr=0.0010
[08/14 17:21:50 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9176, Val Loss=0.2810, lr=0.0010
[08/14 17:23:01 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9174, Val Loss=0.2867, lr=0.0010
[08/14 17:24:10 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9172, Val Loss=0.2854, lr=0.0010
[08/14 17:25:22 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9182, Val Loss=0.2848, lr=0.0010
[08/14 17:26:34 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9162, Val Loss=0.2868, lr=0.0010
[08/14 17:27:45 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9162, Val Loss=0.2858, lr=0.0010
[08/14 17:28:55 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9185, Val Loss=0.2847, lr=0.0001
[08/14 17:30:05 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9177, Val Loss=0.2861, lr=0.0001
[08/14 17:31:15 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9176, Val Loss=0.2857, lr=0.0001
[08/14 17:32:25 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9189, Val Loss=0.2873, lr=0.0001
[08/14 17:33:35 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9180, Val Loss=0.2845, lr=0.0001
[08/14 17:34:44 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9182, Val Loss=0.2859, lr=0.0001
[08/14 17:35:54 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9188, Val Loss=0.2841, lr=0.0001
[08/14 17:37:04 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9183, Val Loss=0.2847, lr=0.0001
[08/14 17:38:13 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9180, Val Loss=0.2853, lr=0.0001
[08/14 17:39:23 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9172, Val Loss=0.2833, lr=0.0001
[08/14 17:40:32 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9196, Val Loss=0.2875, lr=0.0001
[08/14 17:41:42 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9175, Val Loss=0.2858, lr=0.0001
[08/14 17:42:51 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9178, Val Loss=0.2872, lr=0.0001
[08/14 17:44:01 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9195, Val Loss=0.2845, lr=0.0001
[08/14 17:45:10 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9190, Val Loss=0.2863, lr=0.0001
[08/14 17:46:19 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9174, Val Loss=0.2856, lr=0.0001
[08/14 17:47:29 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9180, Val Loss=0.2860, lr=0.0001
[08/14 17:48:38 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9189, Val Loss=0.2854, lr=0.0001
[08/14 17:49:48 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9187, Val Loss=0.2835, lr=0.0001
[08/14 17:51:00 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9182, Val Loss=0.2869, lr=0.0001
[08/14 17:51:00 cifar10-global-group_norm-resnet56]: Best Acc=0.9196
[08/14 17:51:00 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/14 17:51:01 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/14 17:51:12 cifar10-global-group_norm-resnet56]: Acc: 0.1003 Val Loss: 2.4542

[08/14 17:51:12 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/14 17:52:10 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8430, Val Loss=0.4533, lr=0.0100
[08/14 17:53:08 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8551, Val Loss=0.4500, lr=0.0100
[08/14 17:54:05 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8817, Val Loss=0.3566, lr=0.0100
[08/14 17:55:04 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8952, Val Loss=0.3155, lr=0.0100
[08/14 17:56:04 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8860, Val Loss=0.3389, lr=0.0100
[08/14 17:57:02 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8802, Val Loss=0.3744, lr=0.0100
[08/14 17:57:59 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8861, Val Loss=0.3472, lr=0.0100
[08/14 17:58:56 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8821, Val Loss=0.3822, lr=0.0100
[08/14 17:59:53 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8988, Val Loss=0.3127, lr=0.0100
[08/14 18:00:51 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8886, Val Loss=0.3708, lr=0.0100
[08/14 18:01:48 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8959, Val Loss=0.3320, lr=0.0100
[08/14 18:02:46 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8972, Val Loss=0.3341, lr=0.0100
[08/14 18:03:43 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.9017, Val Loss=0.3075, lr=0.0100
[08/14 18:04:40 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8809, Val Loss=0.4240, lr=0.0100
[08/14 18:05:38 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8970, Val Loss=0.3366, lr=0.0100
[08/14 18:06:35 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.9028, Val Loss=0.3161, lr=0.0100
[08/14 18:07:32 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9009, Val Loss=0.3127, lr=0.0100
[08/14 18:08:30 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8907, Val Loss=0.3477, lr=0.0100
[08/14 18:09:27 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8994, Val Loss=0.3333, lr=0.0100
[08/14 18:10:27 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8862, Val Loss=0.3933, lr=0.0100
[08/14 18:11:24 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8974, Val Loss=0.3453, lr=0.0100
[08/14 18:12:21 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.9020, Val Loss=0.3262, lr=0.0100
[08/14 18:13:17 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8953, Val Loss=0.3416, lr=0.0100
[08/14 18:14:14 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8934, Val Loss=0.3526, lr=0.0100
[08/14 18:15:11 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.9010, Val Loss=0.3198, lr=0.0100
[08/14 18:16:07 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.9062, Val Loss=0.3193, lr=0.0100
[08/14 18:17:04 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.9048, Val Loss=0.2961, lr=0.0100
[08/14 18:18:01 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.9061, Val Loss=0.3016, lr=0.0100
[08/14 18:18:57 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8904, Val Loss=0.3789, lr=0.0100
[08/14 18:19:54 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8929, Val Loss=0.3654, lr=0.0100
[08/14 18:20:50 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8945, Val Loss=0.3502, lr=0.0100
[08/14 18:21:47 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9119, Val Loss=0.3028, lr=0.0100
[08/14 18:22:44 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.9062, Val Loss=0.3215, lr=0.0100
[08/14 18:23:40 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.8998, Val Loss=0.3276, lr=0.0100
[08/14 18:24:36 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.9112, Val Loss=0.2985, lr=0.0100
[08/14 18:25:33 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.9030, Val Loss=0.3248, lr=0.0100
[08/14 18:26:29 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.9037, Val Loss=0.3397, lr=0.0100
[08/14 18:27:27 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.9039, Val Loss=0.3453, lr=0.0100
[08/14 18:28:23 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8889, Val Loss=0.3942, lr=0.0100
[08/14 18:29:20 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8940, Val Loss=0.3600, lr=0.0100
[08/14 18:30:16 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9284, Val Loss=0.2403, lr=0.0010
[08/14 18:31:14 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9282, Val Loss=0.2433, lr=0.0010
[08/14 18:32:11 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9313, Val Loss=0.2409, lr=0.0010
[08/14 18:33:08 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9297, Val Loss=0.2439, lr=0.0010
[08/14 18:34:05 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9303, Val Loss=0.2428, lr=0.0010
[08/14 18:35:03 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9317, Val Loss=0.2393, lr=0.0010
[08/14 18:36:01 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9312, Val Loss=0.2429, lr=0.0010
[08/14 18:36:58 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9322, Val Loss=0.2431, lr=0.0010
[08/14 18:37:54 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9312, Val Loss=0.2445, lr=0.0010
[08/14 18:38:51 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9328, Val Loss=0.2456, lr=0.0010
[08/14 18:39:48 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9302, Val Loss=0.2474, lr=0.0010
[08/14 18:40:44 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9334, Val Loss=0.2454, lr=0.0010
[08/14 18:41:41 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9327, Val Loss=0.2453, lr=0.0010
[08/14 18:42:37 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9311, Val Loss=0.2495, lr=0.0010
[08/14 18:43:34 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9313, Val Loss=0.2517, lr=0.0010
[08/14 18:44:30 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9307, Val Loss=0.2548, lr=0.0010
[08/14 18:45:27 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9325, Val Loss=0.2541, lr=0.0010
[08/14 18:46:23 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9313, Val Loss=0.2539, lr=0.0010
[08/14 18:47:19 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9329, Val Loss=0.2485, lr=0.0010
[08/14 18:48:16 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9328, Val Loss=0.2541, lr=0.0010
[08/14 18:49:13 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9336, Val Loss=0.2506, lr=0.0001
[08/14 18:50:09 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9332, Val Loss=0.2508, lr=0.0001
[08/14 18:51:07 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9337, Val Loss=0.2514, lr=0.0001
[08/14 18:52:05 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9336, Val Loss=0.2518, lr=0.0001
[08/14 18:53:04 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9335, Val Loss=0.2502, lr=0.0001
[08/14 18:54:00 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9329, Val Loss=0.2505, lr=0.0001
[08/14 18:54:57 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9329, Val Loss=0.2508, lr=0.0001
[08/14 18:55:53 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9329, Val Loss=0.2492, lr=0.0001
[08/14 18:56:49 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9341, Val Loss=0.2514, lr=0.0001
[08/14 18:57:46 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9343, Val Loss=0.2517, lr=0.0001
[08/14 18:58:42 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9342, Val Loss=0.2494, lr=0.0001
[08/14 18:59:38 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9334, Val Loss=0.2491, lr=0.0001
[08/14 19:00:35 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9335, Val Loss=0.2526, lr=0.0001
[08/14 19:01:31 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9335, Val Loss=0.2526, lr=0.0001
[08/14 19:02:29 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9331, Val Loss=0.2505, lr=0.0001
[08/14 19:03:27 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9328, Val Loss=0.2529, lr=0.0001
[08/14 19:04:25 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9330, Val Loss=0.2517, lr=0.0001
[08/14 19:05:22 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9342, Val Loss=0.2505, lr=0.0001
[08/14 19:06:19 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9343, Val Loss=0.2498, lr=0.0001
[08/14 19:07:16 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9329, Val Loss=0.2513, lr=0.0001
[08/14 19:07:16 cifar10-global-group_norm-resnet56]: Best Acc=0.9343

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>python main.py --mode prune --model resnet56 --batch-size 128 --restore cifar10_resnet56.pth --dataset cifar10  --method group_norm --sparsity 0.2 --global-pruning --total-epochs 80
Files already downloaded and verified
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: mode: prune
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: model: resnet56
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: verbose: False
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: dataset: cifar10
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: batch_size: 128
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: total_epochs: 80
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: lr_decay_milestones: 40,60
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: lr_decay_gamma: 0.1
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: lr: 0.01
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: restore: cifar10_resnet56.pth
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: output_dir: run\cifar10\prune\cifar10-global-group_norm-resnet56
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: method: group_norm
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: speed_up: 2.55
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: max_sparsity: 1.0
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sparsity: 0.2
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: soft_keeping_ratio: 0.0
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: reg: 0.0005
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: weight_decay: 0.0005
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: seed: None
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: global_pruning: True
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_total_epochs: 10
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_lr: 0.01
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_lr_decay_milestones: 60,80
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_reg_warmup: 0
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: sl_restore: None
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: iterative_steps: 1
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: logger: <Logger cifar10-global-group_norm-resnet56 (DEBUG)>
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: device: cuda
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: num_classes: 10
[08/14 19:10:43 cifar10-global-group_norm-resnet56]: Loading model from cifar10_resnet56.pth
[08/14 19:11:06 cifar10-global-group_norm-resnet56]: Pruning...
layer3.0.downsample.0 [28, 30]
layer2.0.downsample.0 [1, 12]
conv1 [6, 7, 10, 13]
layer1.0.conv1 [0, 3, 5, 6, 7, 15]
layer1.1.conv1 [0, 3, 5, 7, 12]
layer1.2.conv1 [0, 3, 6, 8, 9, 11, 14]
layer1.3.conv1 [3, 11, 12]
layer1.4.conv1 [7, 11]
layer1.5.conv1 [0, 2, 11, 13]
layer1.6.conv1 [4, 7, 13, 14]
layer1.7.conv1 [3, 5, 7, 9]
layer1.8.conv1 [0, 5, 7, 9, 11, 15]
layer2.0.conv1 [9, 11]
layer2.1.conv1 [1, 3, 4, 11, 16, 19, 22, 25, 26, 29, 30, 31]
layer2.2.conv1 [8, 12, 20, 21]
layer2.3.conv1 [11, 17, 26, 28]
layer2.4.conv1 [9, 15, 22, 23, 26]
layer2.5.conv1 [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29, 31]
layer2.6.conv1 [2, 4, 5, 6, 9, 11, 14, 18, 19, 20, 24, 25, 28, 30, 31]
layer2.7.conv1 [1, 2, 3, 4, 5, 9, 10, 14, 15, 18, 19, 20, 22, 23, 24, 25, 26, 27, 30]
layer2.8.conv1 [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 26, 27, 29, 30, 31]
layer3.0.conv1 [50]
layer3.1.conv1 [10, 37, 55, 56, 63]
layer3.2.conv1 [3, 11, 29, 36, 45, 49]
layer3.3.conv1 [2, 6, 24, 31, 33, 39, 44, 46, 47, 49, 55]
layer3.4.conv1 [0, 30, 45, 52, 54]
layer3.5.conv1 [0, 3, 5, 6, 8, 9, 10, 28, 38, 40, 45, 49, 52, 63]
layer3.6.conv1 [3, 12, 14, 26, 31, 45, 47, 49, 56, 57, 60]
layer3.7.conv1 [0, 2, 19, 21, 23, 27, 31, 39, 46, 51, 55, 57, 63]
layer3.8.conv1 [24, 41, 45]
=> Start history generation
layer3.8.conv1 [28, 30]
layer3.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.6.conv1 [28, 30]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]
layer3.3.conv1 [28, 30]
layer3.2.conv1 [28, 30]
layer3.1.conv1 [28, 30]
layer3.0.conv1 [1, 12]
layer2.8.conv1 [1, 12]
layer2.7.conv1 [1, 12]
layer2.6.conv1 [1, 12]
layer2.5.conv1 [1, 12]
layer2.4.conv1 [1, 12]
layer2.3.conv1 [1, 12]
layer2.2.conv1 [1, 12]
layer2.1.conv1 [1, 12]
layer2.0.conv1 [6, 7, 10, 13]
layer1.8.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.7.conv1 [6, 7, 10, 13]
layer1.6.conv1 [6, 7, 10, 13]
layer1.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.4.conv1 [6, 7, 10, 13]
layer1.3.conv1 [6, 7, 10, 13]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.1.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
layer1.0.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
conv1 []
layer2.0.downsample.0 [6, 7, 10, 13]
layer3.0.downsample.0 [1, 12]
[08/14 19:11:09 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 30, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 30, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(30, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(30, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(30, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(30, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(30, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(30, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(30, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(30, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(30, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(30, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=10, bias=True)
)
[08/14 19:11:09 cifar10-global-group_norm-resnet56]: Validating partial prune...
[08/14 19:11:20 cifar10-global-group_norm-resnet56]: Acc: 0.2797 Val Loss: 6.9964

[08/14 19:11:20 cifar10-global-group_norm-resnet56]: Pruning 02...
layer3.0.downsample.0 [3, 16]
layer2.0.downsample.0 [0, 23]
conv1 [6, 7, 11]
layer1.1.conv1 [8, 9]
layer1.2.conv1 [0, 1, 5, 7]
layer1.3.conv1 [3, 4, 5, 9]
layer1.4.conv1 [1, 2, 6, 7, 13]
layer1.5.conv1 [4, 11]
layer1.6.conv1 [0, 1, 2, 4]
layer1.7.conv1 [1, 8]
layer2.0.conv1 [1, 28, 29]
layer2.1.conv1 [1, 4, 5, 9, 10, 15, 16, 17]
layer2.2.conv1 [8, 11, 14, 19, 22, 23, 26, 27]
layer2.3.conv1 [11, 16, 18, 24, 25]
layer2.4.conv1 [0, 1, 5, 8, 14, 16]
layer2.5.conv1 [0, 1, 2, 3, 5, 6]
layer2.6.conv1 [1, 3, 5, 12, 13]
layer2.7.conv1 [0, 2, 3, 5, 8, 10]
layer2.8.conv1 [1, 2, 5]
layer3.0.conv1 [4, 22, 29, 47]
layer3.1.conv1 [2, 20, 22, 24, 26, 28, 32, 44, 56]
layer3.2.conv1 [2, 5, 18, 25, 27, 30, 34, 38, 40, 43, 51]
layer3.3.conv1 [3, 5, 6, 7, 8, 14, 19, 27, 34, 47, 49, 50]
layer3.4.conv1 [2, 13, 17, 19, 25, 26, 29, 30, 41, 46, 47, 54]
layer3.5.conv1 [0, 5, 7, 11, 12, 13, 19, 35, 39, 46, 47]
layer3.6.conv1 [8, 10, 12, 13, 18, 19, 21, 32, 34, 38, 40, 41, 44, 48, 50, 52]
layer3.7.conv1 [2, 6, 7, 16, 19, 26, 27, 29, 30, 34, 40, 41, 42, 44, 47]
layer3.8.conv1 [17, 30, 40, 43, 48, 57]
=> Start history generation
layer3.8.conv1 [3, 16]
layer3.7.conv1 [3, 16]
layer3.6.conv1 [3, 16]
layer3.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
layer3.4.conv1 [3, 16]
layer3.3.conv1 [3, 16]
layer3.2.conv1 [3, 16]
layer3.1.conv1 [3, 16]
layer3.0.conv1 [0, 23]
layer2.8.conv1 [0, 23]
layer2.7.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
layer2.6.conv1 [0, 23]
layer2.5.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
layer2.4.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]
layer2.3.conv1 [0, 23]
layer2.2.conv1 [0, 23]
layer2.1.conv1 [0, 23]
layer2.0.conv1 [6, 7, 11]
layer1.7.conv1 [6, 7, 11]
layer1.6.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
layer1.5.conv1 [6, 7, 11]
layer1.4.conv1 [6, 7, 11]
layer1.3.conv1 [6, 7, 11]
layer1.2.conv1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
layer1.1.conv1 [6, 7, 11]
conv1 []
layer2.0.downsample.0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
layer3.0.downsample.0 [0, 23]
[08/14 19:11:23 cifar10-global-group_norm-resnet56]: ResNet(
  (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(9, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(9, 27, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(9, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(28, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(28, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(28, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(28, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(28, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(28, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(28, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(28, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(28, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(28, 60, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(60, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(60, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(60, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(60, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(60, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(39, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(60, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(60, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(60, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=60, out_features=10, bias=True)
)
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Params: 0.86 M => 0.51 M (59.33%)
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: FLOPs: 127.12 M => 57.71 M (45.40%, 2.20X )
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Acc: 0.9353 => 0.1211
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Val Loss: 0.2647 => 7.6279
[08/14 19:11:33 cifar10-global-group_norm-resnet56]: Finetuning...
[08/14 19:12:25 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.8670, Val Loss=0.4191, lr=0.0100
[08/14 19:13:15 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.8839, Val Loss=0.3759, lr=0.0100
[08/14 19:14:04 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.8919, Val Loss=0.3411, lr=0.0100
[08/14 19:14:54 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8950, Val Loss=0.3413, lr=0.0100
[08/14 19:15:45 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8855, Val Loss=0.3607, lr=0.0100
[08/14 19:16:35 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8891, Val Loss=0.3521, lr=0.0100
[08/14 19:17:26 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8945, Val Loss=0.3373, lr=0.0100
[08/14 19:18:16 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8814, Val Loss=0.3867, lr=0.0100
[08/14 19:19:06 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8848, Val Loss=0.3879, lr=0.0100
[08/14 19:19:57 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8843, Val Loss=0.3834, lr=0.0100
[08/14 19:20:48 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8923, Val Loss=0.3539, lr=0.0100
[08/14 19:21:38 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8908, Val Loss=0.3736, lr=0.0100
[08/14 19:22:29 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8771, Val Loss=0.4110, lr=0.0100
[08/14 19:23:21 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8950, Val Loss=0.3530, lr=0.0100
[08/14 19:24:12 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8937, Val Loss=0.3519, lr=0.0100
[08/14 19:25:02 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8961, Val Loss=0.3495, lr=0.0100
[08/14 19:25:52 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.9015, Val Loss=0.3289, lr=0.0100
[08/14 19:26:42 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8936, Val Loss=0.3418, lr=0.0100
[08/14 19:27:32 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8910, Val Loss=0.3715, lr=0.0100
[08/14 19:28:21 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.9078, Val Loss=0.2991, lr=0.0100
[08/14 19:29:11 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8970, Val Loss=0.3525, lr=0.0100
[08/14 19:30:00 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8838, Val Loss=0.3857, lr=0.0100
[08/14 19:30:50 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.9028, Val Loss=0.3263, lr=0.0100
[08/14 19:31:39 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8984, Val Loss=0.3473, lr=0.0100
[08/14 19:32:29 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8980, Val Loss=0.3424, lr=0.0100
[08/14 19:33:18 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.9045, Val Loss=0.3180, lr=0.0100
[08/14 19:34:08 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8952, Val Loss=0.3494, lr=0.0100
[08/14 19:34:57 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8992, Val Loss=0.3326, lr=0.0100
[08/14 19:35:46 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8907, Val Loss=0.3606, lr=0.0100
[08/14 19:36:36 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.9016, Val Loss=0.3351, lr=0.0100
[08/14 19:37:25 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8912, Val Loss=0.3629, lr=0.0100
[08/14 19:38:15 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.9009, Val Loss=0.3238, lr=0.0100
[08/14 19:39:05 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8919, Val Loss=0.3688, lr=0.0100
[08/14 19:39:54 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9042, Val Loss=0.3270, lr=0.0100
[08/14 19:40:44 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.9006, Val Loss=0.3204, lr=0.0100
[08/14 19:41:33 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8937, Val Loss=0.3416, lr=0.0100
[08/14 19:42:22 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8954, Val Loss=0.3558, lr=0.0100
[08/14 19:43:12 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8989, Val Loss=0.3432, lr=0.0100
[08/14 19:44:01 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8945, Val Loss=0.3559, lr=0.0100
[08/14 19:44:50 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.9025, Val Loss=0.3336, lr=0.0100
[08/14 19:45:42 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9240, Val Loss=0.2551, lr=0.0010
[08/14 19:46:35 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9241, Val Loss=0.2582, lr=0.0010
[08/14 19:47:27 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9250, Val Loss=0.2586, lr=0.0010
[08/14 19:48:19 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9252, Val Loss=0.2580, lr=0.0010
[08/14 19:49:10 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9263, Val Loss=0.2560, lr=0.0010
[08/14 19:50:01 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9280, Val Loss=0.2544, lr=0.0010
[08/14 19:50:51 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9286, Val Loss=0.2596, lr=0.0010
[08/14 19:51:41 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9291, Val Loss=0.2599, lr=0.0010
[08/14 19:52:32 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9268, Val Loss=0.2632, lr=0.0010
[08/14 19:53:22 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9282, Val Loss=0.2678, lr=0.0010
[08/14 19:54:12 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9276, Val Loss=0.2643, lr=0.0010
[08/14 19:55:03 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9279, Val Loss=0.2640, lr=0.0010
[08/14 19:55:53 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9274, Val Loss=0.2680, lr=0.0010
[08/14 19:56:43 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9255, Val Loss=0.2689, lr=0.0010
[08/14 19:57:34 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9277, Val Loss=0.2717, lr=0.0010
[08/14 19:58:25 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9269, Val Loss=0.2720, lr=0.0010
[08/14 19:59:15 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9294, Val Loss=0.2701, lr=0.0010
[08/14 20:00:06 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9278, Val Loss=0.2720, lr=0.0010
[08/14 20:00:56 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9291, Val Loss=0.2726, lr=0.0010
[08/14 20:01:46 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9300, Val Loss=0.2717, lr=0.0010
[08/14 20:02:36 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9287, Val Loss=0.2725, lr=0.0001
[08/14 20:03:27 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9302, Val Loss=0.2696, lr=0.0001
[08/14 20:04:18 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9296, Val Loss=0.2724, lr=0.0001
[08/14 20:05:10 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9310, Val Loss=0.2705, lr=0.0001
[08/14 20:06:03 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9314, Val Loss=0.2707, lr=0.0001
[08/14 20:06:54 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9307, Val Loss=0.2703, lr=0.0001
[08/14 20:07:44 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9312, Val Loss=0.2693, lr=0.0001
[08/14 20:08:34 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9307, Val Loss=0.2708, lr=0.0001
[08/14 20:09:25 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9319, Val Loss=0.2712, lr=0.0001
[08/14 20:10:15 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9314, Val Loss=0.2719, lr=0.0001
[08/14 20:11:06 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9309, Val Loss=0.2706, lr=0.0001
[08/14 20:11:56 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9307, Val Loss=0.2716, lr=0.0001
[08/14 20:12:46 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9308, Val Loss=0.2740, lr=0.0001
[08/14 20:13:36 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9316, Val Loss=0.2732, lr=0.0001
[08/14 20:14:27 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9299, Val Loss=0.2735, lr=0.0001
[08/14 20:15:17 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9310, Val Loss=0.2700, lr=0.0001
[08/14 20:16:07 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9306, Val Loss=0.2723, lr=0.0001
[08/14 20:16:58 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9311, Val Loss=0.2711, lr=0.0001
[08/14 20:17:48 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9321, Val Loss=0.2727, lr=0.0001
[08/14 20:18:38 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9301, Val Loss=0.2726, lr=0.0001
[08/14 20:18:38 cifar10-global-group_norm-resnet56]: Best Acc=0.9321
[08/14 20:18:39 cifar10-global-group_norm-resnet56]: Params: 0.51 M
[08/14 20:18:39 cifar10-global-group_norm-resnet56]: ops: 57.71 M
[08/14 20:18:49 cifar10-global-group_norm-resnet56]: Acc: 0.9301 Val Loss: 0.2726

[08/14 20:18:49 cifar10-global-group_norm-resnet56]: Partial Rebuilding...
=> Starting rebuilding...
(1 of 28) layer3.8.conv1 has been rebuilt.
(2 of 28) layer3.7.conv1 has been rebuilt.
(3 of 28) layer3.6.conv1 has been rebuilt.
(4 of 28) layer3.5.conv1 has been rebuilt.
(5 of 28) layer3.4.conv1 has been rebuilt.
(6 of 28) layer3.3.conv1 has been rebuilt.
(7 of 28) layer3.2.conv1 has been rebuilt.
(8 of 28) layer3.1.conv1 has been rebuilt.
(9 of 28) layer3.0.conv1 has been rebuilt.
(10 of 28) layer2.8.conv1 has been rebuilt.
(11 of 28) layer2.7.conv1 has been rebuilt.
(12 of 28) layer2.6.conv1 has been rebuilt.
(13 of 28) layer2.5.conv1 has been rebuilt.
(14 of 28) layer2.4.conv1 has been rebuilt.
(15 of 28) layer2.3.conv1 has been rebuilt.
(16 of 28) layer2.2.conv1 has been rebuilt.
(17 of 28) layer2.1.conv1 has been rebuilt.
(18 of 28) layer2.0.conv1 has been rebuilt.
(19 of 28) layer1.7.conv1 has been rebuilt.
(20 of 28) layer1.6.conv1 has been rebuilt.
(21 of 28) layer1.5.conv1 has been rebuilt.
(22 of 28) layer1.4.conv1 has been rebuilt.
(23 of 28) layer1.3.conv1 has been rebuilt.
(24 of 28) layer1.2.conv1 has been rebuilt.
(25 of 28) layer1.1.conv1 has been rebuilt.
(26 of 28) conv1 has been rebuilt.
(27 of 28) layer2.0.downsample.0 has been rebuilt.
(28 of 28) layer3.0.downsample.0 has been rebuilt.
[08/14 20:18:50 cifar10-global-group_norm-resnet56]: Validating partial rebuilt...
[08/14 20:19:01 cifar10-global-group_norm-resnet56]: Acc: 0.1943 Val Loss: 5.7395

[08/14 20:19:01 cifar10-global-group_norm-resnet56]: Finetuning partial rebuilt...
[08/14 20:20:05 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.4448, Val Loss=1.5433, lr=0.0100
[08/14 20:21:09 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.5517, Val Loss=1.2586, lr=0.0100
[08/14 20:22:12 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.6136, Val Loss=1.0870, lr=0.0100
[08/14 20:23:15 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.6187, Val Loss=1.0698, lr=0.0100
[08/14 20:24:18 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.6298, Val Loss=1.0598, lr=0.0100
[08/14 20:25:21 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.6750, Val Loss=0.9442, lr=0.0100
[08/14 20:26:25 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.6564, Val Loss=0.9997, lr=0.0100
[08/14 20:27:29 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.7144, Val Loss=0.8298, lr=0.0100
[08/14 20:28:33 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.7061, Val Loss=0.8907, lr=0.0100
[08/14 20:29:35 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.7287, Val Loss=0.8097, lr=0.0100
[08/14 20:30:38 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.7574, Val Loss=0.7087, lr=0.0100
[08/14 20:31:41 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.7567, Val Loss=0.7305, lr=0.0100
[08/14 20:32:44 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.7561, Val Loss=0.7285, lr=0.0100
[08/14 20:33:47 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.7705, Val Loss=0.6835, lr=0.0100
[08/14 20:35:02 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.7366, Val Loss=0.7849, lr=0.0100
[08/14 20:36:18 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.7773, Val Loss=0.6527, lr=0.0100
[08/14 20:37:21 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.7740, Val Loss=0.6888, lr=0.0100
[08/14 20:38:24 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.7579, Val Loss=0.7566, lr=0.0100
[08/14 20:39:28 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.7886, Val Loss=0.6339, lr=0.0100
[08/14 20:40:31 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8065, Val Loss=0.5794, lr=0.0100
[08/14 20:41:35 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.7979, Val Loss=0.6071, lr=0.0100
[08/14 20:42:38 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8049, Val Loss=0.5873, lr=0.0100
[08/14 20:43:41 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.7955, Val Loss=0.6307, lr=0.0100
[08/14 20:44:49 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.7962, Val Loss=0.6237, lr=0.0100
[08/14 20:45:51 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8133, Val Loss=0.5679, lr=0.0100
[08/14 20:46:55 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8028, Val Loss=0.5984, lr=0.0100
[08/14 20:47:58 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.7599, Val Loss=0.7445, lr=0.0100
[08/14 20:49:00 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.7918, Val Loss=0.6539, lr=0.0100
[08/14 20:50:04 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.7872, Val Loss=0.6643, lr=0.0100
[08/14 20:51:07 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.7995, Val Loss=0.6109, lr=0.0100
[08/14 20:52:11 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8171, Val Loss=0.5477, lr=0.0100
[08/14 20:53:13 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8256, Val Loss=0.5310, lr=0.0100
[08/14 20:54:16 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8053, Val Loss=0.6058, lr=0.0100
[08/14 20:55:20 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.7930, Val Loss=0.6490, lr=0.0100
[08/14 20:56:25 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8125, Val Loss=0.5540, lr=0.0100
[08/14 20:57:28 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8049, Val Loss=0.5996, lr=0.0100
[08/14 20:58:31 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8319, Val Loss=0.5050, lr=0.0100
[08/14 20:59:34 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8076, Val Loss=0.5860, lr=0.0100
[08/14 21:00:37 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8225, Val Loss=0.5292, lr=0.0100
[08/14 21:01:40 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8338, Val Loss=0.4986, lr=0.0100
[08/14 21:02:42 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.8691, Val Loss=0.3925, lr=0.0010
[08/14 21:03:55 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.8699, Val Loss=0.3892, lr=0.0010
[08/14 21:05:06 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.8694, Val Loss=0.3857, lr=0.0010
[08/14 21:06:15 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.8724, Val Loss=0.3874, lr=0.0010
[08/14 21:07:20 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.8712, Val Loss=0.3836, lr=0.0010
[08/14 21:08:23 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.8716, Val Loss=0.3862, lr=0.0010
[08/14 21:09:26 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.8731, Val Loss=0.3894, lr=0.0010
[08/14 21:10:30 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.8743, Val Loss=0.3861, lr=0.0010
[08/14 21:11:35 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.8718, Val Loss=0.3878, lr=0.0010
[08/14 21:12:38 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.8725, Val Loss=0.3889, lr=0.0010
[08/14 21:13:41 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.8720, Val Loss=0.3889, lr=0.0010
[08/14 21:14:44 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.8735, Val Loss=0.3866, lr=0.0010
[08/14 21:15:47 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.8729, Val Loss=0.3932, lr=0.0010
[08/14 21:16:50 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.8725, Val Loss=0.3887, lr=0.0010
[08/14 21:17:53 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.8727, Val Loss=0.3951, lr=0.0010
[08/14 21:18:56 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.8715, Val Loss=0.3924, lr=0.0010
[08/14 21:20:00 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.8737, Val Loss=0.3969, lr=0.0010
[08/14 21:21:02 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.8720, Val Loss=0.3929, lr=0.0010
[08/14 21:22:05 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.8732, Val Loss=0.3963, lr=0.0010
[08/14 21:23:08 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.8736, Val Loss=0.3967, lr=0.0010
[08/14 21:24:12 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.8763, Val Loss=0.3901, lr=0.0001
[08/14 21:25:15 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.8750, Val Loss=0.3892, lr=0.0001
[08/14 21:26:19 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.8767, Val Loss=0.3885, lr=0.0001
[08/14 21:27:22 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.8777, Val Loss=0.3884, lr=0.0001
[08/14 21:28:25 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.8767, Val Loss=0.3913, lr=0.0001
[08/14 21:29:30 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.8766, Val Loss=0.3894, lr=0.0001
[08/14 21:30:33 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.8775, Val Loss=0.3891, lr=0.0001
[08/14 21:31:36 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.8769, Val Loss=0.3900, lr=0.0001
[08/14 21:32:39 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.8758, Val Loss=0.3905, lr=0.0001
[08/14 21:33:41 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.8768, Val Loss=0.3912, lr=0.0001
[08/14 21:34:45 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.8762, Val Loss=0.3885, lr=0.0001
[08/14 21:35:48 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.8761, Val Loss=0.3885, lr=0.0001
[08/14 21:36:51 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.8771, Val Loss=0.3875, lr=0.0001
[08/14 21:37:54 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.8771, Val Loss=0.3903, lr=0.0001
[08/14 21:38:57 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.8762, Val Loss=0.3903, lr=0.0001
[08/14 21:40:00 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.8758, Val Loss=0.3890, lr=0.0001
[08/14 21:41:03 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.8763, Val Loss=0.3891, lr=0.0001
[08/14 21:42:07 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.8762, Val Loss=0.3897, lr=0.0001
[08/14 21:43:10 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.8768, Val Loss=0.3917, lr=0.0001
[08/14 21:44:13 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.8766, Val Loss=0.3897, lr=0.0001
[08/14 21:44:13 cifar10-global-group_norm-resnet56]: Best Acc=0.8777
[08/14 21:44:13 cifar10-global-group_norm-resnet56]: Final Rebuilding...
=> Starting rebuilding...
(1 of 30) layer3.8.conv1 has been rebuilt.
(2 of 30) layer3.7.conv1 has been rebuilt.
(3 of 30) layer3.6.conv1 has been rebuilt.
(4 of 30) layer3.5.conv1 has been rebuilt.
(5 of 30) layer3.4.conv1 has been rebuilt.
(6 of 30) layer3.3.conv1 has been rebuilt.
(7 of 30) layer3.2.conv1 has been rebuilt.
(8 of 30) layer3.1.conv1 has been rebuilt.
(9 of 30) layer3.0.conv1 has been rebuilt.
(10 of 30) layer2.8.conv1 has been rebuilt.
(11 of 30) layer2.7.conv1 has been rebuilt.
(12 of 30) layer2.6.conv1 has been rebuilt.
(13 of 30) layer2.5.conv1 has been rebuilt.
(14 of 30) layer2.4.conv1 has been rebuilt.
(15 of 30) layer2.3.conv1 has been rebuilt.
(16 of 30) layer2.2.conv1 has been rebuilt.
(17 of 30) layer2.1.conv1 has been rebuilt.
(18 of 30) layer2.0.conv1 has been rebuilt.
(19 of 30) layer1.8.conv1 has been rebuilt.
(20 of 30) layer1.7.conv1 has been rebuilt.
(21 of 30) layer1.6.conv1 has been rebuilt.
(22 of 30) layer1.5.conv1 has been rebuilt.
(23 of 30) layer1.4.conv1 has been rebuilt.
(24 of 30) layer1.3.conv1 has been rebuilt.
(25 of 30) layer1.2.conv1 has been rebuilt.
(26 of 30) layer1.1.conv1 has been rebuilt.
(27 of 30) layer1.0.conv1 has been rebuilt.
(28 of 30) conv1 has been rebuilt.
(29 of 30) layer2.0.downsample.0 has been rebuilt.
(30 of 30) layer3.0.downsample.0 has been rebuilt.
[08/14 21:44:14 cifar10-global-group_norm-resnet56]: Validating final rebuilt...
[08/14 21:44:25 cifar10-global-group_norm-resnet56]: Acc: 0.1000 Val Loss: 12.2184

[08/14 21:44:25 cifar10-global-group_norm-resnet56]: Fine tuning rebuilt...
[08/14 21:45:22 cifar10-global-group_norm-resnet56]: Epoch 0/80, Acc=0.6368, Val Loss=1.0552, lr=0.0100
[08/14 21:46:21 cifar10-global-group_norm-resnet56]: Epoch 1/80, Acc=0.7355, Val Loss=0.7960, lr=0.0100
[08/14 21:47:20 cifar10-global-group_norm-resnet56]: Epoch 2/80, Acc=0.7856, Val Loss=0.6199, lr=0.0100
[08/14 21:48:19 cifar10-global-group_norm-resnet56]: Epoch 3/80, Acc=0.8050, Val Loss=0.5843, lr=0.0100
[08/14 21:49:16 cifar10-global-group_norm-resnet56]: Epoch 4/80, Acc=0.8335, Val Loss=0.4865, lr=0.0100
[08/14 21:50:14 cifar10-global-group_norm-resnet56]: Epoch 5/80, Acc=0.8478, Val Loss=0.4532, lr=0.0100
[08/14 21:51:11 cifar10-global-group_norm-resnet56]: Epoch 6/80, Acc=0.8549, Val Loss=0.4243, lr=0.0100
[08/14 21:52:08 cifar10-global-group_norm-resnet56]: Epoch 7/80, Acc=0.8397, Val Loss=0.4803, lr=0.0100
[08/14 21:53:05 cifar10-global-group_norm-resnet56]: Epoch 8/80, Acc=0.8391, Val Loss=0.4749, lr=0.0100
[08/14 21:54:03 cifar10-global-group_norm-resnet56]: Epoch 9/80, Acc=0.8589, Val Loss=0.4329, lr=0.0100
[08/14 21:55:00 cifar10-global-group_norm-resnet56]: Epoch 10/80, Acc=0.8543, Val Loss=0.4326, lr=0.0100
[08/14 21:55:57 cifar10-global-group_norm-resnet56]: Epoch 11/80, Acc=0.8671, Val Loss=0.4028, lr=0.0100
[08/14 21:56:54 cifar10-global-group_norm-resnet56]: Epoch 12/80, Acc=0.8773, Val Loss=0.3876, lr=0.0100
[08/14 21:57:51 cifar10-global-group_norm-resnet56]: Epoch 13/80, Acc=0.8801, Val Loss=0.3492, lr=0.0100
[08/14 21:58:48 cifar10-global-group_norm-resnet56]: Epoch 14/80, Acc=0.8863, Val Loss=0.3447, lr=0.0100
[08/14 21:59:45 cifar10-global-group_norm-resnet56]: Epoch 15/80, Acc=0.8662, Val Loss=0.4258, lr=0.0100
[08/14 22:00:43 cifar10-global-group_norm-resnet56]: Epoch 16/80, Acc=0.8671, Val Loss=0.3968, lr=0.0100
[08/14 22:01:41 cifar10-global-group_norm-resnet56]: Epoch 17/80, Acc=0.8866, Val Loss=0.3495, lr=0.0100
[08/14 22:02:40 cifar10-global-group_norm-resnet56]: Epoch 18/80, Acc=0.8913, Val Loss=0.3357, lr=0.0100
[08/14 22:03:38 cifar10-global-group_norm-resnet56]: Epoch 19/80, Acc=0.8773, Val Loss=0.3994, lr=0.0100
[08/14 22:04:36 cifar10-global-group_norm-resnet56]: Epoch 20/80, Acc=0.8798, Val Loss=0.3810, lr=0.0100
[08/14 22:05:35 cifar10-global-group_norm-resnet56]: Epoch 21/80, Acc=0.8797, Val Loss=0.3920, lr=0.0100
[08/14 22:06:33 cifar10-global-group_norm-resnet56]: Epoch 22/80, Acc=0.8857, Val Loss=0.3680, lr=0.0100
[08/14 22:07:32 cifar10-global-group_norm-resnet56]: Epoch 23/80, Acc=0.8930, Val Loss=0.3352, lr=0.0100
[08/14 22:08:30 cifar10-global-group_norm-resnet56]: Epoch 24/80, Acc=0.8880, Val Loss=0.3623, lr=0.0100
[08/14 22:09:29 cifar10-global-group_norm-resnet56]: Epoch 25/80, Acc=0.8893, Val Loss=0.3521, lr=0.0100
[08/14 22:10:27 cifar10-global-group_norm-resnet56]: Epoch 26/80, Acc=0.8889, Val Loss=0.3817, lr=0.0100
[08/14 22:11:26 cifar10-global-group_norm-resnet56]: Epoch 27/80, Acc=0.8767, Val Loss=0.3957, lr=0.0100
[08/14 22:12:25 cifar10-global-group_norm-resnet56]: Epoch 28/80, Acc=0.8863, Val Loss=0.3609, lr=0.0100
[08/14 22:13:23 cifar10-global-group_norm-resnet56]: Epoch 29/80, Acc=0.8695, Val Loss=0.4216, lr=0.0100
[08/14 22:14:22 cifar10-global-group_norm-resnet56]: Epoch 30/80, Acc=0.8915, Val Loss=0.3361, lr=0.0100
[08/14 22:15:20 cifar10-global-group_norm-resnet56]: Epoch 31/80, Acc=0.8800, Val Loss=0.3908, lr=0.0100
[08/14 22:16:19 cifar10-global-group_norm-resnet56]: Epoch 32/80, Acc=0.8963, Val Loss=0.3372, lr=0.0100
[08/14 22:17:17 cifar10-global-group_norm-resnet56]: Epoch 33/80, Acc=0.9007, Val Loss=0.3205, lr=0.0100
[08/14 22:18:18 cifar10-global-group_norm-resnet56]: Epoch 34/80, Acc=0.8872, Val Loss=0.3646, lr=0.0100
[08/14 22:19:19 cifar10-global-group_norm-resnet56]: Epoch 35/80, Acc=0.8974, Val Loss=0.3338, lr=0.0100
[08/14 22:20:20 cifar10-global-group_norm-resnet56]: Epoch 36/80, Acc=0.8922, Val Loss=0.3494, lr=0.0100
[08/14 22:21:20 cifar10-global-group_norm-resnet56]: Epoch 37/80, Acc=0.8892, Val Loss=0.3719, lr=0.0100
[08/14 22:22:18 cifar10-global-group_norm-resnet56]: Epoch 38/80, Acc=0.8947, Val Loss=0.3532, lr=0.0100
[08/14 22:23:17 cifar10-global-group_norm-resnet56]: Epoch 39/80, Acc=0.8958, Val Loss=0.3508, lr=0.0100
[08/14 22:24:15 cifar10-global-group_norm-resnet56]: Epoch 40/80, Acc=0.9241, Val Loss=0.2494, lr=0.0010
[08/14 22:25:14 cifar10-global-group_norm-resnet56]: Epoch 41/80, Acc=0.9271, Val Loss=0.2471, lr=0.0010
[08/14 22:26:12 cifar10-global-group_norm-resnet56]: Epoch 42/80, Acc=0.9274, Val Loss=0.2469, lr=0.0010
[08/14 22:27:11 cifar10-global-group_norm-resnet56]: Epoch 43/80, Acc=0.9294, Val Loss=0.2483, lr=0.0010
[08/14 22:28:09 cifar10-global-group_norm-resnet56]: Epoch 44/80, Acc=0.9278, Val Loss=0.2504, lr=0.0010
[08/14 22:29:08 cifar10-global-group_norm-resnet56]: Epoch 45/80, Acc=0.9276, Val Loss=0.2523, lr=0.0010
[08/14 22:30:06 cifar10-global-group_norm-resnet56]: Epoch 46/80, Acc=0.9282, Val Loss=0.2538, lr=0.0010
[08/14 22:31:05 cifar10-global-group_norm-resnet56]: Epoch 47/80, Acc=0.9273, Val Loss=0.2555, lr=0.0010
[08/14 22:32:03 cifar10-global-group_norm-resnet56]: Epoch 48/80, Acc=0.9287, Val Loss=0.2542, lr=0.0010
[08/14 22:33:03 cifar10-global-group_norm-resnet56]: Epoch 49/80, Acc=0.9297, Val Loss=0.2594, lr=0.0010
[08/14 22:34:01 cifar10-global-group_norm-resnet56]: Epoch 50/80, Acc=0.9295, Val Loss=0.2633, lr=0.0010
[08/14 22:35:00 cifar10-global-group_norm-resnet56]: Epoch 51/80, Acc=0.9280, Val Loss=0.2655, lr=0.0010
[08/14 22:35:59 cifar10-global-group_norm-resnet56]: Epoch 52/80, Acc=0.9254, Val Loss=0.2663, lr=0.0010
[08/14 22:36:57 cifar10-global-group_norm-resnet56]: Epoch 53/80, Acc=0.9275, Val Loss=0.2658, lr=0.0010
[08/14 22:37:56 cifar10-global-group_norm-resnet56]: Epoch 54/80, Acc=0.9302, Val Loss=0.2647, lr=0.0010
[08/14 22:38:54 cifar10-global-group_norm-resnet56]: Epoch 55/80, Acc=0.9295, Val Loss=0.2640, lr=0.0010
[08/14 22:39:54 cifar10-global-group_norm-resnet56]: Epoch 56/80, Acc=0.9290, Val Loss=0.2697, lr=0.0010
[08/14 22:40:53 cifar10-global-group_norm-resnet56]: Epoch 57/80, Acc=0.9295, Val Loss=0.2719, lr=0.0010
[08/14 22:41:51 cifar10-global-group_norm-resnet56]: Epoch 58/80, Acc=0.9281, Val Loss=0.2754, lr=0.0010
[08/14 22:42:50 cifar10-global-group_norm-resnet56]: Epoch 59/80, Acc=0.9283, Val Loss=0.2743, lr=0.0010
[08/14 22:43:48 cifar10-global-group_norm-resnet56]: Epoch 60/80, Acc=0.9296, Val Loss=0.2706, lr=0.0001
[08/14 22:44:46 cifar10-global-group_norm-resnet56]: Epoch 61/80, Acc=0.9301, Val Loss=0.2690, lr=0.0001
[08/14 22:45:45 cifar10-global-group_norm-resnet56]: Epoch 62/80, Acc=0.9302, Val Loss=0.2700, lr=0.0001
[08/14 22:46:43 cifar10-global-group_norm-resnet56]: Epoch 63/80, Acc=0.9303, Val Loss=0.2684, lr=0.0001
[08/14 22:47:41 cifar10-global-group_norm-resnet56]: Epoch 64/80, Acc=0.9301, Val Loss=0.2685, lr=0.0001
[08/14 22:48:39 cifar10-global-group_norm-resnet56]: Epoch 65/80, Acc=0.9306, Val Loss=0.2708, lr=0.0001
[08/14 22:49:41 cifar10-global-group_norm-resnet56]: Epoch 66/80, Acc=0.9306, Val Loss=0.2700, lr=0.0001
[08/14 22:50:42 cifar10-global-group_norm-resnet56]: Epoch 67/80, Acc=0.9310, Val Loss=0.2685, lr=0.0001
[08/14 22:51:42 cifar10-global-group_norm-resnet56]: Epoch 68/80, Acc=0.9299, Val Loss=0.2698, lr=0.0001
[08/14 22:52:40 cifar10-global-group_norm-resnet56]: Epoch 69/80, Acc=0.9306, Val Loss=0.2694, lr=0.0001
[08/14 22:53:39 cifar10-global-group_norm-resnet56]: Epoch 70/80, Acc=0.9308, Val Loss=0.2717, lr=0.0001
[08/14 22:54:37 cifar10-global-group_norm-resnet56]: Epoch 71/80, Acc=0.9317, Val Loss=0.2716, lr=0.0001
[08/14 22:55:43 cifar10-global-group_norm-resnet56]: Epoch 72/80, Acc=0.9295, Val Loss=0.2708, lr=0.0001
[08/14 22:56:47 cifar10-global-group_norm-resnet56]: Epoch 73/80, Acc=0.9307, Val Loss=0.2712, lr=0.0001
[08/14 22:57:45 cifar10-global-group_norm-resnet56]: Epoch 74/80, Acc=0.9300, Val Loss=0.2722, lr=0.0001
[08/14 22:58:43 cifar10-global-group_norm-resnet56]: Epoch 75/80, Acc=0.9309, Val Loss=0.2700, lr=0.0001
[08/14 22:59:41 cifar10-global-group_norm-resnet56]: Epoch 76/80, Acc=0.9304, Val Loss=0.2717, lr=0.0001
[08/14 23:00:39 cifar10-global-group_norm-resnet56]: Epoch 77/80, Acc=0.9307, Val Loss=0.2709, lr=0.0001
[08/14 23:01:37 cifar10-global-group_norm-resnet56]: Epoch 78/80, Acc=0.9289, Val Loss=0.2736, lr=0.0001
[08/14 23:02:36 cifar10-global-group_norm-resnet56]: Epoch 79/80, Acc=0.9301, Val Loss=0.2713, lr=0.0001
[08/14 23:02:36 cifar10-global-group_norm-resnet56]: Best Acc=0.9317

(base) C:\Users\35679\Downloads\Torch-Pruning-1.1.4\Torch-Pruning-1.1.4\benchmarks>
